{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DerivaML Dataset\n",
    "\n",
    "DerivaML is a class library built on the Deriva Scientific Asset management system that is designed to help simplify a number of the basic operations associated with building and testing ML libraries based on common toolkits such as TensorFlow.  This notebook reviews the basic features of the DerivaML library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Set up DerivaML  for test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "from deriva_ml.demo_catalog import create_demo_catalog, DemoML\n",
    "from deriva_ml import MLVocab, ExecutionConfiguration, Workflow, DerivaSystemColumns, VersionPart, DatasetSpec\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, HTML, JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Set the details for the catalog we want and authenticate to the server if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "hostname = 'localhost'\n",
    "domain_schema = 'demo-schema'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnl = GlobusNativeLogin(host=hostname)\n",
    "if gnl.is_logged_in([hostname]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([hostname], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Create a test catalog and get an instance of the DemoML class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_catalog = create_demo_catalog(hostname, domain_schema)\n",
    "ml_instance = DemoML(hostname, test_catalog.catalog_id, use_minid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Configure DerivaML Datasets\n",
    "\n",
    "In Deriva-ML a dataset is used to aggregate instances of entities.  However, before we can create any datasets, we must configure \n",
    "Deriva-ML for the specifics of the datasets.  The first stp is we need to tell Deriva-ML what types of use defined objects can be associated with a dataset.  \n",
    "\n",
    "Note that out of the box, Deriva-ML is configured to allow datasets to contained dataset (i.e. nested datasets), so we don't need to do anything for that specific configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current dataset_table element types: {[a.name for a in ml_instance.list_dataset_element_types()]}\")\n",
    "ml_instance.add_dataset_element_type(\"Subject\")\n",
    "ml_instance.add_dataset_element_type(\"Image\")\n",
    "print(f\"New dataset_table element types {[a.name for a in ml_instance.list_dataset_element_types()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Now that we have configured our datasets, we need to identify the dataset types so we can distinguish between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset_table\n",
    "ml_instance.add_term(MLVocab.dataset_type, \"DemoSet\", description=\"A test dataset_table\")\n",
    "ml_instance.add_term(MLVocab.dataset_type, 'Partitioned', description=\"A partitioned dataset_table for ML training.\")\n",
    "ml_instance.add_term(MLVocab.dataset_type, \"Subject\", description=\"A test dataset_table\")\n",
    "ml_instance.add_term(MLVocab.dataset_type, \"Image\", description=\"A test dataset_table\")\n",
    "ml_instance.add_term(MLVocab.dataset_type, \"Training\", description=\"Training dataset_table\")\n",
    "ml_instance.add_term(MLVocab.dataset_type, \"Testing\", description=\"Training dataset_table\")\n",
    "ml_instance.add_term(MLVocab.dataset_type, \"Validation\", description=\"Validation dataset_table\")\n",
    "\n",
    "ml_instance.list_vocabulary_terms(MLVocab.dataset_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Now create datasets and populate with elements from the test catalogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_instance.add_term(MLVocab.workflow_type, \"Create Dataset Notebook\", description=\"A Workflow that creates a new dataset_table\")\n",
    "\n",
    "# Now lets create model configuration for our program.\n",
    "api_workflow = Workflow(\n",
    "    name=\"API Workflow\",\n",
    "    url=\"https://github.com/informatics-isi-edu/deriva-ml/blob/main/docs/Notebooks/DerivaML%20Dataset.ipynb\",\n",
    "    workflow_type=\"Create Dataset Notebook\"\n",
    ")\n",
    "\n",
    "dataset_execution = ml_instance.create_execution(\n",
    "    ExecutionConfiguration(\n",
    "        workflow=api_workflow,\n",
    "        description=\"Our Sample Workflow instance\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_dataset = dataset_execution.create_dataset(['DemoSet', 'Subject'], description=\"A subject dataset_table\")\n",
    "image_dataset = dataset_execution.create_dataset(['DemoSet', 'Image'], description=\"A image training dataset_table\")\n",
    "datasets = pd.DataFrame(ml_instance.find_datasets()).drop(columns=DerivaSystemColumns)\n",
    "display(\n",
    "    Markdown('## Datasets'),\n",
    "    datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "And now that we have defined some datasets, we can add elements of the appropriate type to them.  We can see what is in our new datasets by listing the dataset members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of subjects and images from the catalog using the DataPath API.\n",
    "dp = ml_instance.domain_path  # Each call returns a new path instance, so only call once...\n",
    "subject_rids = [i['RID'] for i in dp.tables['Subject'].entities().fetch()]\n",
    "image_rids = [i['RID'] for i in dp.tables['Image'].entities().fetch()]\n",
    "\n",
    "ml_instance.add_dataset_members(dataset_rid=subject_dataset, members=subject_rids)\n",
    "ml_instance.add_dataset_members(dataset_rid=image_dataset, members=image_rids)\n",
    "\n",
    "# List the contents of our datasets, and let's not include columns like modify time.\n",
    "display(\n",
    "    Markdown('## Subject Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(subject_dataset)['Subject']).drop(columns=DerivaSystemColumns),\n",
    "    Markdown('## Image Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(image_dataset)['Image']).drop(columns=DerivaSystemColumns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Create partitioned dataset\n",
    "\n",
    "Now let's create some subsets of the original dataset based on subject level metadata. We are going to create the subsets based on the metadata values of the subjects. We will download the subject dataset and look at its metadata to figure out how to partition the original data. Since we are not going to look at the images, we use the materialize=False option to save some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bag = ml_instance.download_dataset_bag(DatasetSpec(rid=subject_dataset, version=ml_instance.dataset_version(subject_dataset), materialize=False))\n",
    "print(f\"Bag materialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "The domain model has two objects: Subject and Images where an Image is associated with a subject, but a subject can have multiple images associated with it.  Let's look at the subjects and partition into test and training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the subjects.....\n",
    "subject_df = dataset_bag.get_table_as_dataframe('Subject')[['RID', 'Name']]\n",
    "image_df = dataset_bag.get_table_as_dataframe('Image')[['RID', 'Subject', 'URL']]\n",
    "metadata_df = subject_df.join(image_df, lsuffix=\"_subject\", rsuffix=\"_image\")\n",
    "display(metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "For ths example, lets partition the data based on the name of the subject.  Of course in real examples, we would do a more complex analysis in deciding\n",
    "what subset goes into each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thing_number(name: pd.Series) -> pd.Series:\n",
    "    return name.map(lambda n: int(n.replace('Thing','')))\n",
    "\n",
    "training_rids = metadata_df.loc[lambda df: thing_number(df['Name']) % 3 == 0]['RID_image'].tolist()\n",
    "testing_rids =  metadata_df.loc[lambda df: thing_number(df['Name']) % 3 == 1]['RID_image'].tolist()\n",
    "validation_rids = metadata_df.loc[lambda df: thing_number(df['Name']) % 3 == 2]['RID_image'].tolist()\n",
    "\n",
    "print(f'Training images: {training_rids}')\n",
    "print(f'Testing images: {testing_rids}')\n",
    "print(f'Validation images: {validation_rids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Now that we know what we want in each dataset, lets create datasets for each of our partitioned elements along with a nested dataset to track the entire collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_dataset = dataset_execution.create_dataset(['Partitioned', 'Image'], description='A nested dataset_table for machine learning')\n",
    "training_dataset = dataset_execution.create_dataset('Training', description='An image dataset_table for training')\n",
    "testing_dataset = dataset_execution.create_dataset('Testing', description='A image dataset_table for testing')\n",
    "validation_dataset = dataset_execution.create_dataset('Validation', description='A image dataset_table for validation')\n",
    "pd.DataFrame(ml_instance.find_datasets()).drop(columns=DerivaSystemColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "And then fill the datasets with the appropriate members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_instance.add_dataset_members(dataset_rid=nested_dataset, members=[training_dataset, testing_dataset, validation_dataset])\n",
    "ml_instance.add_dataset_members(dataset_rid=training_dataset, members=training_rids)\n",
    "ml_instance.add_dataset_members(dataset_rid=testing_dataset, members=testing_rids)\n",
    "ml_instance.add_dataset_members(dataset_rid=validation_dataset, members=validation_rids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Ok, lets see what we have now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "As our very last step, lets get a PID that will allow us to share and cite the dataset that we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Markdown('## Nested Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(nested_dataset)['Dataset']).drop(columns=DerivaSystemColumns),\n",
    "    Markdown('## Training Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(training_dataset)['Image']).drop(columns=DerivaSystemColumns),\n",
    "    Markdown('## Testing Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(testing_dataset)['Image']).drop(columns=DerivaSystemColumns),\n",
    "    Markdown('## Validation Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(validation_dataset)['Image']).drop(columns=DerivaSystemColumns),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dataset parents: {ml_instance.list_dataset_parents(training_dataset)}')\n",
    "print(f'Dataset children: {ml_instance.list_dataset_children(nested_dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_citation = ml_instance.cite(nested_dataset)\n",
    "display(\n",
    "    HTML(f'Nested dataset_table citation: <a href={dataset_citation}>{dataset_citation}</a>')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "     Markdown('## Nested Dataset -- Recursive Listing'),\n",
    "    JSON(ml_instance.list_dataset_members(nested_dataset, recurse=True))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Dataset Versions\n",
    "Datasets have a version number which can be retrieved or incremented.  We follow the equivalent of semantic versioning, but for data rather than code.  Note that datasets are also versioned by virtue of the fact that the dataset RID can include a catalog snapshot ID as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Current dataset_table version for training_dataset: {ml_instance.dataset_version(training_dataset)}')\n",
    "next_version = ml_instance.increment_dataset_version(training_dataset, VersionPart.minor)\n",
    "print(f'Next dataset_table version for training_dataset: {next_version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(f'<a href={ml_instance.chaise_url(\"Dataset\")}>Browse Datasets</a>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_catalog.delete_ermrest_catalog(really=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
