{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32bc524e",
   "metadata": {},
   "source": [
    "# Networkx ATLAS KG construction and RAG example\n",
    "This notebook demonstrates the full streamlined process of creating a knowledge graph (KG) using the atlas-rag package and performing retrieval-augmented generation (RAG) with our created RAG methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a035b3",
   "metadata": {},
   "source": [
    "## ATLAS KG Construction\n",
    "It is suggested to use local hf model to run the KG construction code, as llm api service provider use optimized, lightweight models to reduce costs, which may sacrifice performance, and hence hard to have guaranteed performance. (for example from fp16 to fp8 etc.)\n",
    "\n",
    "ATLAS KG construction consist of 5 steps:\n",
    "- Triples Json Generation (Base KG Json)\n",
    "- Convert Triples Json to Triples csv\n",
    "- Conceptualize Entity in Triples csv\n",
    "- Merge Concept CSV to Triples CSV\n",
    "- Convert CSV to graphml for networkx to perform rag / to neo4j dumps for Billion KG RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c083856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/httsangaj/miniconda3/envs/atlas-rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "from atlas_rag.kg_construction.triple_extraction import KnowledgeGraphExtractor\n",
    "from atlas_rag.kg_construction.triple_config import ProcessingConfig\n",
    "from atlas_rag.llm_generator import LLMGenerator\n",
    "from openai import OpenAI\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from transformers import pipeline\n",
    "from configparser import ConfigParser\n",
    "# Load OpenRouter API key from config file\n",
    "config = ConfigParser()\n",
    "config.read('config.ini')\n",
    "# model_name = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "# connection = AIProjectClient(\n",
    "#     endpoint=config[\"urls\"][\"AZURE_URL\"],\n",
    "#     credential=DefaultAzureCredential(),\n",
    "# )\n",
    "# client = connection.inference.get_azure_openai_client(api_version=\"2024-12-01-preview\")\n",
    "client = OpenAI(base_url=\"http://0.0.0.0:8129/v1\", api_key=\"EMPTY\")\n",
    "triple_generator = LLMGenerator(client=client, model_name=\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "# model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# client = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model_name,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "filename_pattern = 'test_data'\n",
    "output_directory = f'benchmark_data/autograph/test_data'\n",
    "# triple_generator = LLMGenerator(client, model_name=model_name)\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37353f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom kg extraction prompt:\n",
      "{'en': {'system': 'You are a helpful assistant', 'triple_extraction': 'You are an expert knowledge graph constructor. Your task is to extract factual information from the provided text and represent it as a list of knowledge graph triples.\\nEach triple should be a JSON object with three keys:\\n1.  `subject`: The main entity, concept, event, or attribute of the triple.\\n2.  `relation`: The relationship between the subject and the object.\\n3.  `object`: The entity, concept, value, event, or attribute that the subject has a relationship with.\\nConstraints:\\n- Extract all possible and relevant triples.\\n- The `subject` and `object` can be specific entities (e.g., \"Radio City\", \"Football in Albania\", \"Echosmith\") or specific values (e.g., \"3 July 2001\", \"1,310,696\").\\n- The `relation` should be a concise, descriptive phrase or verb that accurately describes the relationship (e.g., \"founded by\", \"started on\", \"is a\", \"has circulation of\").\\n- Ensure the triples are self-contained and logically sound.\\n- If no triples can be extracted from the text, return an empty JSON list: `[]`.\\n- Do not include any text other than the JSON output.'}}\n",
      "Using custom kg extraction schema:\n",
      "{'triple_extraction': {'type': 'array', 'items': {'type': 'object', 'properties': {'subject': {'type': 'string'}, 'relation': {'type': 'string'}, 'object': {'type': 'string'}}, 'required': ['subject', 'relation', 'object']}}}\n",
      "{'type': 'array', 'items': {'type': 'object', 'properties': {'subject': {'type': 'string'}, 'relation': {'type': 'string'}, 'object': {'type': 'string'}}, 'required': ['subject', 'relation', 'object']}}\n"
     ]
    }
   ],
   "source": [
    "kg_extraction_config = ProcessingConfig(\n",
    "      model_path=model_name,\n",
    "      data_directory=f'benchmark_data/autograph/{filename_pattern}',\n",
    "      filename_pattern=filename_pattern,\n",
    "      batch_size_triple=16,\n",
    "      batch_size_concept=16,\n",
    "      output_directory=f\"{output_directory}\",\n",
    "      max_new_tokens=2048,\n",
    "      max_workers=3,\n",
    "      remove_doc_spaces=True, # For removing duplicated spaces in the document text\n",
    "      include_concept=False, # Whether to include concept nodes and edges in the knowledge graph\n",
    "      triple_extraction_prompt_path='benchmark_data/autograph/custom_prompt.json',\n",
    "      triple_extraction_schema_path='benchmark_data/autograph/custom_schema.json',\n",
    "      record=True, # Whether to record the results in a JSON file\n",
    ")\n",
    "kg_extractor = KnowledgeGraphExtractor(model=triple_generator, config=kg_extraction_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c248a3",
   "metadata": {},
   "source": [
    "### Triples Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c10bffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data files: ['test_data.json']\n",
      "Processing shard 1/1 (texts 0-0 of 1, 1 documents)\n",
      "Generated 1 chunks for shard 1/1\n",
      "Model: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 batches (16 chunks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# construct entity&event graph\n",
    "kg_extractor.run_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c8f043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from the json files\n",
      "Number of files:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 880.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file for file ids:  Qwen_Qwen2.5-7B-Instruct_test_data_output_20250810150302_1_in_1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert Triples Json to CSV\n",
    "kg_extractor.convert_json_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "335211ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_batches 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shard_0:   0%|          | 0/3 [00:00<?, ?it/s]2025-08-10 14:57:49,132 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:49,156 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:49,179 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:49,342 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:49,343 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:49,363 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:49,504 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:49,527 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:49,552 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:49,692 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:49,715 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:49,765 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:49,858 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:49,929 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:49,929 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:50,012 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:50,014 - INFO - Usage log: Node for departure, completion_usage: {'completion_tokens': 6, 'prompt_tokens': 402, 'total_tokens': 408, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.16327786445617676}\n",
      "2025-08-10 14:57:50,016 - INFO - Usage log: Node with a strained smile, completion_usage: {'completion_tokens': 7, 'prompt_tokens': 399, 'total_tokens': 406, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.2081594467163086}\n",
      "2025-08-10 14:57:50,017 - INFO - Usage log: Node Sam Rivera, completion_usage: {'completion_tokens': 6, 'prompt_tokens': 405, 'total_tokens': 411, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.18347549438476562}\n",
      "2025-08-10 14:57:50,017 - INFO - Usage log: Node briefing room, completion_usage: {'completion_tokens': 8, 'prompt_tokens': 401, 'total_tokens': 409, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.2123873233795166}\n",
      "2025-08-10 14:57:50,018 - INFO - Usage log: Node Paranormal Military Squad's elite, completion_usage: {'completion_tokens': 8, 'prompt_tokens': 401, 'total_tokens': 409, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.20557045936584473}\n",
      "2025-08-10 14:57:50,019 - INFO - Usage log: Node resolve, completion_usage: {'completion_tokens': 6, 'prompt_tokens': 403, 'total_tokens': 409, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.1625075340270996}\n",
      "2025-08-10 14:57:50,020 - INFO - Usage log: Node Jordan Hayes, completion_usage: {'completion_tokens': 6, 'prompt_tokens': 401, 'total_tokens': 407, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.16160011291503906}\n",
      "2025-08-10 14:57:50,021 - INFO - Usage log: Node if Agent Alex Mercer was having second thoughts, completion_usage: {'completion_tokens': 8, 'prompt_tokens': 402, 'total_tokens': 410, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.20620322227478027}\n",
      "2025-08-10 14:57:50,021 - INFO - Usage log: Node their impending odyssey into Operation: Dulce, completion_usage: {'completion_tokens': 6, 'prompt_tokens': 403, 'total_tokens': 409, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.16378402709960938}\n",
      "2025-08-10 14:57:50,022 - INFO - Usage log: Node with the words 'counter-productive', completion_usage: {'completion_tokens': 7, 'prompt_tokens': 403, 'total_tokens': 410, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.18829751014709473}\n",
      "2025-08-10 14:57:50,023 - INFO - Usage log: Node after Taylor Cruz's reprimand, completion_usage: {'completion_tokens': 7, 'prompt_tokens': 406, 'total_tokens': 413, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.1880354881286621}\n",
      "2025-08-10 14:57:50,024 - INFO - Usage log: Node as they prepared for departure, completion_usage: {'completion_tokens': 8, 'prompt_tokens': 403, 'total_tokens': 411, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.21364498138427734}\n",
      "2025-08-10 14:57:50,025 - INFO - Usage log: Node to Agent Alex Mercer, completion_usage: {'completion_tokens': 6, 'prompt_tokens': 401, 'total_tokens': 407, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.16596055030822754}\n",
      "2025-08-10 14:57:50,026 - INFO - Usage log: Node ever so slightly, completion_usage: {'completion_tokens': 8, 'prompt_tokens': 399, 'total_tokens': 407, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.21551299095153809}\n",
      "2025-08-10 14:57:50,026 - INFO - Usage log: Node with Agent Taylor Cruz, completion_usage: {'completion_tokens': 6, 'prompt_tokens': 399, 'total_tokens': 405, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.1633291244506836}\n",
      "2025-08-10 14:57:50,027 - INFO - Usage log: Node for the two of them, completion_usage: {'completion_tokens': 6, 'prompt_tokens': 405, 'total_tokens': 411, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.15354061126708984}\n",
      "Shard_0:  33%|███▎      | 1/3 [00:01<00:02,  1.06s/it]2025-08-10 14:57:50,255 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:50,256 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:50,275 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:50,417 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:50,417 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:50,458 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:50,460 - INFO - Usage log: Node Agent Alex Mercer, completion_usage: {'completion_tokens': 8, 'prompt_tokens': 399, 'total_tokens': 407, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.22786402702331543}\n",
      "2025-08-10 14:57:50,461 - INFO - Usage log: Node projectors, completion_usage: {'completion_tokens': 10, 'prompt_tokens': 403, 'total_tokens': 413, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.24458694458007812}\n",
      "2025-08-10 14:57:50,462 - INFO - Usage log: Node about Agent Alex Mercer's struggle, completion_usage: {'completion_tokens': 8, 'prompt_tokens': 404, 'total_tokens': 412, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.2265322208404541}\n",
      "2025-08-10 14:57:50,463 - INFO - Usage log: Node the projectors, completion_usage: {'completion_tokens': 6, 'prompt_tokens': 398, 'total_tokens': 404, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.16108441352844238}\n",
      "2025-08-10 14:57:50,464 - INFO - Usage log: Node Taylor Cruz, completion_usage: {'completion_tokens': 6, 'prompt_tokens': 403, 'total_tokens': 409, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.1590867042541504}\n",
      "2025-08-10 14:57:50,465 - INFO - Usage log: Node unfailingly, completion_usage: {'completion_tokens': 7, 'prompt_tokens': 400, 'total_tokens': 407, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.18223118782043457}\n",
      "Shard_0:  67%|██████▋   | 2/3 [00:01<00:00,  1.44it/s]2025-08-10 14:57:50,665 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:50,687 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:50,712 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:50,828 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:50,875 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:50,922 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:50,992 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:51,106 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:51,198 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:51,290 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:51,338 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:51,497 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:51,520 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:51,567 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:51,713 - INFO - HTTP Request: POST http://0.0.0.0:8129/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 14:57:51,715 - INFO - Usage log: Node tilted their head, completion_usage: {'completion_tokens': 8, 'prompt_tokens': 274, 'total_tokens': 282, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.19976162910461426}\n",
      "2025-08-10 14:57:51,716 - INFO - Usage log: Node offered a supportive nod, completion_usage: {'completion_tokens': 8, 'prompt_tokens': 275, 'total_tokens': 283, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.21835613250732422}\n",
      "2025-08-10 14:57:51,716 - INFO - Usage log: Node agreed, completion_usage: {'completion_tokens': 9, 'prompt_tokens': 272, 'total_tokens': 281, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.24070119857788086}\n",
      "2025-08-10 14:57:51,717 - INFO - Usage log: Node outlining, completion_usage: {'completion_tokens': 6, 'prompt_tokens': 272, 'total_tokens': 278, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.1629652976989746}\n",
      "2025-08-10 14:57:51,717 - INFO - Usage log: Node scanned, completion_usage: {'completion_tokens': 9, 'prompt_tokens': 272, 'total_tokens': 281, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.23455119132995605}\n",
      "2025-08-10 14:57:51,718 - INFO - Usage log: Node enormous to, completion_usage: {'completion_tokens': 6, 'prompt_tokens': 273, 'total_tokens': 279, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.16263270378112793}\n",
      "2025-08-10 14:57:51,718 - INFO - Usage log: Node asked, completion_usage: {'completion_tokens': 6, 'prompt_tokens': 272, 'total_tokens': 278, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.16267132759094238}\n",
      "2025-08-10 14:57:51,719 - INFO - Usage log: Node reprimanded, completion_usage: {'completion_tokens': 9, 'prompt_tokens': 274, 'total_tokens': 283, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.23096084594726562}\n",
      "2025-08-10 14:57:51,719 - INFO - Usage log: Node revealed a spark of understanding, completion_usage: {'completion_tokens': 11, 'prompt_tokens': 276, 'total_tokens': 287, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.27586984634399414}\n",
      "2025-08-10 14:57:51,720 - INFO - Usage log: Node combed through the last transmission logs, completion_usage: {'completion_tokens': 12, 'prompt_tokens': 278, 'total_tokens': 290, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.2985377311706543}\n",
      "2025-08-10 14:57:51,720 - INFO - Usage log: Node took form within Agent Alex Mercer, completion_usage: {'completion_tokens': 9, 'prompt_tokens': 277, 'total_tokens': 286, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.23150014877319336}\n",
      "2025-08-10 14:57:51,721 - INFO - Usage log: Node straightened in his seat, completion_usage: {'completion_tokens': 12, 'prompt_tokens': 276, 'total_tokens': 288, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.2989840507507324}\n",
      "2025-08-10 14:57:51,724 - INFO - Usage log: Node determined on paper, completion_usage: {'completion_tokens': 11, 'prompt_tokens': 274, 'total_tokens': 285, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.2763478755950928}\n",
      "2025-08-10 14:57:51,724 - INFO - Usage log: Node began to collect his binders, completion_usage: {'completion_tokens': 7, 'prompt_tokens': 277, 'total_tokens': 284, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.18216967582702637}\n",
      "2025-08-10 14:57:51,725 - INFO - Usage log: Node responded, completion_usage: {'completion_tokens': 9, 'prompt_tokens': 272, 'total_tokens': 281, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'time': 0.21616435050964355}\n",
      "Shard_0: 100%|██████████| 3/3 [00:02<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique conceptualized nodes: 113\n",
      "Number of unique conceptualized events: 0\n",
      "Number of unique conceptualized entities: 64\n",
      "Number of unique conceptualized relations: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Concept Generation\n",
    "kg_extractor.generate_concept_csv_temp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5823e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading concepts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:00, 46422.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading concepts done.\n",
      "Relation to concepts: 15\n",
      "Node to concepts: 22\n",
      "Processing triple nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:00, 34302.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing concept nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 222953.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing triple edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 72232.56it/s]\n"
     ]
    }
   ],
   "source": [
    "kg_extractor.create_concept_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84480f15",
   "metadata": {},
   "source": [
    "# Choice 1: Convert to graphml for networkx rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "348f651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert csv to graphml for networkx\n",
    "kg_extractor.convert_to_graphml()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8f197d",
   "metadata": {},
   "source": [
    "## ATLAS Multihop QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b957c1",
   "metadata": {},
   "source": [
    "In order to perform RAG, one need to first create embeddings & faiss index for constructed KG\n",
    "\n",
    "[There maybe performance difference in using AutoModel and Sentence Transformer for NV-Ebmed-v2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d1aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from atlas_rag.vectorstore.embedding_model import NvEmbed, SentenceEmbedding\n",
    "from transformers import AutoModel\n",
    "# Load the SentenceTransformer model\n",
    "encoder_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "sentence_model = SentenceTransformer(encoder_model_name, trust_remote_code=True, model_kwargs={'device_map': \"auto\"})\n",
    "sentence_encoder = SentenceEmbedding(sentence_model)\n",
    "# sentence_model.max_seq_length = 32768\n",
    "# sentence_model.tokenizer.padding_side=\"right\"\n",
    "# sentence_model = AutoModel.from_pretrained(encoder_model_name, trust_remote_code=True, device_map=\"auto\")\n",
    "# sentence_encoder = NvEmbed(sentence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a106e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from atlas_rag.llm_generator import LLMGenerator\n",
    "from configparser import ConfigParser\n",
    "# Load OpenRouter API key from config file\n",
    "config = ConfigParser()\n",
    "config.read('config.ini')\n",
    "# reader_model_name = \"meta-llama/llama-3.3-70b-instruct\"\n",
    "reader_model_name = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "client = OpenAI(\n",
    "  # base_url=\"https://openrouter.ai/api/v1\",\n",
    "  # api_key=config['settings']['OPENROUTER_API_KEY'],\n",
    "  base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "  api_key=config['settings']['DEEPINFRA_API_KEY'],\n",
    ")\n",
    "llm_generator = LLMGenerator(client=client, model_name=reader_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ba40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlas_rag.vectorstore import create_embeddings_and_index\n",
    "keyword = 'CICGPC_Glazing_ver1.0a'\n",
    "working_directory = f'import/{keyword}'\n",
    "data = create_embeddings_and_index(\n",
    "    sentence_encoder=sentence_encoder,\n",
    "    model_name = encoder_model_name,\n",
    "    working_directory=working_directory,\n",
    "    keyword=keyword,\n",
    "    include_concept=True,\n",
    "    include_events=True,\n",
    "    normalize_embeddings= True,\n",
    "    text_batch_size=64,\n",
    "    node_and_edge_batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86997e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize desired RAG method for benchmarking\n",
    "from atlas_rag.retriever import HippoRAG2Retriever\n",
    "from atlas_rag import setup_logger\n",
    "\n",
    "hipporag2_retriever = HippoRAG2Retriever(\n",
    "    llm_generator=llm_generator,\n",
    "    sentence_encoder=sentence_encoder,\n",
    "    data = data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179b3850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform retrieval\n",
    "content, sorted_context_ids = hipporag2_retriever.retrieve(\"How is the U-value relevant to thermal insulation performance in glazing products?\", topN=3)\n",
    "print(f\"Retrieved content: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b515ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start benchmarking\n",
    "sorted_context = \"\\n\".join(content)\n",
    "llm_generator.generate_with_context(\"How is the U-value relevant to thermal insulation performance in glazing products?\", sorted_context, max_new_tokens=2048, temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5100a428",
   "metadata": {},
   "source": [
    "# Choice 2: Convert to neo4j dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d114689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from atlas_rag.vectorstore.embedding_model import SentenceEmbedding\n",
    "# use sentence embedding if you want to use sentence transformer\n",
    "# use NvEmbed if you want to use NvEmbed-v2 model\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "sentence_encoder = SentenceEmbedding(sentence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a4e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add numeric id to the csv so that we can use vector indices\n",
    "kg_extractor.add_numeric_id()\n",
    "\n",
    "# compute embedding\n",
    "kg_extractor.compute_kg_embedding(sentence_encoder) # default encoder_model_name=\"all-MiniLM-L12-v2\", only compute all embeddings except any concept related embeddings\n",
    "# kg_extractor.compute_embedding(encoder_model_name=\"all-MiniLM-L12-v2\")\n",
    "# kg_extractor.compute_embedding(encoder_model_name=\"nvidia/NV-Embed-v2\")\n",
    "\n",
    "# create faiss index\n",
    "kg_extractor.create_faiss_index(faiss_gpu=False) # default index_type=\"HNSW,Flat\", other options: \"IVF65536_HNSW32,Flat\" for large KG\n",
    "# kg_extractor.create_faiss_index(index_type=\"HNSW,Flat\")\n",
    "# kg_extractor.create_faiss_index(index_type=\"IVF65536_HNSW32,Flat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a89d518",
   "metadata": {},
   "source": [
    "## Install Neo4j Server\n",
    "\n",
    "Go to the AutoschemaKG/neo4j_scripts directory\n",
    "\n",
    "```sh get_neo4j_demo.sh```\n",
    "\n",
    "Then there a neo4j server is install in the directory: neo4j-server-dulce\n",
    "\n",
    "Start the newly instealled empty Neo4j server for testing\n",
    "\n",
    "```sh start_neo4j_demo.sh```\n",
    "\n",
    "\n",
    "\n",
    "## Config Neo4j Server\n",
    "\n",
    "Stop the server first before config and import data\n",
    "\n",
    "```sh stop_neo4j_demo.sh```\n",
    "\n",
    "\n",
    "Copy the ```AutoschemaKG/neo4j_scripts/neo4j.conf``` file to the conf directory of the Neo4j server (```neo4j-server-dulce/conf```). Then, update the following settings as needed: 1.Set dbms.default_database to the desired dataset name, such as ```wiki-csv-json-text```, ```pes2o-csv-json-text```, or ```cc-csv-json-text```. In this case we make it ```dulce-csv-json-text``` 2.Configure the Bolt, HTTP, and HTTPS connectors according to your requirements.\n",
    "\n",
    "I have set up the config port to some random ports to avoid port conflicts in ```neo4j-server-dulce/conf/neo4j.conf``` .\n",
    "\n",
    " \n",
    "``` \n",
    "# Bolt connector\n",
    "server.bolt.enabled=true\n",
    "#server.bolt.tls_level=DISABLED\n",
    "server.bolt.listen_address=0.0.0.0:8612\n",
    "server.bolt.advertised_address=:8612\n",
    "\n",
    "# HTTP Connector. There can be zero or one HTTP connectors.\n",
    "server.http.enabled=true\n",
    "server.http.listen_address=0.0.0.0:7612\n",
    "server.http.advertised_address=:7612\n",
    "\n",
    "# HTTPS Connector. There can be zero or one HTTPS connectors.\n",
    "server.https.enabled=false\n",
    "server.https.listen_address=0.0.0.0:7781\n",
    "server.https.advertised_address=:7781\n",
    "```\n",
    "\n",
    "\n",
    "## Import Data\n",
    "We use the admin import method to import data, which is the fastest way. Other methods are too slow for large graphs.\n",
    "\n",
    "\n",
    "## Load the CSV files into Neo4j\n",
    "\n",
    "We try to import data from previously constructed csv files with numeric ids. All the csv files are in ```import/Dulce```. \n",
    "In total six csv files for the nodes and edges of triples, text chunks, and concepts. \n",
    "\n",
    "``` shell\n",
    "./neo4j-server-dulce/bin/neo4j-admin database import full dulce-csv-json-text \\\n",
    "    --nodes ./import/Dulce/triples_csv/triple_nodes_Dulce_from_json_without_emb_with_numeric_id.csv \\\n",
    "    --nodes ./import/Dulce/triples_csv/text_nodes_Dulce_from_json_with_numeric_id.csv \\\n",
    "    --nodes ./import/Dulce/concept_csv/concept_nodes_Dulce_from_json_with_concept.csv \\\n",
    "    --relationships ./import/Dulce/triples_csv/triple_edges_Dulce_from_json_without_emb_with_numeric_id.csv \\\n",
    "    --relationships ./import/Dulce/triples_csv/text_edges_Dulce_from_json.csv \\\n",
    "    --relationships ./import/Dulce/concept_csv/concept_edges_Dulce_from_json_with_concept.csv  \\\n",
    "    --overwrite-destination \\\n",
    "    --multiline-fields=true \\\n",
    "    --id-type=string \\\n",
    "    --verbose --skip-bad-relationships=true\n",
    "```\n",
    "\n",
    "When this is finished, you can see the following notifications\n",
    "\n",
    "```shell\n",
    "IMPORT DONE in 2s 475ms. \n",
    "Imported:\n",
    "  1183 nodes\n",
    "  2519 relationships\n",
    "  6743 properties\n",
    "Peak memory usage: 1.032GiB\n",
    "```\n",
    "\n",
    "Then you can start host it by running in ```./neo4j_scripts```\n",
    "\n",
    "```sh start_neo4j_demo.sh```\n",
    "\n",
    "When you see the following line, then it is working well.\n",
    "\n",
    "\n",
    "```Started neo4j (pid:742490). It is available at http://0.0.0.0:7612```\n",
    "\n",
    "\n",
    "\n",
    "If you want to use the python driver to run neo4j, you need to use port 8612. You can access http://0.0.0.0:7612 in browser as well to use the neo4j GUI. \n",
    "\n",
    "The default user is ```neo4j``` with password ```admin2024```. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5e827",
   "metadata": {},
   "source": [
    "## ATLAS Billion Level RAG\n",
    "The LargeKGRetriever is designed to perform retrieval on a billion-level graph. \n",
    "\n",
    "There is a trade-off between retrieval performance and speed; this serves as a proof of concept for a billion-level knowledge graph.\n",
    "\n",
    "After successfully hosting the Neo4j database, you can run the provided Python script to host the RAG API:\n",
    "```shell\n",
    "python neo4j_api_host/atlas_api_demo.py \n",
    "```\n",
    "\n",
    "During the first startup of the API, it will create the necessary indexes and projection graphs in the Neo4j database for faster queries and computations. The time required for this process may vary depending on the size of the database. You can monitor the creation of these items in http://localhost:7612 by using the following commands:\n",
    "\n",
    "To view the projected graphs:\n",
    "```cypher\n",
    "CALL gds.graph.list()\n",
    "```\n",
    "To view the indexes:\n",
    "```cypher\n",
    "SHOW INDEXES\n",
    "```\n",
    "\n",
    "The projected graph will be deleted after the database is shut down, while the indexes will not be removed.\n",
    "\n",
    "After you saw: \\\n",
    "Index NodeNumericIDIndex created in 0.09 seconds \\\n",
    "Index TextNumericIDIndex created in 0.11 seconds \\\n",
    "Index EntityEventEdgeNumericIDIndex created in 0.02 seconds \\\n",
    "Projection graph largekgrag_graph created in 5.42 seconds \n",
    "\n",
    "You can perform rag as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a02dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "base_url =\"http://0.0.0.0:10085/v1/\"\n",
    "client = OpenAI(api_key=\"EMPTY\", base_url=base_url)\n",
    "\n",
    "# knowledge graph en_simple_wiki_v0\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that answers questions based on the knowledge graph.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Question: Who is Alex Mercer?\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama\",\n",
    "    messages=message,\n",
    "    max_tokens=2048,\n",
    "    temperature=0.5\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b5d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
