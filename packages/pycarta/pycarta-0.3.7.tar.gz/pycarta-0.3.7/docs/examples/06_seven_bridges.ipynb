{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pycarta Seven Bridges Integration\n",
    "\n",
    "This notebook demonstrates integration with Seven Bridges Genomics platform for computational workflows.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Valid Carta authentication (see `01_authentication.ipynb`)\n",
    "- Seven Bridges platform access and API token\n",
    "- Understanding of Seven Bridges Apps and Workflows\n",
    "- Valid Seven Bridges project access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycarta as pc\n",
    "from pycarta import get_agent\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure you're authenticated\n",
    "# pc.login()  # Uncomment and authenticate as needed\n",
    "\n",
    "print(\"Seven Bridges integration setup complete\")\n",
    "print(\"Note: Requires Seven Bridges platform access and credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seven Bridges Authentication\n",
    "\n",
    "Multiple ways to authenticate with Seven Bridges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_sbg_authentication():\n",
    "    \"\"\"Demonstrate Seven Bridges authentication methods.\"\"\"\n",
    "    print(\"\"\"\n",
    "    SEVEN BRIDGES AUTHENTICATION:\n",
    "    \n",
    "    1. Via Carta Authentication (Recommended):\n",
    "       If you have Seven Bridges credentials stored as Carta secrets\n",
    "       or configured via environment variables, pycarta login handles SBG automatically:\n",
    "       \n",
    "       import pycarta as pc\n",
    "       pc.login()\n",
    "       # You are now authorized to call SBG Apps and Workflows\n",
    "    \n",
    "    2. Via Environment Variables:\n",
    "       Set these environment variables:\n",
    "       \n",
    "       export SB_API_ENDPOINT=\"https://api.sbgenomics.com/v2/\"\n",
    "       export SB_AUTH_TOKEN=\"your_sbg_token_here\"\n",
    "    \n",
    "    3. Via Seven Bridges Credentials File:\n",
    "       Create ~/.sevenbridges/credentials with:\n",
    "       \n",
    "       [default]\n",
    "       api_endpoint = https://api.sbgenomics.com/v2/\n",
    "       auth_token = your_token_here\n",
    "    \n",
    "    4. Via Carta Secrets (for portability):\n",
    "       Store SBG credentials as Carta secrets:\n",
    "       \n",
    "       from pycarta.admin.secret import put_secret\n",
    "       put_secret(\"sbg_api_endpoint\", \"https://api.sbgenomics.com/v2/\")\n",
    "       put_secret(\"sbg_auth_token\", \"your_token_here\")\n",
    "    \n",
    "    Once authenticated through any method, you can access SBG API:\n",
    "    \n",
    "    agent = pc.get_agent()\n",
    "    sbg_api = agent.sbg_manager.api\n",
    "    \"\"\")\n",
    "\n",
    "demonstrate_sbg_authentication()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Seven Bridges Projects\n",
    "\n",
    "Explore and manage SBG projects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_sbg_projects():\n",
    "    \"\"\"Explore Seven Bridges projects and resources.\"\"\"\n",
    "    try:\n",
    "        # Get authenticated agent and SBG API\n",
    "        # agent = get_agent()\n",
    "        # sbg_api = agent.sbg_manager.api\n",
    "        \n",
    "        print(\"\"\"\n",
    "        SBG PROJECT EXPLORATION:\n",
    "        \n",
    "        # List available projects\n",
    "        projects = sbg_api.projects.query()\n",
    "        print(f\"Available projects: {len(projects)}\")\n",
    "        \n",
    "        for project in projects[:5]:  # Show first 5\n",
    "            print(f\"  - {project.name} ({project.id})\")\n",
    "        \n",
    "        # Get specific project\n",
    "        project = sbg_api.projects.get(\"division/my-project\")\n",
    "        print(f\"Project: {project.name}\")\n",
    "        print(f\"Description: {project.description}\")\n",
    "        \n",
    "        # List apps in project\n",
    "        apps = sbg_api.apps.query(project=project)\n",
    "        print(f\"Available apps: {len(apps)}\")\n",
    "        \n",
    "        for app in apps[:3]:  # Show first 3\n",
    "            print(f\"  - {app.name} (v{app.revision})\")\n",
    "        \n",
    "        # List files in project\n",
    "        files = sbg_api.files.query(project=project)\n",
    "        print(f\"Project files: {len(files)}\")\n",
    "        \n",
    "        for file in files[:3]:  # Show first 3\n",
    "            print(f\"  - {file.name} ({file.size} bytes)\")\n",
    "        \"\"\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"SBG exploration error: {e}\")\n",
    "        print(\"Note: Requires valid SBG authentication and project access\")\n",
    "\n",
    "explore_sbg_projects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExecutableApp - Single App Execution\n",
    "\n",
    "Convert Seven Bridges Apps to Python functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycarta.sbg import ExecutableApp\n",
    "\n",
    "def demonstrate_executable_app():\n",
    "    \"\"\"Demonstrate ExecutableApp usage.\"\"\"\n",
    "    print(\"\"\"\n",
    "    EXECUTABLE APP USAGE:\n",
    "    \n",
    "    # Step 1: Login and get SBG API\n",
    "    pc.login()\n",
    "    agent = get_agent()\n",
    "    sbg_api = agent.sbg_manager.api\n",
    "    \n",
    "    # Step 2: Retrieve a Seven Bridges App\n",
    "    sbg_app = sbg_api.apps.get(\"MyAppName\", project=\"division/project\")\n",
    "    print(f\"Retrieved app: {sbg_app.name}\")\n",
    "    \n",
    "    # Step 3: Create an ExecutableApp with configuration\n",
    "    app = ExecutableApp(\n",
    "        sbg_app,\n",
    "        cleanup=True,          # Delete uploaded/downloaded files after execution\n",
    "        polling_freq=5.0,      # Check task status every 5 seconds\n",
    "        overwrite_local=True,  # Overwrite local files with results\n",
    "        overwrite_remote=True, # Overwrite remote files when uploading\n",
    "        strict=True,           # Strict type checking (recommended)\n",
    "    )\n",
    "    \n",
    "    # Step 4: Execute the app as a Python function\n",
    "    # Example: app that takes an input file and a parameter\n",
    "    result = app(\n",
    "        input_file=\"data/myfile.csv\",  # Local file - will be uploaded\n",
    "        num_iterations=42,             # App parameter\n",
    "        output_format=\"json\"           # App parameter\n",
    "    )\n",
    "    \n",
    "    print(f\"App execution complete\")\n",
    "    print(f\"Result files downloaded to: {result}\")\n",
    "    \n",
    "    # Step 5: Access app documentation\n",
    "    print(f\"App description: {app.description}\")\n",
    "    print(f\"App inputs: {app.inputs}\")\n",
    "    print(f\"App outputs: {app.outputs}\")\n",
    "    \"\"\")\n",
    "\n",
    "def demonstrate_app_configuration():\n",
    "    \"\"\"Demonstrate ExecutableApp configuration options.\"\"\"\n",
    "    print(\"\"\"\n",
    "    EXECUTABLE APP CONFIGURATION:\n",
    "    \n",
    "    Configuration Parameters:\n",
    "    \n",
    "    - cleanup (bool): \n",
    "        True: Delete uploaded files and downloaded results after execution\n",
    "        False: Keep files for manual inspection\n",
    "        Default: True (recommended for production)\n",
    "    \n",
    "    - polling_freq (float):\n",
    "        How often to check if the SBG task is still running (seconds)\n",
    "        Minimum: 3 seconds\n",
    "        Default: 10 seconds\n",
    "        Recommendation: 5-30 seconds depending on app duration\n",
    "    \n",
    "    - overwrite_local (bool):\n",
    "        True: Overwrite local files with downloaded results\n",
    "        False: Raise error if local files exist\n",
    "        Default: False\n",
    "    \n",
    "    - overwrite_remote (bool):\n",
    "        True: Overwrite remote files when uploading\n",
    "        False: Use existing remote files if they exist\n",
    "        Default: False\n",
    "    \n",
    "    - strict (bool):\n",
    "        True: Strict type checking based on CWL specification\n",
    "        False: Relaxed type checking (Python duck typing)\n",
    "        Default: True (recommended - catches errors early)\n",
    "    \n",
    "    Example configurations for different use cases:\n",
    "    \n",
    "    # Development/debugging\n",
    "    dev_app = ExecutableApp(sbg_app, \n",
    "                           cleanup=False,        # Keep files for inspection\n",
    "                           polling_freq=3.0,     # Check frequently\n",
    "                           overwrite_local=True, # Allow overwrites\n",
    "                           strict=False)         # Relaxed for testing\n",
    "    \n",
    "    # Production pipeline\n",
    "    prod_app = ExecutableApp(sbg_app,\n",
    "                            cleanup=True,         # Clean up resources\n",
    "                            polling_freq=30.0,    # Check less frequently\n",
    "                            overwrite_local=False, # Prevent accidents\n",
    "                            strict=True)          # Strict validation\n",
    "    \n",
    "    # Long-running analysis\n",
    "    analysis_app = ExecutableApp(sbg_app,\n",
    "                                cleanup=True,\n",
    "                                polling_freq=60.0,  # Check every minute\n",
    "                                overwrite_remote=False, # Reuse uploads\n",
    "                                strict=True)\n",
    "    \"\"\")\n",
    "\n",
    "demonstrate_executable_app()\n",
    "demonstrate_app_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExecutableProject - Project-Wide Access\n",
    "\n",
    "Convert entire Seven Bridges projects to Python classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycarta.sbg import ExecutableProject\n",
    "\n",
    "def demonstrate_executable_project():\n",
    "    \"\"\"Demonstrate ExecutableProject usage.\"\"\"\n",
    "    print(\"\"\"\n",
    "    EXECUTABLE PROJECT USAGE:\n",
    "    \n",
    "    # Step 1: Login to pycarta (handles SBG auth automatically)\n",
    "    pc.login()\n",
    "    \n",
    "    # Step 2: Create ExecutableProject\n",
    "    sandbox = ExecutableProject(project=\"division/sandbox\")\n",
    "    \n",
    "    # Step 3: Explore available apps\n",
    "    # The project becomes a dynamic class with methods for each app\n",
    "    print(f\"Available apps in project:\")\n",
    "    for app_name in sandbox.apps:\n",
    "        app = getattr(sandbox, app_name)\n",
    "        print(f\"  - {app_name}: {app.__doc__}\")\n",
    "    \n",
    "    # Step 4: Execute apps as methods\n",
    "    # Example: if project has a \"hello_world\" app\n",
    "    result = sandbox.hello_world(\n",
    "        greeting=\"Hello from pycarta!\",\n",
    "        output_format=\"text\"\n",
    "    )\n",
    "    \n",
    "    # Example: if project has a data processing app\n",
    "    analysis_result = sandbox.data_processor(\n",
    "        input_data=\"dataset.csv\",\n",
    "        analysis_type=\"statistical\",\n",
    "        parameters={\"confidence\": 0.95, \"method\": \"t-test\"}\n",
    "    )\n",
    "    \n",
    "    print(f\"Analysis complete: {analysis_result}\")\n",
    "    \"\"\")\n",
    "\n",
    "def demonstrate_project_features():\n",
    "    \"\"\"Demonstrate ExecutableProject advanced features.\"\"\"\n",
    "    print(\"\"\"\n",
    "    EXECUTABLE PROJECT FEATURES:\n",
    "    \n",
    "    # Introspection and documentation\n",
    "    project = ExecutableProject(\"division/my-project\")\n",
    "    \n",
    "    # List all available apps\n",
    "    apps = project.list_apps()\n",
    "    print(f\"Available apps: {apps}\")\n",
    "    \n",
    "    # Get app documentation\n",
    "    app_doc = project.get_app_documentation(\"my_analysis_app\")\n",
    "    print(f\"App documentation:\\n{app_doc}\")\n",
    "    \n",
    "    # Get app signature (inputs/outputs)\n",
    "    signature = project.get_app_signature(\"my_analysis_app\")\n",
    "    print(f\"Inputs: {signature['inputs']}\")\n",
    "    print(f\"Outputs: {signature['outputs']}\")\n",
    "    \n",
    "    # Execute with configuration\n",
    "    project.configure(\n",
    "        cleanup=True,\n",
    "        polling_freq=15.0,\n",
    "        overwrite_local=True\n",
    "    )\n",
    "    \n",
    "    # Chain multiple apps in a workflow\n",
    "    # Step 1: Preprocess data\n",
    "    preprocessed = project.data_preprocessor(\n",
    "        raw_data=\"input.csv\",\n",
    "        normalize=True,\n",
    "        remove_outliers=True\n",
    "    )\n",
    "    \n",
    "    # Step 2: Run analysis on preprocessed data\n",
    "    results = project.statistical_analysis(\n",
    "        processed_data=preprocessed,\n",
    "        test_type=\"anova\",\n",
    "        alpha=0.05\n",
    "    )\n",
    "    \n",
    "    # Step 3: Generate report\n",
    "    report = project.report_generator(\n",
    "        analysis_results=results,\n",
    "        format=\"pdf\",\n",
    "        include_plots=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Workflow complete: {report}\")\n",
    "    \"\"\")\n",
    "\n",
    "demonstrate_executable_project()\n",
    "demonstrate_project_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Management\n",
    "\n",
    "Automatic file upload/download handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_file_management():\n",
    "    \"\"\"Demonstrate automatic file management in SBG integration.\"\"\"\n",
    "    print(\"\"\"\n",
    "    AUTOMATIC FILE MANAGEMENT:\n",
    "    \n",
    "    pycarta.sbg handles file upload/download automatically:\n",
    "    \n",
    "    1. File Upload (Input Files):\n",
    "       - Local files are automatically uploaded to SBG before execution\n",
    "       - Files are uploaded to the project workspace\n",
    "       - Duplicate files are handled based on overwrite_remote setting\n",
    "       - Upload progress can be monitored\n",
    "    \n",
    "    # Example: Local file will be uploaded automatically\n",
    "    result = project.sequence_aligner(\n",
    "        reference_genome=\"/local/path/hg38.fa\",     # Will be uploaded\n",
    "        reads=\"/local/path/sample_reads.fastq\",     # Will be uploaded\n",
    "        output_format=\"bam\"\n",
    "    )\n",
    "    \n",
    "    2. File Download (Output Files):\n",
    "       - Result files are automatically downloaded after execution\n",
    "       - Files are downloaded to local working directory\n",
    "       - Download location can be customized\n",
    "       - Overwrites handled based on overwrite_local setting\n",
    "    \n",
    "    # Result contains paths to downloaded files\n",
    "    print(f\"Downloaded files: {result}\")\n",
    "    # Example output: {\n",
    "    #   \"aligned_reads\": \"/local/output/aligned.bam\",\n",
    "    #   \"alignment_stats\": \"/local/output/stats.txt\"\n",
    "    # }\n",
    "    \n",
    "    3. File Management Options:\n",
    "    \n",
    "    # Custom download directory\n",
    "    app = ExecutableApp(sbg_app, download_dir=\"/custom/output/path\")\n",
    "    \n",
    "    # Keep remote files for reuse\n",
    "    app = ExecutableApp(sbg_app, cleanup=False)\n",
    "    \n",
    "    # Handle large files efficiently\n",
    "    app = ExecutableApp(sbg_app, \n",
    "                       chunk_size=1024*1024,  # 1MB chunks\n",
    "                       parallel_uploads=4)     # Parallel upload streams\n",
    "    \n",
    "    4. File Type Handling:\n",
    "    \n",
    "    # Single files\n",
    "    result = app(input_file=\"data.csv\")\n",
    "    \n",
    "    # Multiple files\n",
    "    result = app(input_files=[\"file1.txt\", \"file2.txt\", \"file3.txt\"])\n",
    "    \n",
    "    # Mixed inputs (files and parameters)\n",
    "    result = app(\n",
    "        input_file=\"data.csv\",\n",
    "        reference_file=\"reference.txt\",\n",
    "        threshold=0.05,\n",
    "        output_format=\"json\"\n",
    "    )\n",
    "    \n",
    "    5. Error Handling:\n",
    "    \n",
    "    try:\n",
    "        result = app(input_file=\"nonexistent.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Input file not found\")\n",
    "    except PermissionError:\n",
    "        print(\"Permission denied for file access\")\n",
    "    except Exception as e:\n",
    "        print(f\"Execution error: {e}\")\n",
    "    \"\"\")\n",
    "\n",
    "def demonstrate_advanced_file_operations():\n",
    "    \"\"\"Demonstrate advanced file operations.\"\"\"\n",
    "    print(\"\"\"\n",
    "    ADVANCED FILE OPERATIONS:\n",
    "    \n",
    "    1. Working with Large Files:\n",
    "    \n",
    "    # Configure for large file handling\n",
    "    large_file_app = ExecutableApp(sbg_app,\n",
    "                                  chunk_size=10*1024*1024,  # 10MB chunks\n",
    "                                  timeout=3600,             # 1 hour timeout\n",
    "                                  retry_attempts=3)         # Retry on failure\n",
    "    \n",
    "    result = large_file_app(\n",
    "        large_dataset=\"/path/to/100GB_dataset.h5\",\n",
    "        processing_type=\"genomic_analysis\"\n",
    "    )\n",
    "    \n",
    "    2. Batch Processing:\n",
    "    \n",
    "    # Process multiple files in batch\n",
    "    batch_files = [\n",
    "        \"sample_001.fastq\",\n",
    "        \"sample_002.fastq\", \n",
    "        \"sample_003.fastq\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for file in batch_files:\n",
    "        result = project.sequence_processor(\n",
    "            input_file=file,\n",
    "            quality_threshold=30,\n",
    "            trim_adapters=True\n",
    "        )\n",
    "        results.append(result)\n",
    "    \n",
    "    3. Workflow Chaining with Files:\n",
    "    \n",
    "    # Step 1: Quality control\n",
    "    qc_result = project.quality_control(\n",
    "        raw_reads=\"sample.fastq\",\n",
    "        min_quality=20\n",
    "    )\n",
    "    \n",
    "    # Step 2: Use QC output as input for alignment\n",
    "    alignment_result = project.read_aligner(\n",
    "        cleaned_reads=qc_result[\"cleaned_reads\"],  # Output from step 1\n",
    "        reference_genome=\"hg38.fa\",\n",
    "        alignment_method=\"bwa\"\n",
    "    )\n",
    "    \n",
    "    # Step 3: Variant calling on aligned reads\n",
    "    variants = project.variant_caller(\n",
    "        aligned_reads=alignment_result[\"alignment_file\"],  # Output from step 2\n",
    "        reference_genome=\"hg38.fa\",\n",
    "        min_coverage=10\n",
    "    )\n",
    "    \n",
    "    4. Custom File Handling:\n",
    "    \n",
    "    # Custom preprocessing before upload\n",
    "    def preprocess_file(filepath):\n",
    "        # Custom file processing logic\n",
    "        processed_path = filepath.replace('.txt', '_processed.txt')\n",
    "        # ... processing code ...\n",
    "        return processed_path\n",
    "    \n",
    "    preprocessed_file = preprocess_file(\"raw_data.txt\")\n",
    "    result = app(input_file=preprocessed_file)\n",
    "    \n",
    "    # Custom postprocessing after download\n",
    "    def postprocess_results(result_files):\n",
    "        # Custom result processing\n",
    "        for file_path in result_files.values():\n",
    "            # ... postprocessing code ...\n",
    "            pass\n",
    "        return \"Processing complete\"\n",
    "    \n",
    "    postprocess_results(result)\n",
    "    \"\"\")\n",
    "\n",
    "demonstrate_file_management()\n",
    "demonstrate_advanced_file_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Tracking and Monitoring\n",
    "\n",
    "Monitor execution progress and handle long-running tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_progress_tracking():\n",
    "    \"\"\"Demonstrate progress tracking for SBG tasks.\"\"\"\n",
    "    print(\"\"\"\n",
    "    PROGRESS TRACKING AND MONITORING:\n",
    "    \n",
    "    1. Built-in Progress Monitoring:\n",
    "    \n",
    "    # ExecutableApp automatically polls task status\n",
    "    app = ExecutableApp(sbg_app, \n",
    "                       polling_freq=10.0,    # Check every 10 seconds\n",
    "                       verbose=True)         # Enable progress output\n",
    "    \n",
    "    result = app(input_file=\"large_dataset.csv\")\n",
    "    # Output:\n",
    "    # Task submitted: task_id_12345\n",
    "    # Status: QUEUED (0:00:15)\n",
    "    # Status: RUNNING (0:02:30)\n",
    "    # Status: RUNNING (0:05:45)\n",
    "    # Status: COMPLETED (0:08:20)\n",
    "    # Downloading results...\n",
    "    # Execution complete\n",
    "    \n",
    "    2. Custom Progress Callbacks:\n",
    "    \n",
    "    def progress_callback(task_status, elapsed_time):\n",
    "        print(f\"[{elapsed_time}] Task status: {task_status}\")\n",
    "        \n",
    "        if task_status == \"RUNNING\":\n",
    "            print(\"  Processing data...\")\n",
    "        elif task_status == \"QUEUED\":\n",
    "            print(\"  Waiting for resources...\")\n",
    "        elif task_status == \"COMPLETED\":\n",
    "            print(\"  ✓ Task completed successfully\")\n",
    "        elif task_status == \"FAILED\":\n",
    "            print(\"  ✗ Task failed\")\n",
    "    \n",
    "    app = ExecutableApp(sbg_app, progress_callback=progress_callback)\n",
    "    result = app(input_file=\"data.csv\")\n",
    "    \n",
    "    3. Long-Running Task Management:\n",
    "    \n",
    "    # For very long tasks, save task ID for later retrieval\n",
    "    app = ExecutableApp(sbg_app, \n",
    "                       save_task_info=True,   # Save task metadata\n",
    "                       task_info_file=\"my_task.json\")  # Custom save location\n",
    "    \n",
    "    # Start task (returns immediately if async=True)\n",
    "    task_info = app.start_async(\n",
    "        input_file=\"huge_dataset.csv\",\n",
    "        analysis_type=\"comprehensive\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Task started: {task_info['task_id']}\")\n",
    "    print(f\"You can check status later with: app.check_status('{task_info['task_id']}')\")\n",
    "    \n",
    "    # Later, check status and retrieve results\n",
    "    status = app.check_status(task_info['task_id'])\n",
    "    if status == \"COMPLETED\":\n",
    "        results = app.download_results(task_info['task_id'])\n",
    "    \n",
    "    4. Error Handling and Retry Logic:\n",
    "    \n",
    "    app = ExecutableApp(sbg_app,\n",
    "                       max_retries=3,           # Retry failed tasks\n",
    "                       retry_delay=60,          # Wait 60s between retries\n",
    "                       timeout=7200)            # 2 hour maximum execution time\n",
    "    \n",
    "    try:\n",
    "        result = app(input_file=\"problematic_data.csv\")\n",
    "    except TimeoutError:\n",
    "        print(\"Task timed out after 2 hours\")\n",
    "    except ExecutionError as e:\n",
    "        print(f\"Task failed after {app.max_retries} retries: {e}\")\n",
    "        # Access error details\n",
    "        print(f\"Error log: {e.error_log}\")\n",
    "        print(f\"Failed task ID: {e.task_id}\")\n",
    "    \n",
    "    5. Resource Monitoring:\n",
    "    \n",
    "    # Monitor resource usage during execution\n",
    "    app = ExecutableApp(sbg_app, monitor_resources=True)\n",
    "    \n",
    "    result = app(input_file=\"data.csv\")\n",
    "    \n",
    "    # Access resource usage statistics\n",
    "    stats = app.get_execution_stats()\n",
    "    print(f\"Execution time: {stats['duration']}\")\n",
    "    print(f\"CPU usage: {stats['cpu_hours']}\")\n",
    "    print(f\"Memory peak: {stats['max_memory_gb']} GB\")\n",
    "    print(f\"Storage used: {stats['storage_gb']} GB\")\n",
    "    print(f\"Cost estimate: ${stats['estimated_cost']}\")\n",
    "    \"\"\")\n",
    "\n",
    "demonstrate_progress_tracking()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Example: Genomics Pipeline\n",
    "\n",
    "Complete example of a genomics analysis pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genomics_pipeline_example():\n",
    "    \"\"\"Complete genomics analysis pipeline example.\"\"\"\n",
    "    print(\"\"\"\n",
    "    GENOMICS ANALYSIS PIPELINE:\n",
    "    \n",
    "    This example demonstrates a complete genomics workflow using\n",
    "    Seven Bridges Apps through pycarta integration.\n",
    "    \n",
    "    # Setup\n",
    "    import pycarta as pc\n",
    "    from pycarta.sbg import ExecutableProject\n",
    "    import os\n",
    "    \n",
    "    # Authenticate\n",
    "    pc.login()\n",
    "    \n",
    "    # Connect to genomics project\n",
    "    genomics_project = ExecutableProject(project=\"my-org/genomics-pipeline\")\n",
    "    \n",
    "    # Sample metadata\n",
    "    samples = [\n",
    "        {\n",
    "            \"sample_id\": \"SAMPLE_001\",\n",
    "            \"reads_1\": \"data/sample_001_R1.fastq.gz\",\n",
    "            \"reads_2\": \"data/sample_001_R2.fastq.gz\",\n",
    "            \"phenotype\": \"case\"\n",
    "        },\n",
    "        {\n",
    "            \"sample_id\": \"SAMPLE_002\", \n",
    "            \"reads_1\": \"data/sample_002_R1.fastq.gz\",\n",
    "            \"reads_2\": \"data/sample_002_R2.fastq.gz\",\n",
    "            \"phenotype\": \"control\"\n",
    "        },\n",
    "        {\n",
    "            \"sample_id\": \"SAMPLE_003\",\n",
    "            \"reads_1\": \"data/sample_003_R1.fastq.gz\",\n",
    "            \"reads_2\": \"data/sample_003_R2.fastq.gz\",\n",
    "            \"phenotype\": \"case\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Reference files\n",
    "    reference_genome = \"references/hg38.fa\"\n",
    "    known_variants = \"references/dbsnp.vcf\"\n",
    "    \n",
    "    # Step 1: Quality Control\n",
    "    print(\"Step 1: Running quality control...\")\n",
    "    qc_results = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        qc_result = genomics_project.fastqc_quality_control(\n",
    "            reads_1=sample[\"reads_1\"],\n",
    "            reads_2=sample[\"reads_2\"],\n",
    "            sample_id=sample[\"sample_id\"]\n",
    "        )\n",
    "        qc_results.append(qc_result)\n",
    "        print(f\"  QC complete for {sample['sample_id']}\")\n",
    "    \n",
    "    # Step 2: Read Trimming and Filtering\n",
    "    print(\"Step 2: Trimming and filtering reads...\")\n",
    "    trimmed_results = []\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        trimmed_result = genomics_project.trimmomatic_trimmer(\n",
    "            reads_1=sample[\"reads_1\"],\n",
    "            reads_2=sample[\"reads_2\"],\n",
    "            quality_threshold=20,\n",
    "            min_length=50,\n",
    "            adapter_file=\"adapters/TruSeq3-PE.fa\"\n",
    "        )\n",
    "        trimmed_results.append(trimmed_result)\n",
    "        print(f\"  Trimming complete for {sample['sample_id']}\")\n",
    "    \n",
    "    # Step 3: Read Alignment\n",
    "    print(\"Step 3: Aligning reads to reference genome...\")\n",
    "    alignment_results = []\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        alignment_result = genomics_project.bwa_mem_aligner(\n",
    "            reads_1=trimmed_results[i][\"trimmed_reads_1\"],\n",
    "            reads_2=trimmed_results[i][\"trimmed_reads_2\"],\n",
    "            reference_genome=reference_genome,\n",
    "            sample_id=sample[\"sample_id\"],\n",
    "            read_group_info=f\"@RG\\\\tID:{sample['sample_id']}\\\\tSM:{sample['sample_id']}\"\n",
    "        )\n",
    "        alignment_results.append(alignment_result)\n",
    "        print(f\"  Alignment complete for {sample['sample_id']}\")\n",
    "    \n",
    "    # Step 4: Post-alignment Processing\n",
    "    print(\"Step 4: Post-alignment processing...\")\n",
    "    processed_bams = []\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        # Sort and mark duplicates\n",
    "        processed_bam = genomics_project.picard_process_bam(\n",
    "            input_bam=alignment_results[i][\"aligned_bam\"],\n",
    "            reference_genome=reference_genome,\n",
    "            mark_duplicates=True,\n",
    "            sort_order=\"coordinate\"\n",
    "        )\n",
    "        \n",
    "        # Base quality score recalibration\n",
    "        recalibrated_bam = genomics_project.gatk_base_recalibration(\n",
    "            input_bam=processed_bam[\"processed_bam\"],\n",
    "            reference_genome=reference_genome,\n",
    "            known_sites=known_variants\n",
    "        )\n",
    "        \n",
    "        processed_bams.append(recalibrated_bam)\n",
    "        print(f\"  Processing complete for {sample['sample_id']}\")\n",
    "    \n",
    "    # Step 5: Variant Calling\n",
    "    print(\"Step 5: Calling variants...\")\n",
    "    variant_results = []\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        variants = genomics_project.gatk_haplotype_caller(\n",
    "            input_bam=processed_bams[i][\"recalibrated_bam\"],\n",
    "            reference_genome=reference_genome,\n",
    "            sample_id=sample[\"sample_id\"],\n",
    "            emit_ref_confidence=\"GVCF\",\n",
    "            min_base_quality=20\n",
    "        )\n",
    "        variant_results.append(variants)\n",
    "        print(f\"  Variant calling complete for {sample['sample_id']}\")\n",
    "    \n",
    "    # Step 6: Joint Genotyping\n",
    "    print(\"Step 6: Joint genotyping...\")\n",
    "    gvcf_files = [result[\"output_gvcf\"] for result in variant_results]\n",
    "    \n",
    "    joint_vcf = genomics_project.gatk_joint_genotyping(\n",
    "        input_gvcfs=gvcf_files,\n",
    "        reference_genome=reference_genome,\n",
    "        output_name=\"cohort_variants\"\n",
    "    )\n",
    "    \n",
    "    # Step 7: Variant Filtering\n",
    "    print(\"Step 7: Filtering variants...\")\n",
    "    filtered_vcf = genomics_project.gatk_variant_filtration(\n",
    "        input_vcf=joint_vcf[\"joint_vcf\"],\n",
    "        reference_genome=reference_genome,\n",
    "        filter_expressions=[\n",
    "            \"QD < 2.0\",\n",
    "            \"FS > 60.0\", \n",
    "            \"MQ < 40.0\",\n",
    "            \"ReadPosRankSum < -8.0\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Step 8: Annotation\n",
    "    print(\"Step 8: Annotating variants...\")\n",
    "    annotated_vcf = genomics_project.snpeff_annotator(\n",
    "        input_vcf=filtered_vcf[\"filtered_vcf\"],\n",
    "        genome_version=\"hg38\",\n",
    "        include_statistics=True\n",
    "    )\n",
    "    \n",
    "    # Step 9: Statistical Analysis\n",
    "    print(\"Step 9: Statistical analysis...\")\n",
    "    \n",
    "    # Prepare phenotype file\n",
    "    phenotype_data = \"\\n\".join([\n",
    "        \"sample_id\\tphenotype\",\n",
    "        *[f\"{s['sample_id']}\\t{s['phenotype']}\" for s in samples]\n",
    "    ])\n",
    "    \n",
    "    with open(\"phenotypes.txt\", \"w\") as f:\n",
    "        f.write(phenotype_data)\n",
    "    \n",
    "    # Association analysis\n",
    "    association_results = genomics_project.plink_association(\n",
    "        input_vcf=annotated_vcf[\"annotated_vcf\"],\n",
    "        phenotype_file=\"phenotypes.txt\",\n",
    "        test_type=\"logistic\",\n",
    "        significance_threshold=5e-8\n",
    "    )\n",
    "    \n",
    "    # Step 10: Generate Report\n",
    "    print(\"Step 10: Generating analysis report...\")\n",
    "    final_report = genomics_project.analysis_reporter(\n",
    "        variant_file=annotated_vcf[\"annotated_vcf\"],\n",
    "        association_results=association_results[\"association_stats\"],\n",
    "        qc_metrics=qc_results,\n",
    "        sample_info=samples,\n",
    "        report_format=\"html\",\n",
    "        include_plots=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nGenomics pipeline complete!\")\n",
    "    print(f\"Final report: {final_report['report_html']}\")\n",
    "    print(f\"Significant variants: {association_results['significant_variants']}\")\n",
    "    \n",
    "    # Return summary\n",
    "    return {\n",
    "        \"samples_processed\": len(samples),\n",
    "        \"variants_called\": joint_vcf[\"variant_count\"],\n",
    "        \"filtered_variants\": filtered_vcf[\"filtered_count\"],\n",
    "        \"significant_associations\": association_results[\"significant_count\"],\n",
    "        \"report_file\": final_report[\"report_html\"]\n",
    "    }\n",
    "    \"\"\")\n",
    "\n",
    "genomics_pipeline_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Other pycarta Modules\n",
    "\n",
    "Combining Seven Bridges with other pycarta features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_integration():\n",
    "    \"\"\"Demonstrate integration with other pycarta modules.\"\"\"\n",
    "    print(\"\"\"\n",
    "    INTEGRATION WITH OTHER PYCARTA MODULES:\n",
    "    \n",
    "    1. With FormsDB (Data Management):\n",
    "    \n",
    "    import pycarta as pc\n",
    "    from pycarta.formsdb import FormsDb\n",
    "    from pycarta.sbg import ExecutableProject\n",
    "    \n",
    "    # Store analysis metadata in FormsDB\n",
    "    pc.login()\n",
    "    formsdb = FormsDb(credentials=pc.get_agent(), project_id=\"genomics\")\n",
    "    genomics = ExecutableProject(\"division/genomics\")\n",
    "    \n",
    "    # Create schema for analysis tracking\n",
    "    analysis_schema = formsdb.schema.create(\"sbg-analysis\", {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"analysis_id\": {\"type\": \"string\"},\n",
    "            \"sample_ids\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "            \"pipeline_version\": {\"type\": \"string\"},\n",
    "            \"start_time\": {\"type\": \"string\", \"format\": \"date-time\"},\n",
    "            \"end_time\": {\"type\": \"string\", \"format\": \"date-time\"},\n",
    "            \"results\": {\"type\": \"object\"},\n",
    "            \"resource_usage\": {\"type\": \"object\"}\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # Run analysis and store metadata\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    result = genomics.variant_caller(\n",
    "        input_bam=\"sample.bam\",\n",
    "        reference=\"hg38.fa\"\n",
    "    )\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    # Store analysis record\n",
    "    analysis_record = {\n",
    "        \"analysis_id\": \"ANALYSIS_001\",\n",
    "        \"sample_ids\": [\"SAMPLE_001\"],\n",
    "        \"pipeline_version\": \"v2.1\",\n",
    "        \"start_time\": start_time.isoformat(),\n",
    "        \"end_time\": end_time.isoformat(),\n",
    "        \"results\": result,\n",
    "        \"resource_usage\": genomics.get_last_execution_stats()\n",
    "    }\n",
    "    \n",
    "    folder = formsdb.folder.create(\"genomics/analyses\")\n",
    "    formsdb.data.create(folder, analysis_schema, analysis_record)\n",
    "    \n",
    "    2. With Services (API Creation):\n",
    "    \n",
    "    import pycarta as pc\n",
    "    from pycarta.sbg import ExecutableProject\n",
    "    \n",
    "    # Create service that wraps SBG functionality\n",
    "    genomics = ExecutableProject(\"division/genomics\")\n",
    "    service = pc.service(\"genomics-api\", \"variant-calling\")\n",
    "    \n",
    "    @service.post(\"/analyze/variants\")\n",
    "    def analyze_variants(sample_id: str, bam_file: str, reference: str = \"hg38\"):\n",
    "        \"\"\"Run variant calling analysis via Seven Bridges.\"\"\"\n",
    "        try:\n",
    "            result = genomics.variant_caller(\n",
    "                input_bam=bam_file,\n",
    "                reference_genome=f\"references/{reference}.fa\",\n",
    "                sample_id=sample_id\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"sample_id\": sample_id,\n",
    "                \"variant_file\": result[\"output_vcf\"],\n",
    "                \"stats\": result[\"stats\"]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    @service.get(\"/status/{analysis_id}\")\n",
    "    def get_analysis_status(analysis_id: str):\n",
    "        \"\"\"Get status of running analysis.\"\"\"\n",
    "        status = genomics.check_task_status(analysis_id)\n",
    "        return {\"analysis_id\": analysis_id, \"status\": status}\n",
    "    \n",
    "    3. With MQTT (Real-time Updates):\n",
    "    \n",
    "    from pycarta.mqtt import publish\n",
    "    from pycarta.sbg import ExecutableProject\n",
    "    \n",
    "    genomics = ExecutableProject(\"division/genomics\")\n",
    "    \n",
    "    @publish(\"genomics/analysis/status\")\n",
    "    def run_analysis_with_updates(sample_id, input_files):\n",
    "        \"\"\"Run analysis with real-time status updates.\"\"\"\n",
    "        \n",
    "        # Publish start notification\n",
    "        start_msg = {\n",
    "            \"sample_id\": sample_id,\n",
    "            \"status\": \"started\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Run analysis\n",
    "            result = genomics.variant_caller(**input_files)\n",
    "            \n",
    "            # Publish completion\n",
    "            return {\n",
    "                \"sample_id\": sample_id,\n",
    "                \"status\": \"completed\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"results\": result\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Publish error\n",
    "            return {\n",
    "                \"sample_id\": sample_id,\n",
    "                \"status\": \"failed\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    4. With Secrets Management:\n",
    "    \n",
    "    from pycarta.admin.secret import get_secret, put_secret\n",
    "    \n",
    "    # Store SBG credentials securely\n",
    "    put_secret(\"sbg_token\", \"your_sbg_token_here\")\n",
    "    put_secret(\"sbg_project\", \"division/my-project\")\n",
    "    \n",
    "    # Use stored credentials\n",
    "    sbg_token = get_secret(\"sbg_token\")\n",
    "    sbg_project = get_secret(\"sbg_project\")\n",
    "    \n",
    "    # Configure SBG connection\n",
    "    genomics = ExecutableProject(\n",
    "        project=sbg_project,\n",
    "        token=sbg_token\n",
    "    )\n",
    "    \n",
    "    5. Complete Integrated Workflow:\n",
    "    \n",
    "    # Comprehensive workflow combining all modules\n",
    "    def integrated_genomics_workflow(sample_metadata):\n",
    "        # 1. Store sample metadata in FormsDB\n",
    "        sample_record = formsdb.data.create(\n",
    "            folder, sample_schema, sample_metadata\n",
    "        )\n",
    "        \n",
    "        # 2. Run analysis on Seven Bridges\n",
    "        analysis_result = genomics.comprehensive_pipeline(\n",
    "            **sample_metadata[\"files\"]\n",
    "        )\n",
    "        \n",
    "        # 3. Publish progress via MQTT\n",
    "        publish_analysis_update({\n",
    "            \"sample_id\": sample_metadata[\"id\"],\n",
    "            \"status\": \"analysis_complete\",\n",
    "            \"results_summary\": analysis_result[\"summary\"]\n",
    "        })\n",
    "        \n",
    "        # 4. Store results back in FormsDB\n",
    "        results_record = {\n",
    "            \"sample_id\": sample_metadata[\"id\"],\n",
    "            \"analysis_date\": datetime.now().isoformat(),\n",
    "            \"results\": analysis_result,\n",
    "            \"quality_metrics\": analysis_result[\"qc\"]\n",
    "        }\n",
    "        \n",
    "        formsdb.data.create(\n",
    "            results_folder, results_schema, results_record\n",
    "        )\n",
    "        \n",
    "        # 5. Make results available via service API\n",
    "        return {\n",
    "            \"sample_id\": sample_metadata[\"id\"],\n",
    "            \"status\": \"complete\",\n",
    "            \"results_id\": results_record[\"id\"],\n",
    "            \"api_endpoint\": f\"/genomics/results/{sample_metadata['id']}\"\n",
    "        }\n",
    "    \"\"\")\n",
    "\n",
    "demonstrate_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Seven Bridges Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "SEVEN BRIDGES INTEGRATION BEST PRACTICES:\n",
    "\n",
    "1. Authentication and Security:\n",
    "   - Store SBG credentials as Carta secrets for portability\n",
    "   - Use environment variables for development\n",
    "   - Never hardcode API tokens in code\n",
    "   - Regularly rotate authentication tokens\n",
    "   - Use least-privilege project access\n",
    "\n",
    "2. Resource Management:\n",
    "   - Set appropriate timeouts for long-running tasks\n",
    "   - Use cleanup=True in production to manage storage costs\n",
    "   - Monitor resource usage and costs\n",
    "   - Use appropriate instance types for workloads\n",
    "   - Implement retry logic with exponential backoff\n",
    "\n",
    "3. File Management:\n",
    "   - Validate file existence before starting tasks\n",
    "   - Use checksums to verify file integrity\n",
    "   - Implement efficient upload/download strategies\n",
    "   - Clean up temporary files regularly\n",
    "   - Use compression for large files when appropriate\n",
    "\n",
    "4. Error Handling:\n",
    "   - Implement comprehensive error handling\n",
    "   - Log all task submissions and results\n",
    "   - Set up monitoring and alerting\n",
    "   - Provide meaningful error messages\n",
    "   - Save task metadata for debugging\n",
    "\n",
    "5. Performance Optimization:\n",
    "   - Batch similar tasks when possible\n",
    "   - Use appropriate polling frequencies\n",
    "   - Cache frequently used reference files\n",
    "   - Parallelize independent operations\n",
    "   - Profile and optimize data transfer\n",
    "\n",
    "6. Workflow Design:\n",
    "   - Design modular, reusable workflows\n",
    "   - Document all inputs and outputs\n",
    "   - Version control workflow definitions\n",
    "   - Test workflows with small datasets first\n",
    "   - Implement checkpointing for long workflows\n",
    "\n",
    "7. Integration:\n",
    "   - Use FormsDB for metadata management\n",
    "   - Create service APIs for workflow access\n",
    "   - Implement MQTT for real-time updates\n",
    "   - Store results in structured formats\n",
    "   - Maintain data lineage and provenance\n",
    "\n",
    "8. Testing and Validation:\n",
    "   - Test with known datasets and expected results\n",
    "   - Validate outputs against established benchmarks\n",
    "   - Implement unit tests for workflow components\n",
    "   - Use staging environments for testing\n",
    "   - Document validation procedures\n",
    "\n",
    "9. Monitoring and Maintenance:\n",
    "   - Monitor task success/failure rates\n",
    "   - Track resource usage and costs\n",
    "   - Set up alerts for failures\n",
    "   - Regular review of workflows and performance\n",
    "   - Keep SBG platform and pycarta updated\n",
    "\n",
    "10. Documentation:\n",
    "    - Document all workflow steps clearly\n",
    "    - Provide example usage and expected outputs\n",
    "    - Maintain changelog for workflow versions\n",
    "    - Create troubleshooting guides\n",
    "    - Document resource requirements and costs\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated the comprehensive Seven Bridges integration capabilities of pycarta:\n",
    "\n",
    "1. **Authentication** - Multiple methods for secure SBG access\n",
    "2. **ExecutableApp** - Convert individual SBG Apps to Python functions\n",
    "3. **ExecutableProject** - Access entire SBG projects as Python classes\n",
    "4. **File Management** - Automatic upload/download handling\n",
    "5. **Progress Tracking** - Monitor long-running computational tasks\n",
    "6. **Integration** - Combine with other pycarta modules\n",
    "7. **Best Practices** - Guidelines for production workflows\n",
    "\n",
    "The Seven Bridges integration enables seamless incorporation of cloud-based computational workflows into Python data analysis pipelines, making complex bioinformatics and computational tasks accessible through simple function calls."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}