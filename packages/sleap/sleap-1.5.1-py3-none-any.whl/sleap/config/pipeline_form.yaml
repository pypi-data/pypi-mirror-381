training:

- name: _pipeline
  label: Training/Inference Pipeline Type
  type: stacked
  default: "multi-animal bottom-up "
  options: "multi-animal bottom-up,multi-animal top-down,multi-animal bottom-up-id,multi-animal top-down-id,single animal"

  multi-animal bottom-up:
  - type: text
    text: '<b>Multi-Animal Bottom-Up Pipeline</b>:<br />
      This pipeline uses single model with two output heads:
      a "<u>confidence map</u>" head to predicts the
      nodes for an entire image and a "<u>part affinity field</u>" head to group
      the nodes into distinct animal instances.'
  - label: Max Instances
    name: _max_instances
    type: optional_int
    help: Maximum number of instances per frame.
    none_label: No max
    default_disabled: true
    range: 1,100
    default: 1

  - name: model_config.head_configs.bottomup.confmaps.sigma
    label: Sigma for Nodes
    type: double
    default: 5.0
    help: Spread of the Gaussian distribution of the confidence maps as a scalar float.
      Smaller values are more precise but may be difficult to learn as they have a
      lower density within the image space. Larger values are easier to learn but
      are less precise with respect to the peak coordinate. This spread is in units
      of pixels of the model input image, i.e., the image resolution after any input
      scaling is applied.

  - name: model_config.head_configs.bottomup.pafs.sigma
    label: Sigma for Edges
    type: double
    default: 15.0
    help: Spread of the Gaussian distribution that weigh the part affinity fields
      as a function of their distance from the edge they represent. Smaller values
      are more precise but may be difficult to learn as they have a lower density
      within the image space. Larger values are easier to learn but are less precise
      with respect to the edge distance, so can be less useful in disambiguating between
      edges that are nearby and parallel in direction. This spread is in units of
      pixels of the model input image, i.e., the image resolution after any input
      scaling is applied.

  multi-animal top-down:
  - type: text
    text: '<b>Multi-Animal Top-Down Pipeline</b>:<br />
    This pipeline uses two models: a "<u>centroid</u>" model to
    locate and crop around each animal in the frame, and a
    "<u>centered-instance confidence map</u>" model for predicted node locations
    for each individual animal predicted by the centroid model.'
  - label: Max Instances
    name: _max_instances
    type: optional_int
    help: Maximum number of instances per frame.
    none_label: No max
    default_disabled: true
    range: 1,100
    default: 1

  - default: 5.0
    help: Spread of the Gaussian distribution of the confidence maps as a scalar float.
      Smaller values are more precise but may be difficult to learn as they have a
      lower density within the image space. Larger values are easier to learn but
      are less precise with respect to the peak coordinate. This spread is in units
      of pixels of the model input image, i.e., the image resolution after any input
      scaling is applied.
    label: Sigma for Centroids
    name: model_config.head_configs.centroid.confmaps.sigma
    type: double

  - default: null
    help: Text name of a body part (node) to use as the anchor point. If None, the
      midpoint of the bounding box of all visible instance points will be used as
      the anchor. The bounding box midpoint will also be used if the anchor part is
      specified but not visible in the instance. Setting a reliable anchor point can
      significantly improve topdown model accuracy as they benefit from a consistent
      geometry of the body parts relative to the center of the image.
    label: Anchor Part
    name: model_config.head_configs.centered_instance.confmaps.anchor_part
    type: optional_list

  - default: 5.0
    help: Spread of the Gaussian distribution of the confidence maps as a scalar float.
      Smaller values are more precise but may be difficult to learn as they have a
      lower density within the image space. Larger values are easier to learn but
      are less precise with respect to the peak coordinate. This spread is in units
      of pixels of the model input image, i.e., the image resolution after any input
      scaling is applied.
    label: Sigma for Nodes
    name: model_config.head_configs.centered_instance.confmaps.sigma
    type: double

  multi-animal bottom-up-id:
  - type: text
    text: '<b>Multi-Animal Bottom-Up-Id Pipeline</b>:<br />
      This pipeline uses single model with two output heads: a "<u>confidence
      map</u>" head to predicts the nodes for an entire image and a "<u>part
      affinity field</u>" head to group the nodes into distinct animal
      instances. It also handles classification and tracking.'

  - name: model_config.head_configs.multi_class_bottomup.confmaps.sigma
    label: Sigma for Nodes
    type: double
    default: 5.0
    help: Spread of the Gaussian distribution of the confidence maps as a scalar float.
      Smaller values are more precise but may be difficult to learn as they have a
      lower density within the image space. Larger values are easier to learn but
      are less precise with respect to the peak coordinate. This spread is in units
      of pixels of the model input image, i.e., the image resolution after any input
      scaling is applied.

  - name: model_config.head_configs.multi_class_bottomup.class_maps.sigma
    label: Sigma for Edges
    type: double
    default: 15.0
    help: Spread of the Gaussian distribution that weigh the part affinity fields
      as a function of their distance from the edge they represent. Smaller values
      are more precise but may be difficult to learn as they have a lower density
      within the image space. Larger values are easier to learn but are less precise
      with respect to the edge distance, so can be less useful in disambiguating between
      edges that are nearby and parallel in direction. This spread is in units of
      pixels of the model input image, i.e., the image resolution after any input
      scaling is applied.

  multi-animal top-down-id:
  - type: text
    text: '<b>Multi-Animal Top-Down-Id Pipeline</b>:<br />
    This pipeline uses two models: a "<u>centroid</u>" model to locate and crop
    around each animal in the frame, and a "<u>centered-instance confidence
    map</u>" model for predicted node locations for each individual animal
    predicted by the centroid model. It also handles classification and
    tracking.'

  - default: 5.0
    help: Spread of the Gaussian distribution of the confidence maps as a scalar float.
      Smaller values are more precise but may be difficult to learn as they have a
      lower density within the image space. Larger values are easier to learn but
      are less precise with respect to the peak coordinate. This spread is in units
      of pixels of the model input image, i.e., the image resolution after any input
      scaling is applied.
    label: Sigma for Centroids
    name: model_config.head_configs.centroid.confmaps.sigma
    type: double

  - default: null
    help: Text name of a body part (node) to use as the anchor point. If None, the
      midpoint of the bounding box of all visible instance points will be used as
      the anchor. The bounding box midpoint will also be used if the anchor part is
      specified but not visible in the instance. Setting a reliable anchor point can
      significantly improve topdown model accuracy as they benefit from a consistent
      geometry of the body parts relative to the center of the image.
    label: Anchor Part
    name: model_config.head_configs.multi_class_topdown.confmaps.anchor_part
    type: optional_list

  - default: 5.0
    help: Spread of the Gaussian distribution of the confidence maps as a scalar float.
      Smaller values are more precise but may be difficult to learn as they have a
      lower density within the image space. Larger values are easier to learn but
      are less precise with respect to the peak coordinate. This spread is in units
      of pixels of the model input image, i.e., the image resolution after any input
      scaling is applied.
    label: Sigma for Nodes
    name: model_config.head_configs.multi_class_topdown.confmaps.sigma
    type: double

  single animal:
  - type: text
    text: '<b>Single Animal Pipeline</b>:<br />
      This pipeline uses a single "<u>confidence map</u>"
      model to predicts the nodes for an entire image and then groups all of
      these nodes into a single animal instance.<br /><br />
      For predicting on videos with more than one animal per frame, use a
      multi-animal pipeline (even if your training data has one instance per frame).'

  - name: model_config.head_configs.single_instance.confmaps.sigma
    label: Sigma for Nodes
    type: double
    default: 5.0
    help: Spread of the Gaussian distribution of the confidence maps as a scalar float.
      Smaller values are more precise but may be difficult to learn as they have a
      lower density within the image space. Larger values are easier to learn but
      are less precise with respect to the peak coordinate. This spread is in units
      of pixels of the model input image, i.e., the image resolution after any input
      scaling is applied.

#general:
#- default: 8
#  help: Number of examples per minibatch, i.e., a single step of training. Higher
#    numbers can increase generalization performance by averaging model gradient updates
#    over a larger number of examples at the cost of considerably more GPU memory,
#    especially for larger sized images. Lower numbers may lead to overfitting, but
#    may be beneficial to the optimization process when few but varied examples are
#    available.
#  label: Batch Size
#  name: optimization.batch_size
#  type: int

- type: text
  text: '<b>Input Data Options</b>'

- default: ''
  help: If set, converts the image to RGB/grayscale if not already.
  label: Convert Image To
  name: _ensure_channels
  options: ',RGB,grayscale'
  type: list

- type: text
  text: '<b>Data Pipeline framework options</b>'

- default: 'torch_dataset'
  help: Framework to create the data loaders. (When using torch_dataset,
    num_workers in trainer_config should be set to 0 as multiprocessing
    doesn't work with pickling video backends.)
  label: Data Pipeline Framework
  name: data_config.data_pipeline_fw
  options: 'torch_dataset,torch_dataset_cache_img_memory,torch_dataset_cache_img_disk'
  type: list

- type: text
  text: '<b>WandB options</b>'

- default: false
  help: If True, the model training will be logged to WandB.
  label: Enable WandB for logging
  name: trainer_config.use_wandb
  type: bool

- name: trainer_config.wandb.entity
  label:  Entity Name
  type: string
  default: null

- name: trainer_config.wandb.project
  label:  Project Name
  type: string
  default: null

- name: trainer_config.wandb.name
  label:  Run Name
  type: string
  default: null

- name: trainer_config.wandb.api_key
  label: WandBAPI Key
  type: string
  default: null
  help: WandB API Key. You could also set it in your terminal by exporting the WANDB_API_KEY environment variable.
    Or `wandb.login()` in your Python shell.
  hidden: true

- name: trainer_config.wandb.prv_runid
  label: Previous Run ID
  type: string
  default: null
  help: Previous run ID if you want to resume training from a previous run. If None, a new run will be created.

- name: trainer_config.wandb.group
  label: Group Name
  type: string
  default: null
  help: Group name if you want to group runs together.


- type: text
  text: '<b>ZMQ Options</b>'

- name: trainer_config.zmq.controller_port
  label: Controller Port
  type: int
  default: 9000
  range: 1024,65535

- name: trainer_config.zmq.publish_port
  label: Publish Port
  type: int
  default: 9001
  range: 1024,65535

- type: text
  text: '<b>Output Options</b>'

- default: null
  help: String to set as the run name. ckpts are saved to {Runs folder}/{Run name}. If None, it will be auto-generated {trainer_config.ckpt_dir}/{time_stamp}_{head_name}_n={num_samples}.
  label: Run Name
  name: trainer_config.run_name
  type: string
- default: models
  help: 'Path to the folder that run data should be stored in. All the data for a
    single run are stored in the path: "{Runs folder}/{Run name}". Non-existing folders will be created
    if they do not already exist. Defaults to the "models" subdirectory of the current
    working directory.'
  label: Runs Folder
  name: trainer_config.ckpt_dir
  type: string
- default: true
  help: 'If True, the model will be saved at the end of an epoch if the validation
    loss has improved. If enabled, the model will be serialized to: "{run_folder}/best_model.h5"'
  label: Best Model
  name: trainer_config.save_ckpt
  type: bool
- default: false
  help: 'If True, the model will be saved at the end of every epoch, regardless of
    whether there was an improvement detected, but will overwrite the previous latest
    model. If enabled, the model will be serialized to: "{run_folder}/latest_model.h5"'
  label: Latest Model
  name: trainer_config.model_ckpt.save_last
  type: bool

- name: trainer_config.visualize_preds_during_training
  label: Visualize Predictions During Training
  type: bool
  default: true

- name: trainer_config.keep_viz
  label: Keep Prediction Visualization Images After Training
  type: bool
  default: false

- name: _predict_frames
  label: Predict On
  type: list
  options: current frame,random frames
  default: current frame

inference:

- name: _pipeline
  label: Training/Inference Pipeline Type
  type: stacked
  default: "multi-animal bottom-up "
  options: "multi-animal bottom-up,multi-animal top-down,multi-animal bottom-up-id,multi-animal top-down-id,single animal,movenet-lightning,movenet-thunder,tracking-only"

  multi-animal bottom-up:
  - type: text
    text: '<b>Multi-Animal Bottom-Up Pipeline</b>:<br />
      This pipeline uses single model with two output heads:
      a "<u>confidence map</u>" head to predicts the
      nodes for an entire image and a "<u>part affinity field</u>" head to group
      the nodes into distinct animal instances.'
  - label: Max Instances
    name: _max_instances
    type: optional_int
    help: Maximum number of instances per frame.
    none_label: No max
    default_disabled: true
    range: 1,100
    default: 1

  multi-animal top-down:
  - type: text
    text: '<b>Multi-Animal Top-Down Pipeline</b>:<br />
    This pipeline uses two models: a "<u>centroid</u>" model to
    locate and crop around each animal in the frame, and a
    "<u>centered-instance confidence map</u>" model for predicted node locations
    for each individual animal predicted by the centroid model.'
  - label: Max Instances
    name: _max_instances
    type: optional_int
    help: Maximum number of instances per frame.
    none_label: No max
    default_disabled: true
    range: 1,100
    default: 1

  multi-animal bottom-up-id:
  - type: text
    text: '<b>Multi-Animal Bottom-Up-Id Pipeline</b>:<br />
      This pipeline uses single model with two output heads: a "<u>confidence
      map</u>" head to predicts the nodes for an entire image and a "<u>part
      affinity field</u>" head to group the nodes into distinct animal
      instances. It also handles classification and tracking.'

  multi-animal top-down-id:
  - type: text
    text: '<b>Multi-Animal Top-Down-Id Pipeline</b>:<br />
    This pipeline uses two models: a "<u>centroid</u>" model to locate and crop
    around each animal in the frame, and a "<u>centered-instance confidence
    map</u>" model for predicted node locations for each individual animal
    predicted by the centroid model. It also handles classification and
    tracking.'

  single animal:
  - type: text
    text: '<b>Single Animal Pipeline</b>:<br />
      This pipeline uses a single "<u>confidence map</u>"
      model to predicts the nodes for an entire image and then groups all of
      these nodes into a single animal instance.<br /><br />
      For predicting on videos with more than one animal per frame, use a
      multi-animal pipeline (even if your training data has one instance per frame).'

  movenet-lightning:
  - type: text
    text: '<b>MoveNet Lightning Pipeline</b>:<br />
      This pipeline uses a pretrained MoveNet Lightning model to predict the
      nodes for an entire image and then groups all of these nodes into a single
      instance. Lightning is intended for latency-critical applications. Note
      that this model is intended for human pose estimation. There is no support
      for videos containing more than one instance'

  movenet-thunder:
  - type: text
    text: '<b>MoveNet Thunder Pipeline</b>:<br />
      This pipeline uses a pretrained MoveNet Thunder model to predict the nodes
      for an entire image and then groups all of these nodes into a single
      instance. Thunder is intended for applications that require high accuracy.
      Note that this model is intended for human pose estimation. There is no
      support for videos containing more than one instance'

  tracking-only:

- name: _batch_size
  label: Batch Size
  type: int
  default: 4
  range: 1,512

- name: tracking.tracker
  label: Tracker (cross-frame identity) Method
  type: stacked
  default: none
  options: none,flow,simple

  none:

  flow:
    # - type: text
    #   text: '<b>Pre-tracker data cleaning</b>:'
    # - name: tracking.target_instance_count
    #   label: Target Number of Instances Per Frame
    #   type: optional_int
    #   none_label: No target
    #   default_disabled: true
    #   range: 1,100
    #   default: 1
    # - name: tracking.pre_cull_to_target
    #   label: Cull to Target Instance Count
    #   type: bool
    #   default: false
    # - name: tracking.pre_cull_iou_threshold
    #   label: Cull using IoU Threshold
    #   type: double
    #   default: 0.8
    - type: text
      text: '<b>Tracking with optical flow</b>:<br />
      This tracker "shifts" instances from previous frames using optical flow
      before matching instances in each frame to the <i>shifted</i> instances from
      prior frames.'
    - name: tracking.max_tracks
      label: Max number of tracks
      type: optional_int
      none_label: No limit
      default_disabled: true
      range: 1,100
      default: 1
    - name: tracking.similarity
      label: Similarity Method
      type: list
      default: oks
      options: "oks,iou,centroids"
    - name: tracking.match
      label: Matching Method
      type: list
      default: greedy
      options: greedy,hungarian
    - name: tracking.track_window
      label: Elapsed Frame Window
      type: int
      default: 5
    - name: tracking.robust
      label: 'Robust quantile of similarity scores'
      help: 'For a value between 0 and 1 (excluded), use a robust quantile
      of the similarity scores to assign a track to an instance.<br />If equal to 1,
      use the max similarity score (non-robust).'
      type: optional_double
      default_disabled: true
      none_label: Use max (non-robust)
      range: 0,1
      default: 0.95
    - type: text
      text: '<b>Post-tracker data cleaning</b>:'
    - name: tracking.post_connect_single_breaks
      label: Connect Single Track Breaks
      type: bool
      default: false

  simple:
    # - type: text
    #   text: '<b>Pre-tracker data cleaning</b>:'
    # - name: tracking.target_instance_count
    #   label: Target Number of Instances Per Frame
    #   type: optional_int
    #   none_label: No target
    #   default_disabled: true
    #   range: 1,100
    #   default: 1
    # - name: tracking.pre_cull_to_target
    #   label: Cull to Target Instance Count
    #   type: bool
    #   default: false
    # - name: tracking.pre_cull_iou_threshold
    #   label: Cull using IoU Threshold
    #   type: double
    #   default: 0.8
    - type: text
      text: '<b>Tracking</b>:<br />
        This tracker assigns track identities by matching instances from prior
        frames to instances on subsequent frames.'
    # - name: tracking.max_tracking
    #   label: Limit max number of tracks
    #   type: bool
    #   default: false
    - name: tracking.max_tracks
      label: Max number of tracks
      type: optional_int
      none_label: No limit
      default_disabled: true
      range: 1,100
      default: 1
    - name: tracking.similarity
      label: Similarity Method
      type: list
      default: instance
      options: "centroid,iou,object keypoint"
    - name: tracking.match
      label: Matching Method
      type: list
      default: hungarian
      options: greedy,hungarian
    - name: tracking.track_window
      label: Elapsed Frame Window
      type: int
      default: 5
    - name: tracking.robust
      label: 'Robust quantile of similarity scores'
      help: 'For a value between 0 and 1 (excluded), use a robust quantile
      of the similarity scores to assign a track to an instance.<br />If equal to 1,
      use the max similarity score (non-robust).'
      type: optional_double
      default_disabled: true
      none_label: Use max (non-robust)
      range: 0,1
      default: 0.95
    - type: text
      text: '<b>Post-tracker data cleaning</b>:'
    - name: tracking.post_connect_single_breaks
      label: Connect Single Track Breaks
      type: bool
      default: false




- name: _predict_frames
  label: Predict On
  type: list
  options: current frame,random frames
  default: current frame
