#!/usr/bin/env python3
"""SAGE Chat CLI - Embedded programming assistant backed by SageDB."""
from __future__ import annotations

import hashlib
import json
import os
import re
import shutil
import tempfile
import textwrap
import urllib.request
import zipfile
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence

import typer
from rich.console import Console
from rich.live import Live
from rich.markdown import Markdown
from rich.panel import Panel
from rich.table import Table
from rich.text import Text
from sage.common.config.output_paths import (
    find_sage_project_root,
    get_sage_paths,
)
from sage.middleware.utils.embedding.embedding_model import EmbeddingModel

console = Console()

try:  # pragma: no cover - runtime check
    from sage.middleware.components.sage_db.python.sage_db import (
        SageDB,
        SageDBException,
    )

    SAGE_DB_AVAILABLE = True
    SAGE_DB_IMPORT_ERROR: Optional[Exception] = None
except Exception as exc:  # pragma: no cover - runtime check
    SageDB = None  # type: ignore
    SageDBException = Exception  # type: ignore
    SAGE_DB_AVAILABLE = False
    SAGE_DB_IMPORT_ERROR = exc


DEFAULT_INDEX_NAME = "docs-public"
DEFAULT_CHUNK_SIZE = 800
DEFAULT_CHUNK_OVERLAP = 160
DEFAULT_TOP_K = 4
DEFAULT_BACKEND = "mock"
DEFAULT_EMBEDDING_METHOD = "hash"
DEFAULT_FIXED_DIM = 384
SUPPORTED_MARKDOWN_SUFFIXES = {".md", ".markdown", ".mdx"}

METHODS_REQUIRE_MODEL = {
    "hf",
    "openai",
    "jina",
    "cohere",
    "zhipu",
    "bedrock",
    "ollama",
    "siliconcloud",
    "nvidia_openai",
    "lollms",
}

GITHUB_DOCS_ZIP_URL = (
    "https://github.com/intellistream/SAGE-Pub/archive/refs/heads/main.zip"
)

app = typer.Typer(
    help="üß≠ ÂµåÂÖ•Âºè SAGE ÁºñÁ®ãÂä©Êâã (Docs + SageDB + LLM)",
    invoke_without_command=True,
)


@dataclass
class ChatManifest:
    """Metadata describing a built knowledge index."""

    index_name: str
    db_path: Path
    created_at: str
    source_dir: str
    embedding: Dict[str, object]
    chunk_size: int
    chunk_overlap: int
    num_documents: int
    num_chunks: int

    @property
    def embed_config(self) -> Dict[str, object]:
        return self.embedding


class HashingEmbedder:
    """Lightweight embedding that hashes tokens into a fixed-length vector."""

    def __init__(self, dim: int = DEFAULT_FIXED_DIM) -> None:
        self._dim = max(64, int(dim))

    def get_dim(self) -> int:
        return self._dim

    @property
    def method_name(self) -> str:
        return "hash"

    def embed(self, text: str) -> List[float]:
        if not text:
            return [0.0] * self._dim

        vector = [0.0] * self._dim
        tokens = re.findall(r"[\w\u4e00-\u9fa5]+", text.lower())
        if not tokens:
            tokens = [text.lower()]

        for token in tokens:
            digest = hashlib.sha256(token.encode("utf-8")).digest()
            for offset in range(0, len(digest), 4):
                chunk = digest[offset : offset + 4]
                if len(chunk) < 4:
                    chunk = chunk.ljust(4, b"\0")
                idx = int.from_bytes(chunk, "little") % self._dim
                vector[idx] += 1.0

        norm = sum(v * v for v in vector) ** 0.5 or 1.0
        return [v / norm for v in vector]


def ensure_sage_db() -> None:
    """Exit early if the SageDB extension is unavailable."""

    if SAGE_DB_AVAILABLE:
        return
    message = (
        "[red]SageDB C++ Êâ©Â±ï‰∏çÂèØÁî®ÔºåÊó†Ê≥ï‰ΩøÁî® `sage chat`„ÄÇ[/red]\n"
        "ËØ∑ÂÖàÊûÑÂª∫ SageDB ÁªÑ‰ª∂Ôºö`cd packages/sage-middleware && bash build.sh`."
    )
    if SAGE_DB_IMPORT_ERROR:
        message += f"\nÂéüÂßãÈîôËØØ: {SAGE_DB_IMPORT_ERROR}"
    console.print(message)
    raise typer.Exit(code=1)


def resolve_index_root(index_root: Optional[str]) -> Path:
    if index_root:
        root = Path(index_root).expanduser().resolve()
        root.mkdir(parents=True, exist_ok=True)
        return root
    paths = get_sage_paths()
    cache_dir = paths.cache_dir / "chat"
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir


def default_source_dir() -> Path:
    project_root = find_sage_project_root()
    if not project_root:
        project_root = Path.cwd()
    candidate = project_root / "docs-public" / "docs_src"
    return candidate


def manifest_path(index_root: Path, index_name: str) -> Path:
    return index_root / f"{index_name}.manifest.json"


def db_file_path(index_root: Path, index_name: str) -> Path:
    return index_root / f"{index_name}.sagedb"


def load_manifest(index_root: Path, index_name: str) -> ChatManifest:
    path = manifest_path(index_root, index_name)
    if not path.exists():
        raise FileNotFoundError(
            f"Êú™ÊâæÂà∞Á¥¢Âºï manifest: {path}. ËØ∑ÂÖàËøêË°å `sage chat ingest`."
        )
    payload = json.loads(path.read_text(encoding="utf-8"))
    manifest = ChatManifest(
        index_name=index_name,
        db_path=Path(payload["db_path"]),
        created_at=payload["created_at"],
        source_dir=payload["source_dir"],
        embedding=payload["embedding"],
        chunk_size=payload["chunk_size"],
        chunk_overlap=payload["chunk_overlap"],
        num_documents=payload.get("num_documents", 0),
        num_chunks=payload.get("num_chunks", 0),
    )
    return manifest


def save_manifest(
    index_root: Path,
    index_name: str,
    manifest: ChatManifest,
) -> None:
    path = manifest_path(index_root, index_name)
    payload = {
        "index_name": manifest.index_name,
        "db_path": str(manifest.db_path),
        "created_at": manifest.created_at,
        "source_dir": manifest.source_dir,
        "embedding": manifest.embedding,
        "chunk_size": manifest.chunk_size,
        "chunk_overlap": manifest.chunk_overlap,
        "num_documents": manifest.num_documents,
        "num_chunks": manifest.num_chunks,
    }
    path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")


def build_embedder(config: Dict[str, object]) -> Any:
    method = str(config.get("method", DEFAULT_EMBEDDING_METHOD))
    params = dict(config.get("params", {}))
    if method == "hash":
        dim = int(params.get("dim", params.get("fixed_dim", DEFAULT_FIXED_DIM)))
        return HashingEmbedder(dim)
    if method == "mockembedder" and "fixed_dim" not in params:
        params["fixed_dim"] = DEFAULT_FIXED_DIM
    embedder = EmbeddingModel(method=method, **params)
    return embedder


def iter_markdown_files(source: Path) -> Iterable[Path]:
    for path in sorted(source.rglob("*")):
        if path.is_file() and path.suffix.lower() in SUPPORTED_MARKDOWN_SUFFIXES:
            yield path


_HEADING_PATTERN = re.compile(r"^(#{1,6})\s+(?P<title>.+?)\s*$")


def parse_markdown_sections(content: str) -> List[Dict[str, str]]:
    sections: List[Dict[str, str]] = []
    current_title = "Introduction"
    current_lines: List[str] = []

    for raw_line in content.splitlines():
        match = _HEADING_PATTERN.match(raw_line.strip())
        if match:
            if current_lines:
                sections.append(
                    {
                        "heading": current_title,
                        "content": "\n".join(current_lines).strip(),
                    }
                )
                current_lines = []
            current_title = match.group("title").strip()
        else:
            current_lines.append(raw_line)

    if current_lines:
        sections.append(
            {"heading": current_title, "content": "\n".join(current_lines).strip()}
        )

    return [section for section in sections if section["content"]]


def chunk_text(content: str, chunk_size: int, chunk_overlap: int) -> List[str]:
    normalized = re.sub(r"\n{3,}", "\n\n", content).strip()
    if not normalized:
        return []

    start = 0
    length = len(normalized)
    step = max(1, chunk_size - chunk_overlap)
    chunks: List[str] = []

    while start < length:
        end = min(length, start + chunk_size)
        chunk = normalized[start:end]
        if end < length:
            boundary = max(chunk.rfind("\n"), chunk.rfind("„ÄÇ"), chunk.rfind("."))
            if boundary >= 0 and boundary > len(chunk) * 0.4:
                end = start + boundary
                chunk = normalized[start:end]
        chunks.append(chunk.strip())
        start += step

    return [c for c in chunks if c]


def slugify(text: str) -> str:
    slug = re.sub(r"[^\w\-]+", "-", text.lower()).strip("-")
    slug = re.sub(r"-+", "-", slug)
    return slug or "section"


def truncate_text(text: str, limit: int = 480) -> str:
    if len(text) <= limit:
        return text
    return text[: limit - 3] + "..."


def sanitize_metadata_value(value: str) -> str:
    cleaned = value.replace("\r", " ").replace("\n", " ")
    cleaned = cleaned.replace('"', "'")
    cleaned = re.sub(r"\s+", " ", cleaned)
    return cleaned.strip()


def ensure_docs_corpus(index_root: Path) -> Path:
    """Ensure we have a docs-public/docs_src directory available."""

    local_source = default_source_dir()
    if local_source.exists():
        return local_source

    cache_root = index_root / "remote_docs"
    docs_path = cache_root / "docs_src"
    if docs_path.exists():
        return docs_path

    cache_root.mkdir(parents=True, exist_ok=True)
    console.print(
        "üåê Êú™Ê£ÄÊµãÂà∞Êú¨Âú∞ docs-public/docs_srcÔºåÊ≠£Âú®‰∏ãËΩΩÂÆòÊñπÊñáÊ°£ÂåÖ...",
        style="cyan",
    )

    fd, tmp_path = tempfile.mkstemp(prefix="sage_docs_", suffix=".zip")
    os.close(fd)
    tmp_file = Path(tmp_path)
    try:
        urllib.request.urlretrieve(GITHUB_DOCS_ZIP_URL, tmp_file)
        with zipfile.ZipFile(tmp_file, "r") as zf:
            zf.extractall(cache_root)
    except Exception as exc:
        if tmp_file.exists():
            tmp_file.unlink()
        raise RuntimeError(f"‰∏ãËΩΩ docs-public ÊñáÊ°£Â§±Ë¥•: {exc}") from exc

    if tmp_file.exists():
        tmp_file.unlink()

    extracted_docs: Optional[Path] = None
    for candidate in cache_root.glob("**/docs_src"):
        if candidate.is_dir():
            extracted_docs = candidate
            break

    if extracted_docs is None:
        raise RuntimeError("‰∏ãËΩΩÁöÑÊñáÊ°£ÂåÖ‰∏≠Êú™ÊâæÂà∞ docs_src ÁõÆÂΩï")

    if docs_path.exists() and docs_path == extracted_docs:
        return docs_path

    if not docs_path.exists():
        docs_path.mkdir(parents=True, exist_ok=True)
        for item in extracted_docs.iterdir():
            shutil.move(str(item), docs_path / item.name)
    return docs_path


def bootstrap_default_index(
    index_root: Path, index_name: str
) -> Optional[ChatManifest]:
    try:
        source_dir = ensure_docs_corpus(index_root)
    except Exception as exc:
        console.print(f"[red]Êó†Ê≥ïÂáÜÂ§áÊñáÊ°£ËØ≠Êñô: {exc}[/red]")
        return None

    embedding_config: Dict[str, object] = {
        "method": DEFAULT_EMBEDDING_METHOD,
        "params": {"dim": DEFAULT_FIXED_DIM},
    }
    console.print(
        f"üöÄ Ê≠£Âú®ÂØºÂÖ• [cyan]{source_dir}[/cyan] ‰ª•ÂàùÂßãÂåñ `{index_name}` Á¥¢Âºï...",
        style="green",
    )
    manifest = ingest_source(
        source_dir=source_dir,
        index_root=index_root,
        index_name=index_name,
        chunk_size=DEFAULT_CHUNK_SIZE,
        chunk_overlap=DEFAULT_CHUNK_OVERLAP,
        embedding_config=embedding_config,
        max_files=None,
    )
    return manifest


def load_or_bootstrap_manifest(index_root: Path, index_name: str) -> ChatManifest:
    try:
        return load_manifest(index_root, index_name)
    except FileNotFoundError:
        console.print(
            "üîç Ê£ÄÊµãÂà∞Â∞öÊú™‰∏∫ `sage chat` ÂàùÂßãÂåñÁ¥¢Âºï„ÄÇ",
            style="yellow",
        )
        if not typer.confirm("ÊòØÂê¶Á´ãÂç≥ÂØºÂÖ• docs-public ÊñáÊ°£Ôºü", default=True):
            console.print(
                "üí° ÂèØ‰ΩøÁî® `sage chat ingest` ÊâãÂä®ÂØºÂÖ•ÂêéÂÜçÈáçËØï„ÄÇ",
                style="cyan",
            )
            raise typer.Exit(code=1)

        manifest = bootstrap_default_index(index_root, index_name)
        if manifest is None:
            raise typer.Exit(code=1)
        return manifest


def ingest_source(
    source_dir: Path,
    index_root: Path,
    index_name: str,
    chunk_size: int,
    chunk_overlap: int,
    embedding_config: Dict[str, object],
    max_files: Optional[int] = None,
) -> ChatManifest:
    ensure_sage_db()

    if not source_dir.exists():
        raise FileNotFoundError(f"ÊñáÊ°£ÁõÆÂΩï‰∏çÂ≠òÂú®: {source_dir}")

    embedder = build_embedder(embedding_config)
    db_path = db_file_path(index_root, index_name)
    if db_path.exists():
        db_path.unlink()

    db = SageDB(embedder.get_dim())
    total_chunks = 0
    total_docs = 0

    for idx, file_path in enumerate(iter_markdown_files(source_dir), start=1):
        if max_files is not None and idx > max_files:
            break

        rel_path = file_path.relative_to(source_dir)
        text = file_path.read_text(encoding="utf-8", errors="ignore")
        sections = parse_markdown_sections(text)
        if not sections:
            continue

        doc_title = sections[0]["heading"] if sections else file_path.stem

        for section_idx, section in enumerate(sections):
            section_chunks = chunk_text(section["content"], chunk_size, chunk_overlap)
            for chunk_idx, chunk in enumerate(section_chunks):
                vector = embedder.embed(chunk)
                metadata = {
                    "doc_path": sanitize_metadata_value(str(rel_path)),
                    "title": sanitize_metadata_value(doc_title),
                    "heading": sanitize_metadata_value(section["heading"]),
                    "anchor": sanitize_metadata_value(slugify(section["heading"])),
                    "chunk": str(chunk_idx),
                    "text": sanitize_metadata_value(truncate_text(chunk, limit=1200)),
                }
                db.add(vector, metadata)
                total_chunks += 1

        total_docs += 1
        console.print(
            f"üìÑ Â§ÑÁêÜÊñáÊ°£ {idx}: {rel_path} (sections={len(sections)})", style="cyan"
        )

    if total_chunks == 0:
        raise RuntimeError("Êú™Âú®ÊñáÊ°£‰∏≠ÁîüÊàê‰ªª‰Ωï chunkÔºåÊ£ÄÊü•Ê∫êÁõÆÂΩïÊàñ chunk ÂèÇÊï∞„ÄÇ")

    console.print(f"üß± ÂÖ±ÂÜôÂÖ•ÂêëÈáè: {total_chunks}")
    db.build_index()
    db.save(str(db_path))

    manifest = ChatManifest(
        index_name=index_name,
        db_path=db_path,
        created_at=datetime.utcnow().isoformat(),
        source_dir=str(source_dir),
        embedding=embedding_config,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        num_documents=total_docs,
        num_chunks=total_chunks,
    )
    save_manifest(index_root, index_name, manifest)
    console.print(
        Panel.fit(f"‚úÖ Á¥¢ÂºïÂ∑≤Êõ¥Êñ∞ -> {db_path}", title="INGEST", style="green")
    )
    return manifest


def open_database(manifest: ChatManifest) -> Any:
    ensure_sage_db()
    if not manifest.db_path.exists():
        prefix = manifest.db_path
        siblings = list(prefix.parent.glob(prefix.name + "*"))
        if not siblings:
            raise FileNotFoundError(
                f"Êú™ÊâæÂà∞Êï∞ÊçÆÂ∫ìÊñá‰ª∂ {manifest.db_path}„ÄÇËØ∑ÈáçÊñ∞ËøêË°å `sage chat ingest`."
            )
    embedder = build_embedder(manifest.embed_config)
    db = SageDB(embedder.get_dim())
    db.load(str(manifest.db_path))
    return db


def build_prompt(question: str, contexts: Sequence[str]) -> List[Dict[str, str]]:
    context_block = "\n\n".join(
        f"[{idx}] {textwrap.dedent(ctx).strip()}"
        for idx, ctx in enumerate(contexts, start=1)
        if ctx
    )
    system_instructions = textwrap.dedent(
        """
        You are SAGE ÂÜÖÂµåÁºñÁ®ãÂä©Êâã„ÄÇÂõûÁ≠îÁî®Êà∑ÂÖ≥‰∫é SAGE ÁöÑÈóÆÈ¢òÔºå‰æùÊçÆÊèê‰æõÁöÑ‰∏ä‰∏ãÊñáËøõË°åËß£Èáä„ÄÇ
        - Â¶ÇÊûú‰∏ä‰∏ãÊñá‰∏çË∂≥‰ª•ÂõûÁ≠îÔºåËØ∑Âù¶ËØöËØ¥ÊòéÂπ∂ÁªôÂá∫‰∏ã‰∏ÄÊ≠•Âª∫ËÆÆ„ÄÇ
        - ÂºïÁî®Êó∂‰ΩøÁî® [ÁºñÂè∑] Ë°®Á§∫„ÄÇ
        - ÂõûÁ≠î‰øùÊåÅÁÆÄÊ¥ÅÔºåÁõ¥Êé•ÁªôÂá∫Ê≠•È™§ÊàñÁ§∫‰æã‰ª£Á†Å„ÄÇ
        """
    ).strip()

    if context_block:
        system_instructions += f"\n\nÂ∑≤Ê£ÄÁ¥¢‰∏ä‰∏ãÊñá:\n{context_block}"

    return [
        {"role": "system", "content": system_instructions},
        {"role": "user", "content": question.strip()},
    ]


class ResponseGenerator:
    def __init__(
        self,
        backend: str,
        model: str,
        base_url: Optional[str],
        api_key: Optional[str],
        temperature: float = 0.2,
    ) -> None:
        self.backend = backend.lower()
        self.model = model
        self.base_url = base_url
        self.api_key = api_key
        self.temperature = temperature

        if self.backend == "mock":
            self.client = None
        else:
            try:
                from sage.libs.utils.openaiclient import OpenAIClient

                kwargs = {"seed": 42}
                if base_url:
                    kwargs["base_url"] = base_url
                if api_key:
                    kwargs["api_key"] = api_key
                self.client = OpenAIClient(model_name=model, **kwargs)
            except Exception as exc:  # pragma: no cover - runtime check
                raise RuntimeError(f"Êó†Ê≥ïÂàùÂßãÂåñ OpenAIClient: {exc}") from exc

    def answer(
        self,
        question: str,
        contexts: Sequence[str],
        references: Sequence[Dict[str, str]],
        stream: bool = False,
    ) -> str:
        if self.backend == "mock":
            return self._mock_answer(question, contexts, references)

        messages = build_prompt(question, contexts)
        try:
            response = self.client.generate(
                messages,
                max_tokens=768,
                temperature=self.temperature,
                stream=stream,
            )
            if isinstance(response, str):
                return response
            # Non-streaming ensures string; streaming not yet supported.
            return str(response)
        except Exception as exc:
            raise RuntimeError(f"Ë∞ÉÁî®ËØ≠Ë®ÄÊ®°ÂûãÂ§±Ë¥•: {exc}") from exc

    @staticmethod
    def _mock_answer(
        question: str,
        contexts: Sequence[str],
        references: Sequence[Dict[str, str]],
    ) -> str:
        if not contexts:
            return "ÊöÇÊó∂Ê≤°Êúâ‰ªéÁü•ËØÜÂ∫ìÊ£ÄÁ¥¢Âà∞Á≠îÊ°à„ÄÇËØ∑Â∞ùËØïÊîπÂÜôÊèêÈóÆÔºåÊàñËøêË°å `sage chat ingest` Êõ¥Êñ∞Á¥¢Âºï„ÄÇ"
        top_ref = references[0] if references else {"title": "ËµÑÊñô", "heading": ""}
        snippet = contexts[0].strip().replace("\n", " ")
        citation = top_ref.get("label", top_ref.get("title", "Docs"))
        return (
            f"Ê†πÊçÆ {citation} ÁöÑËØ¥ÊòéÔºö{snippet[:280]}...\n\n"
            "Â¶ÇÈúÄÊõ¥Â§öÁªÜËäÇÔºåÂèØ‰ª•ËæìÂÖ• `more` ÂÜçÊ¨°Ê£ÄÁ¥¢ÔºåÊàñ‰ΩøÁî® `--backend openai` ÂêØÁî®ÁúüÂÆûÊ®°Âûã„ÄÇ"
        )


def render_references(references: Sequence[Dict[str, str]]) -> Table:
    table = Table(title="Áü•ËØÜÂºïÁî®", show_header=True, header_style="bold cyan")
    table.add_column("#", justify="right", width=3)
    table.add_column("ÊñáÊ°£")
    table.add_column("ËäÇ")
    table.add_column("ÂæóÂàÜ", justify="right", width=7)

    for idx, ref in enumerate(references, start=1):
        table.add_row(
            str(idx),
            ref.get("title", "Êú™Áü•"),
            ref.get("heading", "-"),
            f"{ref.get('score', 0.0):.4f}",
        )
    return table


def retrieve_context(
    db: Any,
    embedder: EmbeddingModel,
    question: str,
    top_k: int,
) -> Dict[str, object]:
    query_vector = embedder.embed(question)
    results = db.search(query_vector, top_k, True)

    contexts: List[str] = []
    references: List[Dict[str, str]] = []
    for item in results:
        metadata = dict(item.metadata) if hasattr(item, "metadata") else {}
        contexts.append(metadata.get("text", ""))
        references.append(
            {
                "title": metadata.get("title", metadata.get("doc_path", "Êú™Áü•")),
                "heading": metadata.get("heading", ""),
                "path": metadata.get("doc_path", ""),
                "anchor": metadata.get("anchor", ""),
                "score": float(getattr(item, "score", 0.0)),
                "label": f"[{metadata.get('doc_path', '?')}]",
            }
        )
    return {"contexts": contexts, "references": references}


def interactive_chat(
    manifest: ChatManifest,
    index_root: Path,
    top_k: int,
    backend: str,
    model: str,
    base_url: Optional[str],
    api_key: Optional[str],
    ask: Optional[str],
    stream: bool,
) -> None:
    embedder = build_embedder(manifest.embed_config)
    db = open_database(manifest)
    generator = ResponseGenerator(backend, model, base_url, api_key)

    console.print(
        Panel(
            f"Á¥¢Âºï: [cyan]{manifest.index_name}[/cyan]\n"
            f"Êù•Ê∫ê: [green]{manifest.source_dir}[/green]\n"
            f"ÊñáÊ°£Êï∞: {manifest.num_documents}  ChunkÊï∞: {manifest.num_chunks}\n"
            f"Embedding: {manifest.embed_config}",
            title="SAGE Chat ÂáÜÂ§áÂ∞±Áª™",
        )
    )

    def answer_once(query: str) -> None:
        payload = retrieve_context(db, embedder, query, top_k)
        contexts = payload["contexts"]
        references = payload["references"]

        try:
            reply = generator.answer(query, contexts, references, stream=stream)
        except Exception as exc:
            console.print(f"[red]ÁîüÊàêÂõûÁ≠îÂ§±Ë¥•: {exc}[/red]")
            return

        context_table = render_references(references)
        console.print(context_table)

        if stream:
            text = Text()
            with Live(Panel(text, title="ÂõûÁ≠î"), auto_refresh=False) as live:
                text.append(reply)
                live.refresh()
        else:
            console.print(Panel(Markdown(reply), title="ÂõûÁ≠î", style="bold green"))

    if ask:
        answer_once(ask)
        return

    console.print("ËæìÂÖ• `exit` Êàñ `quit` ÁªìÊùüÂØπËØù„ÄÇ", style="dim")
    while True:
        try:
            question = typer.prompt("ü§ñ ‰Ω†ÁöÑÈóÆÈ¢ò")
        except (EOFError, KeyboardInterrupt):
            console.print("\nÂÜçËßÅ üëã", style="cyan")
            break
        if not question.strip():
            continue
        if question.lower().strip() in {"exit", "quit", "q"}:
            console.print("ÂÜçËßÅ üëã", style="cyan")
            break
        answer_once(question)


@app.callback()
def main(
    ctx: typer.Context,
    index_name: str = typer.Option(
        DEFAULT_INDEX_NAME,
        "--index",
        "-i",
        help="Á¥¢ÂºïÂêçÁß∞ÔºåÁî®‰∫éËØªÂèñ manifest Âíå SageDB Êñá‰ª∂",
    ),
    ask: Optional[str] = typer.Option(
        None,
        "--ask",
        "-q",
        help="Áõ¥Êé•ÊèêÈóÆÂπ∂ÈÄÄÂá∫ÔºåËÄå‰∏çÊòØËøõÂÖ•‰∫§‰∫íÊ®°Âºè",
    ),
    top_k: int = typer.Option(
        DEFAULT_TOP_K,
        "--top-k",
        "-k",
        min=1,
        max=20,
        help="Ê£ÄÁ¥¢Êó∂ËøîÂõûÁöÑÂèÇËÄÉÊñáÊ°£Êï∞Èáè",
    ),
    backend: str = typer.Option(
        DEFAULT_BACKEND,
        "--backend",
        help="ÂõûÁ≠îÁîüÊàêÂêéÁ´Ø: mock / openai / compatible",
    ),
    model: str = typer.Option(
        "qwen-max",
        "--model",
        help="ÂõûÁ≠îÁîüÊàêÊ®°ÂûãÂêçÁß∞",
    ),
    base_url: Optional[str] = typer.Option(
        None,
        "--base-url",
        help="LLM API base_url (‰æãÂ¶Ç vLLM ÊàñÂÖºÂÆπ OpenAI ÁöÑÊé•Âè£)",
    ),
    api_key: Optional[str] = typer.Option(
        lambda: os.environ.get("SAGE_CHAT_API_KEY"),
        "--api-key",
        help="LLM API Key (ÈªòËÆ§ËØªÂèñÁéØÂ¢ÉÂèòÈáè SAGE_CHAT_API_KEY)",
    ),
    index_root: Optional[str] = typer.Option(
        None,
        "--index-root",
        help="Á¥¢ÂºïËæìÂá∫ÁõÆÂΩï (Êú™Êèê‰æõÂàô‰ΩøÁî® ~/.sage/cache/chat)",
    ),
    stream: bool = typer.Option(False, "--stream", help="ÂêØÁî®ÊµÅÂºèËæìÂá∫ (‰ªÖÂΩìÂêéÁ´ØÊîØÊåÅ)"),
) -> None:
    if ctx.invoked_subcommand is not None:
        return

    ensure_sage_db()
    root = resolve_index_root(index_root)
    manifest = load_or_bootstrap_manifest(root, index_name)

    interactive_chat(
        manifest=manifest,
        index_root=root,
        top_k=top_k,
        backend=backend,
        model=model,
        base_url=base_url,
        api_key=api_key,
        ask=ask,
        stream=stream,
    )


@app.command("ingest")
def ingest(
    source_dir: Optional[Path] = typer.Option(
        None,
        "--source",
        "-s",
        exists=True,
        file_okay=False,
        dir_okay=True,
        resolve_path=True,
        help="ÊñáÊ°£Êù•Ê∫êÁõÆÂΩï (ÈªòËÆ§ docs-public/docs_src)",
    ),
    index_name: str = typer.Option(
        DEFAULT_INDEX_NAME, "--index", "-i", help="Á¥¢ÂºïÂêçÁß∞"
    ),
    chunk_size: int = typer.Option(
        DEFAULT_CHUNK_SIZE,
        "--chunk-size",
        help="chunk Â≠óÁ¨¶ÈïøÂ∫¶",
        min=128,
        max=4096,
    ),
    chunk_overlap: int = typer.Option(
        DEFAULT_CHUNK_OVERLAP,
        "--chunk-overlap",
        help="chunk ‰πãÈó¥ÁöÑÈáçÂè†Â≠óÁ¨¶Êï∞",
        min=0,
        max=1024,
    ),
    embedding_method: str = typer.Option(
        DEFAULT_EMBEDDING_METHOD,
        "--embedding-method",
        help="Embedding ÊñπÊ≥ï (mockembedder/hf/openai/...)",
    ),
    embedding_model: Optional[str] = typer.Option(
        None,
        "--embedding-model",
        help="Embedding Ê®°ÂûãÂêçÁß∞ (ÊñπÊ≥ïÈúÄË¶ÅÊó∂Êèê‰æõ)",
    ),
    fixed_dim: int = typer.Option(
        DEFAULT_FIXED_DIM,
        "--fixed-dim",
        help="mockembedder ‰ΩøÁî®ÁöÑÁª¥Â∫¶",
        min=64,
        max=2048,
    ),
    max_files: Optional[int] = typer.Option(
        None,
        "--max-files",
        help="‰ªÖÂ§ÑÁêÜÊåáÂÆöÊï∞ÈáèÁöÑÊñá‰ª∂ (ÊµãËØï/Ë∞ÉËØïÁî®)",
    ),
    index_root: Optional[str] = typer.Option(
        None,
        "--index-root",
        help="Á¥¢ÂºïËæìÂá∫ÁõÆÂΩï (Êú™Êèê‰æõÂàô‰ΩøÁî® ~/.sage/cache/chat)",
    ),
) -> None:
    ensure_sage_db()
    root = resolve_index_root(index_root)
    target_source = source_dir or default_source_dir()

    needs_model = embedding_method in METHODS_REQUIRE_MODEL
    if needs_model and not embedding_model:
        raise typer.BadParameter(f"{embedding_method} ÊñπÊ≥ïÈúÄË¶ÅÊåáÂÆö --embedding-model")

    embedding_config: Dict[str, object] = {"method": embedding_method, "params": {}}
    if embedding_method == "mockembedder":
        embedding_config["params"] = {"fixed_dim": fixed_dim}
    elif embedding_method == "hash":
        embedding_config["params"] = {"dim": fixed_dim}
    if embedding_model:
        embedding_config.setdefault("params", {})["model"] = embedding_model

    console.print(
        Panel(
            f"Á¥¢ÂºïÂêçÁß∞: [cyan]{index_name}[/cyan]\n"
            f"ÊñáÊ°£ÁõÆÂΩï: [green]{target_source}[/green]\n"
            f"Á¥¢ÂºïÁõÆÂΩï: [magenta]{root}[/magenta]\n"
            f"Embedding: {embedding_config}",
            title="SAGE Chat Ingest",
        )
    )

    manifest = ingest_source(
        source_dir=target_source,
        index_root=root,
        index_name=index_name,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        embedding_config=embedding_config,
        max_files=max_files,
    )


@app.command("show")
def show_manifest(
    index_name: str = typer.Option(DEFAULT_INDEX_NAME, "--index", "-i"),
    index_root: Optional[str] = typer.Option(None, "--index-root", help="Á¥¢ÂºïÊâÄÂú®ÁõÆÂΩï"),
) -> None:
    ensure_sage_db()
    root = resolve_index_root(index_root)
    try:
        manifest = load_manifest(root, index_name)
    except FileNotFoundError as exc:
        console.print(f"[red]{exc}[/red]")
        raise typer.Exit(code=1)

    table = Table(title=f"SAGE Chat Á¥¢Âºï: {index_name}")
    table.add_column("Â±ûÊÄß", style="cyan")
    table.add_column("ÂÄº", style="green")
    table.add_row("Á¥¢ÂºïË∑ØÂæÑ", str(manifest.db_path))
    table.add_row("ÂàõÂª∫Êó∂Èó¥", manifest.created_at)
    table.add_row("ÊñáÊ°£ÁõÆÂΩï", manifest.source_dir)
    table.add_row("ÊñáÊ°£Êï∞Èáè", str(manifest.num_documents))
    table.add_row("Chunk Êï∞Èáè", str(manifest.num_chunks))
    table.add_row("Embedding", json.dumps(manifest.embedding, ensure_ascii=False))
    table.add_row(
        "Chunk ÈÖçÁΩÆ", f"size={manifest.chunk_size}, overlap={manifest.chunk_overlap}"
    )
    console.print(table)


__all__ = ["app"]
