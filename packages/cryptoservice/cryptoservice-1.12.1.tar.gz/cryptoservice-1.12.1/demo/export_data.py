"""å¯¼å‡ºæ•°æ®åº“æ•°æ®åˆ°æ–‡ä»¶çš„è„šæœ¬."""

import asyncio
import os
from pathlib import Path

import dotenv
import pandas as pd

from cryptoservice.models import Freq, UniverseDefinition
from cryptoservice.services import MarketDataService
from cryptoservice.storage import Database

# ============== é…ç½®å‚æ•° ==============
UNIVERSE_FILE = "./data/universe.json"
DB_PATH = "./data/database/market.db"
EXPORT_BASE_PATH = "./data/exports"

# æ•°æ®å¯¼å‡ºé…ç½®
DATA_FREQ = Freq.d1
EXPORT_FREQ = Freq.d1
CHUNK_DAYS = 100

# å¯¼å‡ºé€‰é¡¹
EXPORT_KLINES = True  # å¯¼å‡ºKçº¿æ•°æ®
EXPORT_METRICS = True  # å¯¼å‡ºæŒ‡æ ‡æ•°æ®ï¼ˆfr, oi, lsrï¼‰
DOWNLOAD_CATEGORIES = True  # ä¸‹è½½åˆ†ç±»æ•°æ®

# è‡ªå®šä¹‰æ—¶é—´èŒƒå›´ï¼ˆå¯é€‰ï¼‰
CUSTOM_START_DATE = "2024-10-01"
CUSTOM_END_DATE = "2024-10-02"

# å­—æ®µåæ˜ å°„ï¼šé•¿å­—æ®µå -> ç¼©å†™å½¢å¼
FIELD_MAPPING = {
    # Kçº¿æ•°æ®å­—æ®µ
    "open_price": "opn",
    "high_price": "hgh",
    "low_price": "low",
    "close_price": "cls",
    "volume": "vol",
    "quote_volume": "amt",
    "trades_count": "tnum",
    "taker_buy_volume": "tbvol",
    "taker_buy_quote_volume": "tbamt",
    "taker_sell_volume": "tsvol",
    "taker_sell_quote_volume": "tsamt",
    # Metricsæ•°æ®å­—æ®µï¼ˆå·²ç»æ˜¯ç¼©å†™ï¼‰
    "funding_rate": "fr",
    "open_interest": "oi",
    # å¤šç©ºæ¯”ä¾‹åªå¯¼å‡ºtakerç±»å‹ï¼ˆVisionæ•°æ®æœ€å®Œæ•´çš„ç±»å‹ï¼‰
    "long_short_ratio": "lsr",
}


async def validate_and_load_universe() -> UniverseDefinition:
    """éªŒè¯å¹¶åŠ è½½Universeå®šä¹‰."""
    if not Path(UNIVERSE_FILE).exists():
        raise FileNotFoundError(f"Universeæ–‡ä»¶ä¸å­˜åœ¨: {UNIVERSE_FILE}")

    if not Path(DB_PATH).exists():
        raise FileNotFoundError(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {DB_PATH}")

    Path(EXPORT_BASE_PATH).mkdir(parents=True, exist_ok=True)

    print("ğŸ“– åŠ è½½Universeå®šä¹‰...")
    universe_def = UniverseDefinition.load_from_file(UNIVERSE_FILE)
    print(f"   âœ… æˆåŠŸåŠ è½½ {len(universe_def.snapshots)} ä¸ªå¿«ç…§")

    return universe_def


async def create_market_service():
    """åˆ›å»ºå¸‚åœºæœåŠ¡."""
    if not DOWNLOAD_CATEGORIES:
        return None

    try:
        print("ğŸ”— åˆå§‹åŒ–å¸‚åœºæœåŠ¡...")
        dotenv.load_dotenv()
        api_key = os.getenv("BINANCE_API_KEY")
        api_secret = os.getenv("BINANCE_API_SECRET")
        market_service_ctx = await MarketDataService.create(api_key=api_key, api_secret=api_secret)
        print("   âœ… å¸‚åœºæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        return market_service_ctx
    except Exception as e:
        print(f"   âš ï¸ å¸‚åœºæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
        return None


def calculate_time_range(snapshot):
    """è®¡ç®—å®é™…å¯¼å‡ºçš„æ—¶é—´èŒƒå›´."""
    universe_start_ts = int(snapshot.start_date_ts)
    universe_end_ts = int(snapshot.end_date_ts)

    if CUSTOM_START_DATE:
        custom_start_ts = int(pd.Timestamp(CUSTOM_START_DATE).timestamp() * 1000)
        actual_start_ts = max(custom_start_ts, universe_start_ts)
    else:
        actual_start_ts = universe_start_ts

    if CUSTOM_END_DATE:
        custom_end_ts = int(pd.Timestamp(CUSTOM_END_DATE).timestamp() * 1000)
        actual_end_ts = min(custom_end_ts, universe_end_ts)
    else:
        actual_end_ts = universe_end_ts

    start_date = pd.Timestamp(int(actual_start_ts), unit="ms").strftime("%Y-%m-%d")
    end_date = pd.Timestamp(int(actual_end_ts), unit="ms").strftime("%Y-%m-%d")

    return actual_start_ts, actual_end_ts, start_date, end_date


def rename_fields_to_abbreviations(df: pd.DataFrame) -> pd.DataFrame:
    """å°†å­—æ®µåé‡å‘½åä¸ºç¼©å†™å½¢å¼."""
    if df.empty:
        return df

    # è·å–éœ€è¦é‡å‘½åçš„å­—æ®µ
    columns_to_rename = {col: FIELD_MAPPING[col] for col in df.columns if col in FIELD_MAPPING}

    if columns_to_rename:
        df = df.rename(columns=columns_to_rename)
        renamed_fields = list(columns_to_rename.values())
        print(f"      ğŸ”¤ å­—æ®µé‡å‘½å: {len(columns_to_rename)} ä¸ªå­—æ®µ -> {', '.join(renamed_fields)}")

    return df


async def get_kline_data(db: Database, symbols: list[str], start_date: str, end_date: str) -> pd.DataFrame:
    """è·å–Kçº¿æ•°æ®."""
    if not EXPORT_KLINES:
        return pd.DataFrame()

    kline_df = await db.kline_query.select_by_time_range(symbols, start_date, end_date, DATA_FREQ)
    if not kline_df.empty:
        print(f"      âœ… è·å–Kçº¿æ•°æ®: {len(kline_df)} æ¡è®°å½•")
        # é‡å‘½åå­—æ®µä¸ºç¼©å†™å½¢å¼
        kline_df = rename_fields_to_abbreviations(kline_df)
    return kline_df


def resample_to_daily(df: pd.DataFrame, feature_name: str) -> pd.DataFrame:
    """å°†é«˜é¢‘æ•°æ®é‡é‡‡æ ·åˆ°æ—¥çº§åˆ«."""
    if df.empty:
        return df

    # å°†timestampè½¬æ¢ä¸ºæ—¥æœŸ
    df_copy = df.copy()
    df_copy["date"] = pd.to_datetime(df_copy.index.get_level_values("timestamp"), unit="ms").date

    # æŒ‰symbolå’Œdateåˆ†ç»„ï¼Œå–æ¯æ—¥æœ€åä¸€ä¸ªå€¼ï¼ˆä»£è¡¨è¯¥æ—¥çš„æ”¶ç›˜å€¼ï¼‰
    daily_df = df_copy.groupby(["symbol", "date"]).last()

    # é‡æ„ç´¢å¼•ï¼šå°†dateè½¬æ¢å›timestampï¼ˆæ¯æ—¥0ç‚¹çš„æ—¶é—´æˆ³ï¼Œæ¯«ç§’çº§ï¼‰
    daily_df = daily_df.reset_index()
    # ä¿®å¤ï¼šæ­£ç¡®è½¬æ¢ä¸ºæ¯«ç§’æ—¶é—´æˆ³
    daily_df["timestamp"] = pd.to_datetime(daily_df["date"]).astype("int64") // 10**6
    daily_df = daily_df.set_index(["symbol", "timestamp"])
    daily_df = daily_df.drop("date", axis=1)

    print(f"      ğŸ”„ {feature_name}æ•°æ®é‡é‡‡æ ·: {len(df)} -> {len(daily_df)} æ¡è®°å½•")
    return daily_df


async def merge_metrics_data(
    db: Database, combined_df: pd.DataFrame, symbols: list[str], start_date: str, end_date: str
) -> tuple[pd.DataFrame, int]:
    """åˆå¹¶æŒ‡æ ‡æ•°æ®."""
    if not EXPORT_METRICS:
        return combined_df, 0

    metrics_added = 0

    # èµ„é‡‘è´¹ç‡æ•°æ® -> fr (æ¯8å°æ—¶ï¼Œå–æ¯æ—¥æœ€åä¸€ä¸ªå€¼)
    try:
        fr_df = await db.metrics_query.select_funding_rates(symbols, start_date, end_date, ["funding_rate"])
        if not fr_df.empty:
            # é‡é‡‡æ ·åˆ°æ—¥çº§åˆ«
            fr_df = resample_to_daily(fr_df, "èµ„é‡‘è´¹ç‡")
            # é‡å‘½åå­—æ®µä¸ºç¼©å†™å½¢å¼
            fr_df = rename_fields_to_abbreviations(fr_df)
            combined_df = fr_df if combined_df.empty else pd.concat([combined_df, fr_df], axis=1, join="outer")
            metrics_added += 1
            print(f"      âœ… åˆå¹¶èµ„é‡‘è´¹ç‡æ•°æ®: {len(fr_df)} æ¡è®°å½•")
    except Exception as e:
        print(f"      âš ï¸ èµ„é‡‘è´¹ç‡æ•°æ®è·å–å¤±è´¥: {e}")

    # æŒä»“é‡æ•°æ® -> oi (Visioné«˜é¢‘æ•°æ®ï¼Œå–æ¯æ—¥æœ€åä¸€ä¸ªå€¼)
    try:
        oi_df = await db.metrics_query.select_open_interests(symbols, start_date, end_date, columns=["open_interest"])
        if not oi_df.empty:
            # é‡é‡‡æ ·åˆ°æ—¥çº§åˆ«
            oi_df = resample_to_daily(oi_df, "æŒä»“é‡")
            # é‡å‘½åå­—æ®µä¸ºç¼©å†™å½¢å¼
            oi_df = rename_fields_to_abbreviations(oi_df)
            combined_df = oi_df if combined_df.empty else pd.concat([combined_df, oi_df], axis=1, join="outer")
            metrics_added += 1
            print(f"      âœ… åˆå¹¶æŒä»“é‡æ•°æ®: {len(oi_df)} æ¡è®°å½•")
    except Exception as e:
        print(f"      âš ï¸ æŒä»“é‡æ•°æ®è·å–å¤±è´¥: {e}")

    # å¤šç©ºæ¯”ä¾‹æ•°æ® -> åªå¯¼å‡ºtakerç±»å‹ (Visionæ•°æ®æœ€å®Œæ•´çš„ç±»å‹)
    try:
        lsr_df = await db.metrics_query.select_long_short_ratios(
            symbols, start_date, end_date, ratio_type="taker", columns=["long_short_ratio"]
        )
        if not lsr_df.empty:
            # é‡é‡‡æ ·åˆ°æ—¥çº§åˆ«
            lsr_df = resample_to_daily(lsr_df, "å¤šç©ºæ¯”ä¾‹")
            # é‡å‘½åå­—æ®µä¸ºç¼©å†™å½¢å¼
            lsr_df = rename_fields_to_abbreviations(lsr_df)
            combined_df = lsr_df if combined_df.empty else pd.concat([combined_df, lsr_df], axis=1, join="outer")
            metrics_added += 1
            print(f"      âœ… åˆå¹¶å¤šç©ºæ¯”ä¾‹æ•°æ®: {len(lsr_df)} æ¡è®°å½•")
    except Exception as e:
        print(f"      âš ï¸ å¤šç©ºæ¯”ä¾‹æ•°æ®è·å–å¤±è´¥: {e}")

    print(f"      ğŸ“Š æˆåŠŸåˆå¹¶ {metrics_added} ç§æŒ‡æ ‡æ•°æ®")
    return combined_df, metrics_added


async def export_combined_data(db: Database, symbols: list[str], start_date: str, end_date: str, output_path: Path):
    """å¯¼å‡ºåˆå¹¶çš„Kçº¿å’ŒæŒ‡æ ‡æ•°æ®."""
    try:
        print("   ğŸ“ˆ å¯¼å‡ºKçº¿å’ŒæŒ‡æ ‡æ•°æ®...")

        # è·å–Kçº¿æ•°æ®
        kline_df = await get_kline_data(db, symbols, start_date, end_date)
        combined_df = kline_df.copy() if not kline_df.empty else pd.DataFrame()

        # åˆå¹¶æŒ‡æ ‡æ•°æ®
        combined_df, _ = await merge_metrics_data(db, combined_df, symbols, start_date, end_date)

        if combined_df.empty:
            print("      âš ï¸ æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
            return False

        # æ³¨æ„ï¼šæ•°æ®å·²ç»åœ¨å„è‡ªçš„å¤„ç†å‡½æ•°ä¸­é‡é‡‡æ ·åˆ°æ—¥çº§åˆ«ï¼Œè¿™é‡Œä¸å†è¿›è¡ŒäºŒæ¬¡é‡é‡‡æ ·
        export_freq = DATA_FREQ

        # ä½¿ç”¨numpy_exporterçš„å†…éƒ¨æ–¹æ³•ç›´æ¥å¯¼å‡º
        await db.numpy_exporter._export_by_dates(combined_df, output_path, export_freq)

        print(f"      âœ… æ•°æ®å¯¼å‡ºå®Œæˆ: {len(combined_df.columns)} ä¸ªç‰¹å¾ï¼Œ{len(combined_df)} æ¡è®°å½•")
        return True

    except Exception as e:
        print(f"   âŒ æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
        return False


async def export_categories(market_service_ctx, output_path: Path):
    """å¯¼å‡ºåˆ†ç±»æ•°æ®."""
    if not DOWNLOAD_CATEGORIES or market_service_ctx is None:
        return False

    try:
        print("   ğŸ“‚ ä¸‹è½½åˆ†ç±»æ•°æ®...")
        async with market_service_ctx as market_service:
            market_service.download_and_save_categories_for_universe(
                universe_file=UNIVERSE_FILE,
                output_path=output_path,
            )
        print("   âœ… åˆ†ç±»æ•°æ®ä¸‹è½½æˆåŠŸ")
        return True
    except Exception as e:
        print(f"   âŒ åˆ†ç±»æ•°æ®ä¸‹è½½å¤±è´¥: {e}")
        return False


def create_output_path(universe_config, start_date: str, end_date: str) -> Path:
    """åˆ›å»ºè¾“å‡ºè·¯å¾„."""
    config = universe_config
    top_value = f"k{config.top_k}" if config.top_k else f"r{config.top_ratio}"

    if CUSTOM_START_DATE or CUSTOM_END_DATE:
        custom_suffix = f"_custom_{start_date}_{end_date}"
        dir_name = f"univ_{config.t1_months}_{config.t2_months}_{config.t3_months}_{top_value}{custom_suffix}"
    else:
        dir_name = f"univ_{config.t1_months}_{config.t2_months}_{config.t3_months}_{top_value}"

    return Path(EXPORT_BASE_PATH) / dir_name


def show_export_summary(output_path: Path):
    """æ˜¾ç¤ºå¯¼å‡ºæ–‡ä»¶æ‘˜è¦."""
    if output_path.exists():
        npy_files = list(output_path.rglob("*.npy"))
        csv_files = list(output_path.rglob("*.csv"))
        pkl_files = list(output_path.rglob("*.pkl"))

        total_size = sum(f.stat().st_size for f in output_path.rglob("*") if f.is_file()) / (1024 * 1024)

        print("      ğŸ“Š å¯¼å‡ºæ–‡ä»¶æ€»è§ˆ:")
        print(f"         â€¢ NumPyæ–‡ä»¶: {len(npy_files)}ä¸ª")
        print(f"         â€¢ CSVæ–‡ä»¶: {len(csv_files)}ä¸ª")
        print(f"         â€¢ PKLæ–‡ä»¶: {len(pkl_files)}ä¸ª")
        print(f"         â€¢ æ€»å¤§å°: {total_size:.1f} MB")


async def process_snapshot(
    snapshot, snapshot_id: int, total_snapshots: int, universe_config, db: Database, market_service_ctx
):
    """å¤„ç†å•ä¸ªå¿«ç…§çš„å¯¼å‡º."""
    print(f"\nğŸ“‹ å¤„ç†å¿«ç…§ {snapshot_id}/{total_snapshots}: {snapshot.start_date} - {snapshot.end_date}")

    # è®¡ç®—æ—¶é—´èŒƒå›´
    actual_start_ts, actual_end_ts, start_date, end_date = calculate_time_range(snapshot)
    symbols = snapshot.symbols

    # æ˜¾ç¤ºä¿¡æ¯
    if CUSTOM_START_DATE or CUSTOM_END_DATE:
        print(f"   ğŸ“… Universeæ—¶é—´èŒƒå›´: {snapshot.start_date} - {snapshot.end_date}")
        print(f"   ğŸ¯ å®é™…å¯¼å‡ºèŒƒå›´: {start_date} - {end_date}")
    else:
        print(f"   â° å¯¼å‡ºæ—¶é—´èŒƒå›´: {start_date} - {end_date}")

    print(f"   ğŸ’± äº¤æ˜“å¯¹æ•°é‡: {len(symbols)}")
    print(f"   ğŸ“ å‰5ä¸ªäº¤æ˜“å¯¹: {symbols[:5]}")

    # æ£€æŸ¥æ—¶é—´èŒƒå›´
    if actual_start_ts >= actual_end_ts:
        print("   âš ï¸ è­¦å‘Š: å¯¼å‡ºæ—¶é—´èŒƒå›´ä¸ºç©ºï¼Œè·³è¿‡æ­¤å¿«ç…§")
        return {"success": False, "reason": "Empty time range"}

    # åˆ›å»ºè¾“å‡ºè·¯å¾„
    output_path = create_output_path(universe_config, start_date, end_date)

    # å¯¼å‡ºæ•°æ®
    results = {}
    results["data"] = await export_combined_data(db, symbols, start_date, end_date, output_path)
    results["categories"] = await export_categories(market_service_ctx, output_path)

    # æ˜¾ç¤ºæ‘˜è¦
    show_export_summary(output_path)

    return {"success": any(results.values()), "details": results, "output_path": output_path}


async def main():
    """ä¸»å‡½æ•°."""
    print("ğŸ“¤ å¼€å§‹ä»æ•°æ®åº“å¯¼å‡ºæ•°æ®")
    print(f"ğŸ“‹ Universeæ–‡ä»¶: {UNIVERSE_FILE}")
    print(f"ğŸ’¾ æ•°æ®åº“è·¯å¾„: {DB_PATH}")
    print(f"ğŸ“ å¯¼å‡ºè·¯å¾„: {EXPORT_BASE_PATH}")
    print(f"â±ï¸ å¯¼å‡ºé¢‘ç‡: {EXPORT_FREQ}")

    # æ˜¾ç¤ºå¯¼å‡ºçš„ç‰¹å¾
    kline_features = ["opn", "hgh", "low", "cls", "vol", "amt", "tnum", "tbvol", "tbamt", "tsvol", "tsamt"]
    metrics_features = ["fr", "oi", "lsr"]

    if EXPORT_KLINES and EXPORT_METRICS:
        all_features = kline_features + metrics_features
        print(f"ğŸ“Š å¯¼å‡ºç‰¹å¾: {len(all_features)}ä¸ª - {', '.join(all_features)}")
    elif EXPORT_KLINES:
        print(f"ğŸ“ˆ å¯¼å‡ºKçº¿ç‰¹å¾: {len(kline_features)}ä¸ª - {', '.join(kline_features)}")
    elif EXPORT_METRICS:
        print(f"ğŸ“Š å¯¼å‡ºæŒ‡æ ‡ç‰¹å¾: {len(metrics_features)}ä¸ª - {', '.join(metrics_features)}")

    print(f"ğŸŒ åˆ†ç±»æ•°æ®ä¸‹è½½: {'å¯ç”¨' if DOWNLOAD_CATEGORIES else 'ç¦ç”¨'}")

    if CUSTOM_START_DATE or CUSTOM_END_DATE:
        print("ğŸ¯ è‡ªå®šä¹‰æ—¶é—´èŒƒå›´:")
        if CUSTOM_START_DATE:
            print(f"    ğŸ“… å¼€å§‹æ—¥æœŸ: {CUSTOM_START_DATE}")
        if CUSTOM_END_DATE:
            print(f"    ğŸ“… ç»“æŸæ—¥æœŸ: {CUSTOM_END_DATE}")

    try:
        # åŠ è½½é…ç½®
        universe_def = await validate_and_load_universe()
        market_service_ctx = await create_market_service()

        # åˆå§‹åŒ–æ•°æ®åº“
        db = Database(DB_PATH)
        await db.initialize()

        try:
            # å¤„ç†æ¯ä¸ªå¿«ç…§
            results = []
            for i, snapshot in enumerate(universe_def.snapshots):
                result = await process_snapshot(
                    snapshot,
                    i + 1,
                    len(universe_def.snapshots),
                    universe_def.config,
                    db,
                    market_service_ctx,
                )
                results.append(result)

            # æ±‡æ€»ç»“æœ
            print("\n" + "=" * 60)
            print("ğŸ¯ å¯¼å‡ºå®Œæˆæ±‡æ€»:")
            successful = sum(1 for r in results if r["success"])
            print(f"   ğŸ“Š æ€»å¿«ç…§æ•°: {len(results)}")
            print(f"   âœ… æˆåŠŸå¯¼å‡º: {successful}/{len(results)}")

            # æ˜¾ç¤ºå…·ä½“å¯¼å‡ºæƒ…å†µ
            data_success = sum(1 for r in results if r["details"].get("data", False))
            category_success = sum(1 for r in results if r["details"].get("categories", False))
            print(f"   ğŸ“Š æ•°æ®å¯¼å‡ºæˆåŠŸ: {data_success}/{len(results)}")
            print(f"   ğŸ“‚ åˆ†ç±»æ•°æ®æˆåŠŸ: {category_success}/{len(results)}")

            if successful == len(results):
                print("   ğŸ‰ æ‰€æœ‰æ•°æ®å¯¼å‡ºæˆåŠŸï¼")
            else:
                print("   âš ï¸ éƒ¨åˆ†å¿«ç…§å¯¼å‡ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
            print("=" * 60)

        finally:
            await db.close()

    except Exception as e:
        print(f"âŒ æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        raise


if __name__ == "__main__":
    asyncio.run(main())
