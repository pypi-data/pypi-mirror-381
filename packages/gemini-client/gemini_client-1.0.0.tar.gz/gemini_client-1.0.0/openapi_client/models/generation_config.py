# coding: utf-8

"""
    GenerativeService API

    API for using Large Models that generate multimodal content and have  additional capabilities beyond text generation.

    The version of the OpenAPI document: 0.0.1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictFloat, StrictInt, StrictStr
from typing import Any, ClassVar, Dict, List, Optional, Union
from openapi_client.models.model_schema import ModelSchema
from openapi_client.models.speech_config import SpeechConfig
from openapi_client.models.thinking_config import ThinkingConfig
from typing import Optional, Set
from typing_extensions import Self

class GenerationConfig(BaseModel):
    """
    Configuration options for model generation and outputs. Not all parameters  are configurable for every model.
    """ # noqa: E501
    candidate_count: Optional[StrictInt] = Field(default=None, description="Optional. Number of generated responses to return. If unset, this will  default to 1. Please note that this doesn't work for previous generation  models (Gemini 1.0 family)", alias="candidateCount")
    stop_sequences: Optional[List[StrictStr]] = Field(default=None, description="Optional. The set of character sequences (up to 5) that will stop output  generation. If specified, the API will stop at the first appearance of a  `stop_sequence`. The stop sequence will not be included as part of the  response.", alias="stopSequences")
    max_output_tokens: Optional[StrictInt] = Field(default=None, description="Optional. The maximum number of tokens to include in a response candidate.   Note: The default value varies by model, see the `Model.output_token_limit`  attribute of the `Model` returned from the `getModel` function.", alias="maxOutputTokens")
    temperature: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Optional. Controls the randomness of the output.   Note: The default value varies by model, see the `Model.temperature`  attribute of the `Model` returned from the `getModel` function.   Values can range from [0.0, 2.0].")
    top_p: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Optional. The maximum cumulative probability of tokens to consider when  sampling.   The model uses combined Top-k and Top-p (nucleus) sampling.   Tokens are sorted based on their assigned probabilities so that only the  most likely tokens are considered. Top-k sampling directly limits the  maximum number of tokens to consider, while Nucleus sampling limits the  number of tokens based on the cumulative probability.   Note: The default value varies by `Model` and is specified by  the`Model.top_p` attribute returned from the `getModel` function. An empty  `top_k` attribute indicates that the model doesn't apply top-k sampling  and doesn't allow setting `top_k` on requests.", alias="topP")
    top_k: Optional[StrictInt] = Field(default=None, description="Optional. The maximum number of tokens to consider when sampling.   Gemini models use Top-p (nucleus) sampling or a combination of Top-k and  nucleus sampling. Top-k sampling considers the set of `top_k` most probable  tokens. Models running with nucleus sampling don't allow top_k setting.   Note: The default value varies by `Model` and is specified by  the`Model.top_p` attribute returned from the `getModel` function. An empty  `top_k` attribute indicates that the model doesn't apply top-k sampling  and doesn't allow setting `top_k` on requests.", alias="topK")
    seed: Optional[StrictInt] = Field(default=None, description="Optional. Seed used in decoding. If not set, the request uses a randomly  generated seed.")
    response_mime_type: Optional[StrictStr] = Field(default=None, description="Optional. MIME type of the generated candidate text.  Supported MIME types are:  `text/plain`: (default) Text output.  `application/json`: JSON response in the response candidates.  `text/x.enum`: ENUM as a string response in the response candidates.  Refer to the  [docs](https://ai.google.dev/gemini-api/docs/prompting_with_media#plain_text_formats)  for a list of all supported text MIME types.", alias="responseMimeType")
    response_schema: Optional[ModelSchema] = Field(default=None, description="Optional. Output schema of the generated candidate text. Schemas must be a  subset of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schema)  and can be objects, primitives or arrays.   If set, a compatible `response_mime_type` must also be set.  Compatible MIME types:  `application/json`: Schema for JSON response.  Refer to the [JSON text generation  guide](https://ai.google.dev/gemini-api/docs/json-mode) for more details.", alias="responseSchema")
    response_json_schema: Optional[Any] = Field(default=None, description="Optional. Output schema of the generated response. This is an alternative  to `response_schema` that accepts [JSON Schema](https://json-schema.org/).   If set, `response_schema` must be omitted, but `response_mime_type` is  required.   While the full JSON Schema may be sent, not all features are supported.  Specifically, only the following properties are supported:   - `$id`  - `$defs`  - `$ref`  - `$anchor`  - `type`  - `format`  - `title`  - `description`  - `enum` (for strings and numbers)  - `items`  - `prefixItems`  - `minItems`  - `maxItems`  - `minimum`  - `maximum`  - `anyOf`  - `oneOf` (interpreted the same as `anyOf`)  - `properties`  - `additionalProperties`  - `required`   The non-standard `propertyOrdering` property may also be set.   Cyclic references are unrolled to a limited degree and, as such, may only  be used within non-required properties. (Nullable properties are not  sufficient.) If `$ref` is set on a sub-schema, no other properties, except  for than those starting as a `$`, may be set.", alias="responseJsonSchema")
    presence_penalty: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Optional. Presence penalty applied to the next token's logprobs if the  token has already been seen in the response.   This penalty is binary on/off and not dependant on the number of times the  token is used (after the first). Use  [frequency_penalty][google.ai.generativelanguage.v1beta.GenerationConfig.frequency_penalty]  for a penalty that increases with each use.   A positive penalty will discourage the use of tokens that have already  been used in the response, increasing the vocabulary.   A negative penalty will encourage the use of tokens that have already been  used in the response, decreasing the vocabulary.", alias="presencePenalty")
    frequency_penalty: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Optional. Frequency penalty applied to the next token's logprobs,  multiplied by the number of times each token has been seen in the respponse  so far.   A positive penalty will discourage the use of tokens that have already  been used, proportional to the number of times the token has been used:  The more a token is used, the more difficult it is for the model to use  that token again increasing the vocabulary of responses.   Caution: A _negative_ penalty will encourage the model to reuse tokens  proportional to the number of times the token has been used. Small  negative values will reduce the vocabulary of a response. Larger negative  values will cause the model to start repeating a common token  until it  hits the  [max_output_tokens][google.ai.generativelanguage.v1beta.GenerationConfig.max_output_tokens]  limit.", alias="frequencyPenalty")
    response_logprobs: Optional[StrictBool] = Field(default=None, description="Optional. If true, export the logprobs results in response.", alias="responseLogprobs")
    logprobs: Optional[StrictInt] = Field(default=None, description="Optional. Only valid if  [response_logprobs=True][google.ai.generativelanguage.v1beta.GenerationConfig.response_logprobs].  This sets the number of top logprobs to return at each decoding step in the  [Candidate.logprobs_result][google.ai.generativelanguage.v1beta.Candidate.logprobs_result].")
    enable_enhanced_civic_answers: Optional[StrictBool] = Field(default=None, description="Optional. Enables enhanced civic answers. It may not be available for all  models.", alias="enableEnhancedCivicAnswers")
    response_modalities: Optional[List[StrictInt]] = Field(default=None, description="Optional. The requested modalities of the response. Represents the set of  modalities that the model can return, and should be expected in the  response. This is an exact match to the modalities of the response.   A model may have multiple combinations of supported modalities. If the  requested modalities do not match any of the supported combinations, an  error will be returned.   An empty list is equivalent to requesting only text.", alias="responseModalities")
    speech_config: Optional[SpeechConfig] = Field(default=None, description="Optional. The speech generation config.", alias="speechConfig")
    thinking_config: Optional[ThinkingConfig] = Field(default=None, description="Optional. Config for thinking features.  An error will be returned if this field is set for models that don't  support thinking.", alias="thinkingConfig")
    media_resolution: Optional[StrictInt] = Field(default=None, description="Optional. If specified, the media resolution specified will be used.", alias="mediaResolution")
    __properties: ClassVar[List[str]] = ["candidateCount", "stopSequences", "maxOutputTokens", "temperature", "topP", "topK", "seed", "responseMimeType", "responseSchema", "responseJsonSchema", "presencePenalty", "frequencyPenalty", "responseLogprobs", "logprobs", "enableEnhancedCivicAnswers", "responseModalities", "speechConfig", "thinkingConfig", "mediaResolution"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of GenerationConfig from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of response_schema
        if self.response_schema:
            _dict['responseSchema'] = self.response_schema.to_dict()
        # override the default output from pydantic by calling `to_dict()` of speech_config
        if self.speech_config:
            _dict['speechConfig'] = self.speech_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of thinking_config
        if self.thinking_config:
            _dict['thinkingConfig'] = self.thinking_config.to_dict()
        # set to None if response_json_schema (nullable) is None
        # and model_fields_set contains the field
        if self.response_json_schema is None and "response_json_schema" in self.model_fields_set:
            _dict['responseJsonSchema'] = None

        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of GenerationConfig from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "candidateCount": obj.get("candidateCount"),
            "stopSequences": obj.get("stopSequences"),
            "maxOutputTokens": obj.get("maxOutputTokens"),
            "temperature": obj.get("temperature"),
            "topP": obj.get("topP"),
            "topK": obj.get("topK"),
            "seed": obj.get("seed"),
            "responseMimeType": obj.get("responseMimeType"),
            "responseSchema": ModelSchema.from_dict(obj["responseSchema"]) if obj.get("responseSchema") is not None else None,
            "responseJsonSchema": obj.get("responseJsonSchema"),
            "presencePenalty": obj.get("presencePenalty"),
            "frequencyPenalty": obj.get("frequencyPenalty"),
            "responseLogprobs": obj.get("responseLogprobs"),
            "logprobs": obj.get("logprobs"),
            "enableEnhancedCivicAnswers": obj.get("enableEnhancedCivicAnswers"),
            "responseModalities": obj.get("responseModalities"),
            "speechConfig": SpeechConfig.from_dict(obj["speechConfig"]) if obj.get("speechConfig") is not None else None,
            "thinkingConfig": ThinkingConfig.from_dict(obj["thinkingConfig"]) if obj.get("thinkingConfig") is not None else None,
            "mediaResolution": obj.get("mediaResolution")
        })
        return _obj


