"""Bridge between Univia AICore framework and Wodan/Conan processor modules"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/11_aicorebridge.ipynb.

# %% auto 0
__all__ = ['annotated_arg_builders', 'pop_nan_values', 'build_historic_args', 'AICoreModuleBase', 'AICoreModule']

# %% ../nbs/11_aicorebridge.ipynb 4
import typing
import logging
import traceback
import inspect
import platform
import datetime
import time
import pandas as pd

import numpy as np

from dateutil import parser
from fastcore.basics import patch
from . import __version__

from .core import snake_case_to_camel_case, NumpyEncoder
from .timeseriesdataframe import timeseries_dataframe, timeseries_dataframe_resample, timeseries_dataframe_to_datadict
from .timeseriesdataframe import set_time_index_zone, timeseries_dataframe_from_datadict



# %% ../nbs/11_aicorebridge.ipynb 5
#| eval: false
syslog = logging.getLogger(__name__)

try:
    syslog.debug(f"Loading {__name__} {__version__} from {__file__}")
except:  # noqa: E722
    pass

# %% ../nbs/11_aicorebridge.ipynb 10
def pop_nan_values(data):
    """
    Recursively pop keys with nan values from dict or lists with 
    dicts. Use just before handing data to AICore for further
    processing since it explodes when encountering NaN values.

    Args:
        data (Union[list, dict]): The data to be processed.

    Returns:
        Union[list, dict]: The processed data with keys with nan values removed.
    """
    
    if isinstance(data, list):
        return [pop_nan_values(v) for v in data if pd.notnull([v]).any()]
    elif isinstance(data, dict):
        return {k:pop_nan_values(v) for k, v in data.items() if pd.notnull([v]).any()}
    else:
        return data

# %% ../nbs/11_aicorebridge.ipynb 14
def build_historic_args(
        data:pd.DataFrame, 
        history:dict|list
    ) -> dict:

    """Create a timeseries DataFrame from historic data defined in `history`.

    Parameters
    ----------
    data : pd.DataFrame
        The input time-series DataFrame.
    history : dict or list of dicts
        Historic data definition, each item in the list is a dictionary with 
        a startDate key to set the start of a section of historic data in the 
        result and a column-value pair for each of the columns.

    Returns
    -------
    historic_data : dict
        Historic data in dictionary format where keys are column names and 
        values are the historic values as numpy array.
    """

    if not history:
        return {}
    
    if isinstance(history, dict):
        return pd.DataFrame(history, index=data.index)
    
    if not isinstance(history, list):
        return {}
    
    if isinstance(data, pd.DataFrame):
        dates = data.index.astype(np.int64).astype(np.float64) / 1e9
        dates = dates.to_numpy()
    elif data.dtype.names is not None:
        dates = data.view(dtype=np.float64).reshape(data.shape[0],len(data.dtype))[:,0]
    else:
        dates = data[:,0]

    dates = dates.astype(np.int64)
    
    columns = list(set([K for item in history for K in item.keys() if K != 'startDate']))
    column_data = {K:np.full(len(dates), np.nan, dtype=np.float64) for K in columns}

    for item in history:
        date = parser.parse(str((item.pop('startDate','2000-01-01T00:00:00+00:00')))).timestamp()
        mask = np.greater_equal(dates, date)
        for K,V in item.items():
            column_data[K][mask] = V
    
    return pd.DataFrame(column_data, index=data.index)


# %% ../nbs/11_aicorebridge.ipynb 21
class AICoreModuleBase:

    def __init__(
        self, 
        files_dir, 
        save_dir
    ):
        
        self.init_time = datetime.datetime.now(datetime.UTC)
        self.aicorebridge_version = __version__

        self.init_args = []
        self.init_kwargs = dict(
            files_dir=files_dir,
            save_dir=save_dir
        )


        syslog.info(f"Init {self.__class__.__name__}, version {self.aicorebridge_version}, files directory {files_dir}, save dir {save_dir} on {platform.node()}")


# %% ../nbs/11_aicorebridge.ipynb 25
class AICoreModule(AICoreModuleBase):
    def __init__(self, 
        processor:typing.Callable, # data processing function
        files_dir:str,              # path where the module can keep files 
        save_dir:str
    ):
    
        super().__init__(files_dir, save_dir)
        self._init_processor(processor)



# %% ../nbs/11_aicorebridge.ipynb 26
# TODO: Refactor into Processor classes to handle different funtion types

@patch
def _init_processor(
        self:AICoreModule, 
        processor:typing.Callable):
    """Initializes processor related variables on self"""
    
    self.processor = processor
    self.processor_signature = inspect.signature(self.processor)
    self.processor_params = dict(self.processor_signature.parameters)
    self.return_param = self.processor_params.pop('return', None)
    
    self.data_param, *self.call_params = list(self.processor_params.keys())

    if not (
        self.processor_params[self.data_param].annotation == pd.DataFrame
        or self.processor_params[self.data_param].annotation == np.ndarray

    ):

        self.data_param = None
        self.call_params = list(self.processor_params.keys())



# %% ../nbs/11_aicorebridge.ipynb 27
# can be overloaded
@patch
def call_processor(self:AICoreModule, calldata, **callargs):
    if self.data_param:
        return self.processor(calldata, **callargs)
    else:
        return self.processor(**callargs)


# %% ../nbs/11_aicorebridge.ipynb 29
@patch
def call(self:AICoreModule, data:dict, *_, **__):
    """Infer the data using the processor function."""

    payload_data = data

    msg=[
        f"Startup: time {self.init_time.isoformat()}, node {platform.node()}",
        f"Corebridge version: {self.aicorebridge_version}",
    ]

    try:
        t00 = time.perf_counter_ns()

        kwargs = payload_data.get('kwargs', {})
        data = payload_data.get('data', {})

        msg+=[
            f"{self.processor.__name__}({self.processor_signature})",  
            f"Data: {type(data)} length: {len(data)}",    
            f"kwargs {list(kwargs.keys())}",       
            #f"init_args: {self.init_args}, init_kwargs: {self.init_kwargs}",
        ]

        # Pickup params, pop those that are not intended for the processor
        lastSeen = kwargs.pop('lastSeen', False)
        recordformat = kwargs.pop('format', "records").lower()
        timezone = kwargs.get('timezone', 'UTC')
        nested = kwargs.pop('nested', False)
        msg.append(f"lastSeen: {lastSeen}, timezone: {timezone}, recordformat: {recordformat}, nested: {nested}")

        samplerPeriod = kwargs.pop('samplerPeriod', self.init_kwargs.get('samplerPeriod','h'))
        samplerMethod = kwargs.pop('samplerMethod', self.init_kwargs.get('samplerMethod',None))
        reversed = kwargs.pop('reversed', False)

        calldata = self.get_call_data(
            data, 
            recordformat=recordformat,
            timezone=timezone,
            nested=nested,)
        
        history = build_historic_args(calldata, kwargs.pop('history', {}))
        callargs = self.get_callargs(kwargs, history)

        # for arg, val in callargs.items():
        #     msg.append(f"{arg}: {val}")
        
        t02 = time.perf_counter_ns()
        calculated_result = self.call_processor(
            calldata, 
            **callargs
        )
        t03 = time.perf_counter_ns()
        msg.append(f"Processing time: {(t03-t02)/1e6:.1f} ms")
        msg.append(f"Preparation time: {(t02-t00)/1e6:.1f} ms")

        if isinstance(calculated_result, dict):
            msg.append(f"return-data ictionary keys: {calculated_result.keys()}")
            return {
                'msg':msg,
                'data': [calculated_result]
            }
        elif isinstance(calculated_result, list):
            msg.append(f"return-data list length: {len(calculated_result)}")
            return {
                'msg':msg,
                'data': calculated_result
            }

        try:
            result = timeseries_dataframe(
                calculated_result, 
                timezone=timezone)
            
            msg.append(f"result shape: {result.shape}")

            if samplerMethod:
                msg.append(f"Sampler: {samplerMethod}, period: {samplerPeriod}")
                result = timeseries_dataframe_resample(result, samplerPeriod, samplerMethod)

            msg.append(f"return-data shape: {result.shape}")

            if reversed:
                result = result[::-1]

            return {
                'msg':msg,
                'data': pop_nan_values( timeseries_dataframe_to_datadict(
                    result if not lastSeen else result[-1:],
                    recordformat=recordformat,
                    timezone=timezone))
            }
        
        # tries dataframe return
        except Exception as err:
            msg.append(f"No timeseries data, error={err}")
        
        df = pd.DataFrame(calculated_result)
        df
        df.columns = [f"value_{str(c)}" if isinstance(c, int) else str(c) for c in list(df.columns)]
        df.reset_index().to_dict(orient='records')
        return {
            'msg':msg,
            'data': pop_nan_values( df.reset_index().to_dict(orient='records') )
        }

    
    # function try-catch
    except Exception as err:
        msg.append(''.join(traceback.format_exception(None, err, err.__traceback__)))
        return {
            'msg': msg,
            'data': []
        }


# %% ../nbs/11_aicorebridge.ipynb 31
# Specialized types for initializing annotated parameters
# Add types by adding a tuple with the type name and a builder function
annotated_arg_builders = {
    str(B[0]):B[1] for B in [
        (np.ndarray, lambda X: np.array(X, dtype=X.dtype))
    ]
}


# %% ../nbs/11_aicorebridge.ipynb 33
@patch
def init_annotated_param(self:AICoreModule, param_name, value):
    """
    Initialize argument for the processor call
    
    param_name: name of the parameter to be initialized
    value: value of the parameter read from infer data to be used for initialization
    
    """

    annotation = self.processor_signature.parameters[param_name].annotation
    #print(f"param_name: {param_name}, value: {value}, annotation: {annotation}")

    # try to convert value to one of the types in the builders of annotated_arg_builders
    for T in typing.get_args(annotation):
        try:
            builder = annotated_arg_builders.get(str(T), lambda X:T(X))
            return builder(value)
        
        except TypeError as err:
            continue

    try:
        return annotation(value)
    
    except TypeError as err:
        syslog.exception(f"Exception {str(err)} in fallback conversion to {annotation} of {type(value)}")

 

# %% ../nbs/11_aicorebridge.ipynb 34
@patch
def get_callargs(self:AICoreModule, kwargs, history):
    "Get arguments for the processor call"

    # Remove null / None values
    kwargs = {k:v for k,v in kwargs.items() if v is not None}
    
    call_args = {
        K:self.init_annotated_param(
            K,
            history.get(
                K,
                kwargs.get(
                    K,
                    self.init_kwargs.get(
                        K, 
                        history.get(
                            snake_case_to_camel_case(K),
                            kwargs.get(
                                snake_case_to_camel_case(K),
                                self.init_kwargs.get(
                                    snake_case_to_camel_case(K), 
                                    self.processor_signature.parameters[K].default
                                )
                            )
                        )
                    )
                )
            )
        )
        for K in self.call_params
    }

    return call_args


# %% ../nbs/11_aicorebridge.ipynb 37
@patch
def get_call_data(
        self:AICoreModule, 
        data:dict|list, 
        recordformat='records', 
        timezone='UTC',
        nested=False):
    
    "Convert data to the processor signature"
    
    if not self.data_param:
        return None
    
    print(f"recordformat: {recordformat}, timezone: {timezone}, nested: {nested}" )

    df = set_time_index_zone(timeseries_dataframe_from_datadict(
        data, ['datetimeMeasure', 'time'], recordformat=recordformat, nested=nested), timezone)

    df.sort_index(inplace=True)

    if self.processor_params[self.data_param].annotation == pd.DataFrame:
        return df
    elif len(df.columns) > 1:
        df.index = (df.index - datetime.datetime(1970,1,1, tzinfo=datetime.timezone.utc)) / datetime.timedelta(seconds=1)
        return df.to_records(index=True)
    else:
        df.index = (df.index - datetime.datetime(1970,1,1, tzinfo=datetime.timezone.utc)) / datetime.timedelta(seconds=1)
        return df.reset_index().to_numpy()
        
