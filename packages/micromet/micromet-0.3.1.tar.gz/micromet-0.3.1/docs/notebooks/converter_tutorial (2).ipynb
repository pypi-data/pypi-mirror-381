{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01978474",
   "metadata": {},
   "source": [
    "# AmeriFlux Data Processing with `converter.py`\n",
    "This tutorial notebook demonstrates how to use the two key classes in `converter.py` — **`AmerifluxDataProcessor`** and **`Reformatter`** — to\n",
    "1. Parse raw Campbell Scientific **TOA5** or **AmeriFlux Level‑2** CSV files.\n",
    "2. Clean, standardize, and resample them for downstream analysis.\n",
    "\n",
    "Run each code cell sequentially, adjusting the file paths to your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e5a1e",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "```bash\n",
    "pip install pandas numpy pyyaml\n",
    "```\n",
    "Make sure the *micromet* module (or the individual `converter.py` file plus its helpers) is on your `PYTHONPATH`, or is located in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cd309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, pathlib, sys, pandas as pd\n",
    "sys.path.append(\"../../src/\")\n",
    "from micromet import AmerifluxDataProcessor, Reformatter\n",
    "\n",
    "# Show informational messages from the helper classes\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef5aba9",
   "metadata": {},
   "source": [
    "## 1. Parse a raw datalogger file\n",
    "The processor detects whether the file is **TOA5** (four‑row header) or already in AmeriFlux Level‑2 format (single header row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e073a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ➡️  Update this path to point to a real file on your system\n",
    "example_file = pathlib.Path('station_data/US-CdM/21020_Flux_AmeriFluxFormat_2.dat')\n",
    "\n",
    "proc = AmerifluxDataProcessor()\n",
    "raw_df = proc.to_dataframe(example_file)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbda63d",
   "metadata": {},
   "source": [
    "## 2. Clean & resample with `Reformatter`\n",
    "This will:\n",
    "- Convert timestamps to `datetime` and enforce 30‑min spacing\n",
    "- Rename columns to a consistent schema\n",
    "- Remove obvious outliers using `variable_limits.py`\n",
    "- Apply several variable‑specific fixes (e.g., Tau zeros, SWC percent‑to‑fraction)\n",
    "- (Optionally) drop redundant soil columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c764530",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = Reformatter(drop_soil=False)  # set to True to drop extra soil channels\n",
    "clean_df = rf.prepare(raw_df)\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d335e8",
   "metadata": {},
   "source": [
    "### Quick diagnostics\n",
    "Use the standard pandas tools to verify the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d0815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic stats for a few key variables\n",
    "clean_df[['NETRAD', 'SW_IN_1_1_2', 'SWC_3_1_1']].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e1da46",
   "metadata": {},
   "source": [
    "## 3. Compile multiple raw files\n",
    "If your logger writes many daily files, `raw_file_compile` can merge them into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd1472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled = proc.raw_file_compile(\n",
    "    main_dir='station_data',\n",
    "    station_folder_name='Cedar_mesa',\n",
    "    search_str='*Flux_AmeriFluxFormat*.dat'\n",
    ")\n",
    "compiled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea58a4c",
   "metadata": {},
   "source": [
    "## 4. Batch processing for many stations\n",
    "`proc.iterate_through_stations()` loops through a hard‑coded dictionary of station IDs and compiles data for each. Modify the dictionary inside `converter.py` or supply your own loop for full control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6118068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc.iterate_through_stations()  # Uncomment to run (may take a while)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ec5fc4",
   "metadata": {},
   "source": [
    "## Appendix – Configuration files\n",
    "- `reformatter_vars.py` – column rename maps, soil sensor groupings, etc.\n",
    "- `variable_limits.py` – hard min/max limits for QC.\n",
    "\n",
    "Feel free to adjust these to match your particular station setup."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
