{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6cb5d9c8ae57b4",
   "metadata": {},
   "source": [
    "This notebook was used to compile all of the available data from the Utah Flux Network stations.  It should only need to be used once, as other notebooks are used to comile the newer data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e587dad2",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d356450d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T14:28:18.408562Z",
     "start_time": "2025-09-04T14:28:15.196174Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "sys.path.append(\"../../src/\")\n",
    "import micromet\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c349cf54",
   "metadata": {},
   "source": [
    "## Initialize Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24f0462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setFormatter(\n",
    "    logging.Formatter(\n",
    "        fmt=\"%(levelname)s [%(asctime)s] %(name)s – %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    ")\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2354275e",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375909fd",
   "metadata": {},
   "source": [
    "## Fill NA Drop Dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c0e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_drop_dups(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.columns:\n",
    "        if col.endswith(\".1\"):\n",
    "            col1 = col[:-2]\n",
    "            col2 = col\n",
    "            # Treat -9999 as missing\n",
    "            s1 = df[col1].replace(-9999, np.nan)\n",
    "            s2 = df[col2].replace(-9999, np.nan)\n",
    "            df[col1] = s1.combine_first(s2).fillna(-9999)\n",
    "\n",
    "            df = df.drop([col2], axis=1)\n",
    "        elif col.endswith(\".2\"):\n",
    "            col1 = col[:-2]\n",
    "            col2 = col\n",
    "            # Treat -9999 as missing\n",
    "            s1 = df[col1].replace(-9999, np.nan)\n",
    "            s2 = df[col2].replace(-9999, np.nan)\n",
    "            df[col1] = s1.combine_first(s2).fillna(-9999)\n",
    "\n",
    "            df = df.drop([col2], axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9efe547",
   "metadata": {},
   "source": [
    "## Summarize Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c282dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_gaps(\n",
    "    df: pd.DataFrame,\n",
    "    station_level: str = \"STATIONID\",\n",
    "    time_level: str = \"DATETIME_START\",\n",
    "    expected_freq: str = \"30min\",\n",
    "    columns: list | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarize runs of missing data (NaNs) per column for each station in a\n",
    "    MultiIndex DataFrame indexed by (station, datetime).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame with a MultiIndex (station_level, time_level).\n",
    "    station_level : str, default \"STATIONID\"\n",
    "        Name of the station level in the index.\n",
    "    time_level : str, default \"DATETIME_START\"\n",
    "        Name of the datetime level in the index.\n",
    "    expected_freq : str, default \"30min\"\n",
    "        The expected sampling frequency. Used to build a complete timeline per station\n",
    "        so that missing timestamps become explicit NaNs.\n",
    "    columns : list[str] | None\n",
    "        Subset of columns to analyze. Defaults to all columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns:\n",
    "            - STATIONID\n",
    "            - COLUMN\n",
    "            - GAP_START\n",
    "            - GAP_END\n",
    "            - N_STEPS_MISSING\n",
    "            - HOURS_MISSING\n",
    "            - GAP_KIND  (\"MissingTimestamp\", \"NaN\", or \"Mixed\")\n",
    "    \"\"\"\n",
    "    if not isinstance(df.index, pd.MultiIndex):\n",
    "        raise TypeError(\"df must have a MultiIndex (station, datetime).\")\n",
    "\n",
    "    if station_level not in df.index.names or time_level not in df.index.names:\n",
    "        raise KeyError(\"MultiIndex must contain the specified station_level and time_level.\")\n",
    "\n",
    "    # Work on a sorted copy\n",
    "    df = df.copy()\n",
    "    df = df.sort_index()\n",
    "\n",
    "    if columns is None:\n",
    "        columns = list(df.columns)\n",
    "\n",
    "    # Frequency as a Timedelta (e.g., 30 minutes)\n",
    "    freq_td = pd.Timedelta(to_offset(expected_freq))\n",
    "    hours_per_step = freq_td / pd.Timedelta(hours=1)\n",
    "\n",
    "    records = []\n",
    "\n",
    "    # Iterate station by station\n",
    "    stations = df.index.get_level_values(station_level).unique()\n",
    "    for stn in stations:\n",
    "        # Slice one station: index becomes time_level\n",
    "        dfx = df.xs(stn, level=station_level)\n",
    "\n",
    "        # Ensure the time index is datetime and sorted\n",
    "        time_idx = pd.to_datetime(dfx.index)\n",
    "        dfx = dfx.set_index(time_idx).sort_index()\n",
    "        original_idx = dfx.index\n",
    "\n",
    "        # Build a complete timeline so *missing timestamps* are turned into NaNs\n",
    "        full_idx = pd.date_range(start=original_idx.min(), end=original_idx.max(), freq=expected_freq)\n",
    "        # Mask telling which timestamps were missing in the original index\n",
    "        missing_row_mask = pd.Series(~pd.Index(full_idx).isin(original_idx), index=full_idx)\n",
    "\n",
    "        # Reindex to full timeline\n",
    "        dfr = dfx.reindex(full_idx)\n",
    "\n",
    "        for col in columns:\n",
    "            col_na = dfr[col].isna()\n",
    "            if not col_na.any():\n",
    "                continue  # no gaps for this column\n",
    "\n",
    "            # Label contiguous runs (True/False) and keep only True-runs (gaps)\n",
    "            run_id = (col_na != col_na.shift(1)).cumsum()\n",
    "            for rid, run_mask in col_na.groupby(run_id):\n",
    "                if not run_mask.iloc[0]:\n",
    "                    continue  # this run is of non-NaNs\n",
    "\n",
    "                run_times = run_mask.index\n",
    "                gap_start = run_times[0]\n",
    "                gap_end = run_times[-1]\n",
    "                n_steps = int(run_mask.sum())\n",
    "\n",
    "                # Determine the kind of gap: missing timestamps vs NaNs vs mixed\n",
    "                row_missing_in_run = missing_row_mask.loc[run_times]\n",
    "                if row_missing_in_run.all():\n",
    "                    kind = \"MissingTimestamp\"\n",
    "                elif not row_missing_in_run.any():\n",
    "                    kind = \"NaN\"\n",
    "                else:\n",
    "                    kind = \"Mixed\"\n",
    "\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"STATIONID\": stn,\n",
    "                        \"COLUMN\": col,\n",
    "                        \"GAP_START\": gap_start,\n",
    "                        \"GAP_END\": gap_end,\n",
    "                        \"N_STEPS_MISSING\": n_steps,\n",
    "                        \"HOURS_MISSING\": n_steps * hours_per_step,\n",
    "                        \"GAP_KIND\": kind,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    out = pd.DataFrame.from_records(records)\n",
    "    if not out.empty:\n",
    "        out = out.sort_values([\"STATIONID\", \"COLUMN\", \"GAP_START\"]).reset_index(drop=True)\n",
    "    else:\n",
    "        # Ensure expected columns even when no gaps\n",
    "        out = pd.DataFrame(\n",
    "            columns=[\"STATIONID\", \"COLUMN\", \"GAP_START\", \"GAP_END\",\n",
    "                     \"N_STEPS_MISSING\", \"HOURS_MISSING\", \"GAP_KIND\"]\n",
    "        )\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d56b07",
   "metadata": {},
   "source": [
    "## Compare Gap Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae5e1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_gap_summaries(\n",
    "    gaps_a: pd.DataFrame,\n",
    "    gaps_b: pd.DataFrame,\n",
    "    expected_freq: str = \"30min\",\n",
    "    min_steps: int = 1,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare two gap-summary DataFrames (from `summarize_gaps`) and highlight\n",
    "    where one dataset has coverage that could fill the other's gaps.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gaps_a, gaps_b : pd.DataFrame\n",
    "        DataFrames returned by `summarize_gaps`. Must include the columns:\n",
    "        ['STATIONID','COLUMN','GAP_START','GAP_END','N_STEPS_MISSING','HOURS_MISSING','GAP_KIND'].\n",
    "    expected_freq : str, default \"30min\"\n",
    "        Sampling frequency. Used to compute discrete step counts and to\n",
    "        treat intervals on the expected time grid.\n",
    "    min_steps : int, default 1\n",
    "        Only report fillable segments with at least this many steps.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        One row per *fillable segment*.\n",
    "        Columns:\n",
    "            - TARGET_DATASET   (\"A\" or \"B\")\n",
    "            - SOURCE_DATASET   (\"B\" or \"A\")\n",
    "            - STATIONID\n",
    "            - COLUMN\n",
    "            - TARGET_GAP_START\n",
    "            - TARGET_GAP_END\n",
    "            - FILLABLE_START\n",
    "            - FILLABLE_END\n",
    "            - N_STEPS_FILLABLE\n",
    "            - HOURS_FILLABLE\n",
    "            - TARGET_N_STEPS_MISSING\n",
    "            - COVERAGE_RATIO    (steps_fillable / TARGET_N_STEPS_MISSING)\n",
    "            - TARGET_GAP_KIND\n",
    "    \"\"\"\n",
    "    req = {\"STATIONID\",\"COLUMN\",\"GAP_START\",\"GAP_END\",\"N_STEPS_MISSING\"}\n",
    "    for name, g in [(\"gaps_a\", gaps_a), (\"gaps_b\", gaps_b)]:\n",
    "        missing = req - set(g.columns)\n",
    "        if missing:\n",
    "            raise KeyError(f\"{name} missing required columns: {missing}\")\n",
    "\n",
    "    # Normalize dtypes and sort\n",
    "    def _prep(g):\n",
    "        g = g.copy()\n",
    "        g[\"GAP_START\"] = pd.to_datetime(g[\"GAP_START\"])\n",
    "        g[\"GAP_END\"] = pd.to_datetime(g[\"GAP_END\"])\n",
    "        if \"GAP_KIND\" not in g.columns:\n",
    "            g[\"GAP_KIND\"] = \"Unknown\"\n",
    "        return g.sort_values([\"STATIONID\",\"COLUMN\",\"GAP_START\",\"GAP_END\"]).reset_index(drop=True)\n",
    "\n",
    "    gaps_a = _prep(gaps_a)\n",
    "    gaps_b = _prep(gaps_b)\n",
    "\n",
    "    freq_td = to_offset(expected_freq).delta\n",
    "    hours_per_step = freq_td / pd.Timedelta(hours=1)\n",
    "\n",
    "    # Build a quick lookup: for each (station, column), list of (start, end) gaps\n",
    "    def _build_lookup(g):\n",
    "        d = {}\n",
    "        for (stn, col), sub in g.groupby([\"STATIONID\",\"COLUMN\"], sort=False):\n",
    "            d[(stn, col)] = list(zip(sub[\"GAP_START\"], sub[\"GAP_END\"]))\n",
    "        return d\n",
    "\n",
    "    gapsB_lookup = _build_lookup(gaps_b)\n",
    "    gapsA_lookup = _build_lookup(gaps_a)\n",
    "\n",
    "    def _steps_inclusive(s, e):\n",
    "        # number of discrete samples on the regular grid from s..e inclusive\n",
    "        return int(((e - s) // freq_td) + 1)\n",
    "\n",
    "    def _subtract_interval(base, subtracts):\n",
    "        \"\"\"Given a base [a0,a1] (inclusive, on grid) and a list of\n",
    "        subtract intervals (inclusive), return list of remaining\n",
    "        inclusive intervals on the same grid.\"\"\"\n",
    "        a0, a1 = base\n",
    "        if a0 > a1:\n",
    "            return []\n",
    "        # Clip subtracts to base\n",
    "        cl = []\n",
    "        for s, e in subtracts:\n",
    "            s1 = max(s, a0)\n",
    "            e1 = min(e, a1)\n",
    "            if s1 <= e1:\n",
    "                cl.append((s1, e1))\n",
    "        if not cl:\n",
    "            return [(a0, a1)]\n",
    "        cl.sort(key=lambda x: x[0])\n",
    "\n",
    "        segs = []\n",
    "        cur = a0\n",
    "        for s, e in cl:\n",
    "            # segment before s (subtract is inclusive)\n",
    "            before_end = s - freq_td\n",
    "            if cur <= before_end:\n",
    "                segs.append((cur, before_end))\n",
    "            # skip the subtracted run\n",
    "            cur = e + freq_td\n",
    "            if cur > a1:\n",
    "                break\n",
    "        if cur <= a1:\n",
    "            segs.append((cur, a1))\n",
    "        return segs\n",
    "\n",
    "    def _direction_fill(target_gaps, source_lookup, target_label, source_label):\n",
    "        \"\"\"Compute fillable segments where `source` can fill `target`.\"\"\"\n",
    "        out_rows = []\n",
    "        for _, r in target_gaps.iterrows():\n",
    "            key = (r[\"STATIONID\"], r[\"COLUMN\"])\n",
    "            base = (r[\"GAP_START\"], r[\"GAP_END\"])\n",
    "            subtracts = source_lookup.get(key, [])\n",
    "            fill_segments = _subtract_interval(base, subtracts)\n",
    "            for fs, fe in fill_segments:\n",
    "                steps = _steps_inclusive(fs, fe)\n",
    "                if steps < min_steps:\n",
    "                    continue\n",
    "                out_rows.append({\n",
    "                    \"TARGET_DATASET\": target_label,\n",
    "                    \"SOURCE_DATASET\": source_label,\n",
    "                    \"STATIONID\": r[\"STATIONID\"],\n",
    "                    \"COLUMN\": r[\"COLUMN\"],\n",
    "                    \"TARGET_GAP_START\": r[\"GAP_START\"],\n",
    "                    \"TARGET_GAP_END\": r[\"GAP_END\"],\n",
    "                    \"FILLABLE_START\": fs,\n",
    "                    \"FILLABLE_END\": fe,\n",
    "                    \"N_STEPS_FILLABLE\": steps,\n",
    "                    \"HOURS_FILLABLE\": steps * hours_per_step,\n",
    "                    \"TARGET_N_STEPS_MISSING\": int(r[\"N_STEPS_MISSING\"]),\n",
    "                    \"COVERAGE_RATIO\": steps / int(r[\"N_STEPS_MISSING\"]),\n",
    "                    \"TARGET_GAP_KIND\": r.get(\"GAP_KIND\", \"Unknown\"),\n",
    "                })\n",
    "        if not out_rows:\n",
    "            return pd.DataFrame(columns=[\n",
    "                \"TARGET_DATASET\",\"SOURCE_DATASET\",\"STATIONID\",\"COLUMN\",\n",
    "                \"TARGET_GAP_START\",\"TARGET_GAP_END\",\"FILLABLE_START\",\"FILLABLE_END\",\n",
    "                \"N_STEPS_FILLABLE\",\"HOURS_FILLABLE\",\"TARGET_N_STEPS_MISSING\",\n",
    "                \"COVERAGE_RATIO\",\"TARGET_GAP_KIND\"\n",
    "            ])\n",
    "        return pd.DataFrame(out_rows).sort_values(\n",
    "            [\"STATIONID\",\"COLUMN\",\"TARGET_GAP_START\",\"FILLABLE_START\"]\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    # B can fill A (subtract A's gaps by B's gaps)\n",
    "    fill_B_to_A = _direction_fill(gaps_a, gapsB_lookup, target_label=\"A\", source_label=\"B\")\n",
    "    # A can fill B\n",
    "    fill_A_to_B = _direction_fill(gaps_b, gapsA_lookup, target_label=\"B\", source_label=\"A\")\n",
    "\n",
    "    # Combine\n",
    "    combined = pd.concat([fill_B_to_A, fill_A_to_B], ignore_index=True)\n",
    "    return combined.sort_values(\n",
    "        [\"STATIONID\",\"COLUMN\",\"TARGET_DATASET\",\"TARGET_GAP_START\",\"FILLABLE_START\"]\n",
    "    ).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeadfcdb",
   "metadata": {},
   "source": [
    "# Define the root folder for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c940913",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T14:28:19.724869Z",
     "start_time": "2025-09-04T14:28:19.721284Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_fold = pathlib.Path(f'G:/Shared drives/UGS_Flux/Data_Downloads/compiled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb5eef9",
   "metadata": {},
   "source": [
    "# Run Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f391f553",
   "metadata": {},
   "source": [
    "Define the site folders and stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d046c00c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T14:28:22.700334Z",
     "start_time": "2025-09-04T14:28:22.696971Z"
    }
   },
   "outputs": [],
   "source": [
    "site_folders = {'US-UTD':'Dugout_Ranch',\n",
    "                'US-UTB':'BSF',\n",
    "                'US-UTJ':'Bluff',\n",
    "                'US-UTW':'Wellington',\n",
    "                'US-UTE':'Escalante',\n",
    "                'US-UTM':'Matheson',\n",
    "                'US-UTP':'Phrag',\n",
    "                'US-CdM':'Cedar_mesa',\n",
    "                'US-UTV':'Desert_View_Myton',\n",
    "                'US-UTN':'Juab',\n",
    "                'US-UTG':'Green_River',\n",
    "                'US-UTL':'Pelican_Lake',\n",
    "                }\n",
    "\n",
    "loggerids = {\n",
    "    \"eddy\": {\n",
    "        \"US-UTD\": 21314,\n",
    "        \"US-UTB\": 27736,\n",
    "        \"US-UTJ\": 21020,\n",
    "        \"US-UTW\": 21025,\n",
    "        \"US-UTE\": 21021,\n",
    "        \"US-UTM\": 21029,\n",
    "        \"US-UTP\": 8442,\n",
    "        \"US-CdM\": 21313,\n",
    "        \"US-UTV\": 21027,\n",
    "        \"US-UTN\": 8441,\n",
    "        \"US-UTG\": 25415,\n",
    "        \"US-UTL\": 21215,\n",
    "    },\n",
    "    \"met\": {\n",
    "        \"US-UTD\": 21031,\n",
    "        \"US-UTB\": 27736,\n",
    "        \"US-UTJ\": 21030,\n",
    "        \"US-UTW\": 21026,\n",
    "        \"US-UTE\": 21032,\n",
    "        \"US-UTM\": 21023,\n",
    "        \"US-UTP\": 8441,\n",
    "        \"US-CdM\": 21029,\n",
    "        \"US-UTV\": 21311,\n",
    "        \"US-UTG\": 25414,\n",
    "        \"US-UTL\": 21028,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c4d27a",
   "metadata": {},
   "source": [
    "## Met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7fe141",
   "metadata": {},
   "source": [
    "### Compile Met Statistics Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6e9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.platform == 'darwin':\n",
    "    print('mac')\n",
    "    raw_fold = pathlib.Path('/users/ink/Google Drive/Shared drives/UGS_Flux/Data_Downloads/compiled')\n",
    "else:\n",
    "    print('pc')\n",
    "    raw_fold = pathlib.Path('G:/Shared drives/UGS_Flux/Data_Downloads/compiled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045bef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T13:48:40.593872Z",
     "start_time": "2025-09-04T13:47:53.287851Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "stats = {}\n",
    "for key, value in site_folders.items():\n",
    "    print(f\"Processing site: {key} - {value}\")\n",
    "    parent_fold = raw_fold / f\"{key}\" / \"Statistics\"\n",
    "    am_df = {}\n",
    "    i=0\n",
    "    #raw_data = micromet.raw_file_compile(raw_fold, parent_fold, search_str = \"TOA5*Statistics*.dat\")    \n",
    "    for file_name in parent_fold.glob(\"TOA5*Statistics*.dat\"):\n",
    "        i += 1\n",
    "        #print(f\"Processing file: {file_name}\")\n",
    "        sts = pd.read_csv(file_name, skiprows = [0,2,3])\n",
    "        for col in sts.columns:\n",
    "            if col.endswith(\"_Avg\"):\n",
    "                sts.rename(columns={col: col[:-4]}, inplace=True)\n",
    "            elif col.endswith(\"_Tot\"):\n",
    "                sts.rename(columns={col: col[:-4]}, inplace=True)\n",
    "        sts['TIMESTAMP'] = pd.to_datetime(sts['TIMESTAMP'])\n",
    "        sts['DATETIME_START'] = sts['TIMESTAMP']\n",
    "        sts[\"TIMESTAMP_START\"] = sts[\"TIMESTAMP\"].apply(lambda x: f\"{x:%Y%m%d%H%M}\")\n",
    "        am_data = micromet.Reformatter(drop_soil=False, logger=logger,)\n",
    "        #raw_data = raw_data.drop([0], axis=0)\n",
    "        df, report = am_data.prepare(sts, data_type=\"met\")\n",
    "        am_df[file_name.stem] = df\n",
    "    if i > 0:\n",
    "        stats[key] = pd.concat(am_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff1f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_met = pd.concat(stats)\n",
    "stats_met = stats_met.reset_index().rename(columns={'level_0':'STATIONID'})\n",
    "stats_met = stats_met.drop(['level_1'],axis=1)\n",
    "stats_met = stats_met.drop_duplicates(subset=['STATIONID','DATETIME_START'])\n",
    "stats_met = stats_met.set_index(['STATIONID','DATETIME_START'])\n",
    "stats_met = stats_met.mask(stats_met < -5000)\n",
    "stats_met['DATALOGGER_NO'] = stats_met.index.get_level_values(0).map(loggerids['met'])\n",
    "stats_met.to_parquet(raw_fold / \"comp_met_stat.parquet\")\n",
    "gaps_metstat = summarize_gaps(stats_met)\n",
    "\n",
    "gaps_metstat.to_parquet(raw_fold / \"gaps_metstat.parquet\")\n",
    "print(stats_met.index.get_level_values(0).unique())\n",
    "for col in sorted(stats_met.columns):\n",
    "    print(col)\n",
    "\n",
    "\n",
    "bal = stats_met.loc['US-UTE'].sort_index()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=bal.index, y=bal['NETRAD_1_1_2'].shift(-1)*2, mode=\"lines\", name=\"NETRAD\",\n",
    "                         hovertemplate=\"Date: %{x|%Y-%m-%d %H:%M}<br>B: %{y:.2f}<extra></extra>\"))\n",
    "fig.update_layout(\n",
    "    title=\"Two Time Series (single plot)\",\n",
    "    xaxis=dict(title=\"Time\", rangeslider=dict(visible=True), type=\"date\"),\n",
    "    yaxis_title=\"Value\",\n",
    "    legend=dict(orientation=\"h\", y=1.02, x=1, xanchor=\"right\", yanchor=\"bottom\"),\n",
    "    height=500, margin=dict(l=60, r=30, t=60, b=40),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715c9f93",
   "metadata": {},
   "source": [
    "### Compile Statistics Ameriflux .dat Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd6898",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_met_df = {}\n",
    "outlier_report = {}\n",
    "\n",
    "am = micromet.AmerifluxDataProcessor(logger=logger)\n",
    "\n",
    "for key, value in site_folders.items():\n",
    "\n",
    "    parent_fold = raw_fold / f\"{key}\" / \"Statistics_Ameriflux\"\n",
    "    raw_data = am.raw_file_compile(raw_fold, parent_fold, search_str = \"*Statistics_AmeriFlux*.dat\")\n",
    "    if raw_data is not None:\n",
    "        am_data = micromet.Reformatter(drop_soil=False,\n",
    "                                       logger=logger,\n",
    "                                       )\n",
    "        #raw_data = raw_data.drop([0], axis=0)\n",
    "        am_df, report = am_data.prepare(raw_data, data_type=\"met\")\n",
    "        comp_met_df[key] = am_df\n",
    "        outlier_report[key] = report\n",
    "\n",
    "        timestart = am_df['TIMESTAMP_START'].values[0]\n",
    "        timeend = am_df['TIMESTAMP_END'].values[-1]\n",
    "\n",
    "        am_df.to_csv(raw_fold / f\"{key}\" / f\"{key}-met_HH_{timestart:}_{timeend:}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c360c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_met = pd.concat(comp_met_df)\n",
    "\n",
    "comp_met = comp_met.reset_index().rename(columns={'level_0':'STATIONID'})\n",
    "#comp_met = comp_met.drop(['level_1'],axis=1)\n",
    "comp_met = comp_met.drop_duplicates(subset=['STATIONID','DATETIME_START'])\n",
    "comp_met = comp_met.set_index(['STATIONID','DATETIME_START'])\n",
    "comp_met = comp_met.mask(comp_met < -5000)\n",
    "comp_met['DATALOGGER_NO'] = comp_met.index.get_level_values(0).map(loggerids['met'])\n",
    "gaps_met = summarize_gaps(comp_met)\n",
    "\n",
    "gaps_met.to_parquet(raw_fold / \"gaps_met.parquet\")\n",
    "comp_met.to_parquet(raw_fold / \"comp_met.parquet\")\n",
    "\n",
    "out_report_met = pd.concat(outlier_report)\n",
    "out_report_met.to_csv(raw_fold / \"outlier_report_met.csv\")\n",
    "\n",
    "print(comp_met.index.get_level_values(0).unique())\n",
    "for col in sorted(comp_met.columns):\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876ab250",
   "metadata": {},
   "outputs": [],
   "source": [
    "bal = comp_met.loc['US-UTV']\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=bal.index, y=bal['NETRAD_1_1_2'].shift(-1)*2, mode=\"lines\", name=\"NETRAD\",\n",
    "                         hovertemplate=\"Date: %{x|%Y-%m-%d %H:%M}<br>B: %{y:.2f}<extra></extra>\"))\n",
    "fig.update_layout(\n",
    "    title=\"Two Time Series (single plot)\",\n",
    "    xaxis=dict(title=\"Time\", rangeslider=dict(visible=True), type=\"date\"),\n",
    "    yaxis_title=\"Value\",\n",
    "    legend=dict(orientation=\"h\", y=1.02, x=1, xanchor=\"right\", yanchor=\"bottom\"),\n",
    "    height=500, margin=dict(l=60, r=30, t=60, b=40),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40116113",
   "metadata": {},
   "source": [
    "## Eddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658fe113",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_folders = {'US-UTD':'Dugout_Ranch',\n",
    "                'US-UTB':'BSF',\n",
    "                'US-UTJ':'Bluff',\n",
    "                'US-UTW':'Wellington',\n",
    "                'US-UTE':'Escalante',\n",
    "                'US-UTM':'Matheson',\n",
    "                'US-UTP':'Phrag',\n",
    "                'US-CdM':'Cedar_mesa',\n",
    "                'US-UTV':'Desert_View_Myton',\n",
    "                'US-UTN':'Juab',\n",
    "                'US-UTG':'Green_River',\n",
    "                'US-UTL':'Pelican_Lake',\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9add59",
   "metadata": {},
   "source": [
    "### Compile Downloaded Eddy Data from EasyFluxWeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209b8f001cc12a9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T14:28:46.991607Z",
     "start_time": "2025-09-04T14:28:33.602504Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "easyfluxdf = {}\n",
    "ef_reports = {}\n",
    "\n",
    "for key, value in site_folders.items():\n",
    "    site_dir = raw_fold / key\n",
    "    for file in site_dir.glob(\"*_Flux_AmeriFluxFormat.dat\"):\n",
    "\n",
    "        am_data = micromet.Reformatter(drop_soil=True,\n",
    "                                            logger=logger,\n",
    "                                            )\n",
    "        df = pd.read_csv(file,skiprows=[0,2,3],\n",
    "                        na_values=[-9999,\"NAN\",\"NaN\",\"nan\"])\n",
    "        \n",
    "        df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])\n",
    "\n",
    "        am_df, report = am_data.prepare(df, data_type=\"eddy\")\n",
    "        easyfluxdf[key] = am_df\n",
    "        ef_reports[key] = report\n",
    "\n",
    "\n",
    "ef_report = pd.concat(ef_reports, axis=1).T\n",
    "\n",
    "ef_report.to_csv(raw_fold / \"easyflux_report.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb93635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "easyflux = pd.concat(easyfluxdf)\n",
    "easyflux = easyflux.reset_index().rename(columns={'level_0':'STATIONID'})\n",
    "easyflux = easyflux.drop_duplicates(subset=['STATIONID','DATETIME_START'])\n",
    "easyflux = easyflux.set_index(['STATIONID','DATETIME_START'])\n",
    "easyflux = easyflux.mask(easyflux < -5000)\n",
    "easyflux['DATALOGGER_NO'] = easyflux.index.get_level_values(0).map(loggerids['eddy'])\n",
    "\n",
    "easyflux.to_parquet(raw_fold / \"easyflux.parquet\")\n",
    "gaps_easyflux = summarize_gaps(easyflux)\n",
    "gaps_easyflux.to_parquet(raw_fold / \"gaps_easyflux.parquet\")\n",
    "\n",
    "bal = easyflux.loc['US-UTV']\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=bal.index, y=bal['LE_1_1_1'], mode=\"lines\", name=\"LE\",\n",
    "                         hovertemplate=\"Date: %{x|%Y-%m-%d %H:%M}<br>A: %{y:.2f}<extra></extra>\"))\n",
    "fig.add_trace(go.Scatter(x=bal.index, y=bal['NETRAD_1_1_1'].shift(-1)*2, mode=\"lines\", name=\"NETRAD\",\n",
    "                         hovertemplate=\"Date: %{x|%Y-%m-%d %H:%M}<br>B: %{y:.2f}<extra></extra>\"))\n",
    "fig.update_layout(\n",
    "    title=\"Two Time Series (single plot)\",\n",
    "    xaxis=dict(title=\"Time\", rangeslider=dict(visible=True), type=\"date\"),\n",
    "    yaxis_title=\"Value\",\n",
    "    legend=dict(orientation=\"h\", y=1.02, x=1, xanchor=\"right\", yanchor=\"bottom\"),\n",
    "    height=500, margin=dict(l=60, r=30, t=60, b=40),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce6668991ab3d67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T14:28:54.799979Z",
     "start_time": "2025-09-04T14:28:54.780883Z"
    }
   },
   "outputs": [],
   "source": [
    "print(easyflux.index.get_level_values(0).unique())\n",
    "for col in sorted(easyflux.columns):\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e21a8b0391f9a",
   "metadata": {},
   "source": [
    "### Compile Ameriflux Format dat files from Dataloggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b06cfd1c31367d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T15:04:18.115011Z",
     "start_time": "2025-09-04T14:37:15.531111Z"
    }
   },
   "outputs": [],
   "source": [
    "comp_edd_df = {}\n",
    "outlier_report = {}\n",
    "\n",
    "am = micromet.AmerifluxDataProcessor(logger=logger)\n",
    "\n",
    "for key, value in site_folders.items():\n",
    "\n",
    "    parent_fold = raw_fold / f\"{key}\" / \"AmeriFluxFormat\"\n",
    "    #ahp.scan(parent_fold, min_sim=0.3, backup=False)\n",
    "    #pths = micromet.fix_all_in_parent(parent_fold)\n",
    "    raw_data = am.raw_file_compile(raw_fold, parent_fold, search_str = \"*Flux_AmeriFluxFormat*.dat\")\n",
    "    if raw_data is not None:\n",
    "        am_data = micromet.Reformatter(drop_soil=False,\n",
    "                                       logger=logger,\n",
    "                                       )\n",
    "        #raw_data = raw_data.drop([0], axis=0)\n",
    "        am_df, report = am_data.prepare(raw_data, data_type=\"eddy\")\n",
    "        comp_edd_df[key] = am_df\n",
    "        outlier_report[key] = report\n",
    "\n",
    "        timestart = am_df['TIMESTAMP_START'].values[0]\n",
    "        timeend = am_df['TIMESTAMP_END'].values[-1]\n",
    "\n",
    "        am_df.to_csv(raw_fold / f\"{key}\" / f\"{key}_HH_{timestart:}_{timeend:}.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1920f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cmp_edd_df = {}\n",
    "\n",
    "for key, value in site_folders.items():\n",
    "    for file in (raw_fold / f\"{key}\").glob(f\"{key}_HH_*.csv\"):\n",
    "        print(file) \n",
    "        df = pd.read_csv(file, index_col=0)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df.sort_index()\n",
    "        df = df.drop_duplicates(subset=['TIMESTAMP_START','TIMESTAMP_END'])\n",
    "        cmp_edd_df[key] = df\n",
    "\n",
    "cmp_edd = pd.concat(cmp_edd_df)\n",
    "cmp_edd = fill_na_drop_dups(cmp_edd)\n",
    "cmp_edd = cmp_edd.rename(columns={'BATTERY_VOLTAGE':'V_BATT'})\n",
    "\n",
    "cmp_edd = cmp_edd.reset_index().rename(columns={'level_0':'STATIONID'})\n",
    "cmp_edd = cmp_edd.drop_duplicates(subset=['STATIONID','DATETIME_START'])\n",
    "cmp_edd = cmp_edd.set_index(['STATIONID','DATETIME_START'])\n",
    "cmp_edd = cmp_edd.mask(cmp_edd < -5000)\n",
    "cmp_edd['loggerid'] = cmp_edd.index.get_level_values(0).map(loggerids['eddy'])\n",
    "cmp_edd['DATALOGGER_NO'] = cmp_edd['DATALOGGER_NO'].fillna(cmp_edd['loggerid'])\n",
    "cmp_edd = cmp_edd.drop(columns=['loggerid'],axis=1)\n",
    "cmp_edd.to_parquet(raw_fold / \"comp_edd.parquet\")\n",
    "gaps_amfluxfmt = summarize_gaps(cmp_edd)\n",
    "gaps_amfluxfmt.to_parquet(raw_fold / \"gaps_comp_edd.parquet\")\n",
    "\n",
    "\n",
    "bal = cmp_edd.loc['US-UTV']\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=bal.index, y=bal['LE_1_1_1'], mode=\"lines\", name=\"LE\",\n",
    "                         hovertemplate=\"Date: %{x|%Y-%m-%d %H:%M}<br>A: %{y:.2f}<extra></extra>\"))\n",
    "fig.add_trace(go.Scatter(x=bal.index, y=bal['NETRAD_1_1_1'].shift(-1)*2, mode=\"lines\", name=\"NETRAD\",\n",
    "                         hovertemplate=\"Date: %{x|%Y-%m-%d %H:%M}<br>B: %{y:.2f}<extra></extra>\"))\n",
    "fig.update_layout(\n",
    "    title=\"Two Time Series (single plot)\",\n",
    "    xaxis=dict(title=\"Time\", rangeslider=dict(visible=True), type=\"date\"),\n",
    "    yaxis_title=\"Value\",\n",
    "    legend=dict(orientation=\"h\", y=1.02, x=1, xanchor=\"right\", yanchor=\"bottom\"),\n",
    "    height=500, margin=dict(l=60, r=30, t=60, b=40),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5299ad4",
   "metadata": {},
   "source": [
    "### Compile CSFormat Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53912c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_edd_df = {}\n",
    "outlier_report = {}\n",
    "\n",
    "am = micromet.AmerifluxDataProcessor(logger=logger)\n",
    "\n",
    "for key, value in site_folders.items():\n",
    "\n",
    "    parent_fold = raw_fold / f\"{key}\" / \"Flux_CSFormat\"\n",
    "    #ahp.scan(parent_fold, min_sim=0.3, backup=False)\n",
    "    #pths = micromet.fix_all_in_parent(parent_fold)\n",
    "    am_df = {}\n",
    "    for file in parent_fold.glob(\"*_Flux_CSFormat*.dat\"):\n",
    "        name = file.stem\n",
    "        am_data = micromet.Reformatter(drop_soil=False,\n",
    "                                            logger=logger,\n",
    "                                            )\n",
    "        #print(file)\n",
    "        df = pd.read_csv(file,skiprows=[0,2,3],\n",
    "                        na_values=[-9999,\"NAN\",\"NaN\",\"nan\"])\n",
    "        #print(df.columns)\n",
    "        # must create a timestamp_end column to feed into prepare\n",
    "        # b/c otherwise no data will be returned\n",
    "        df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])\n",
    "        df[\"TIMESTAMP_END\"] = df.TIMESTAMP.dt.strftime(\"%Y%m%d%H%M\").astype(int)\n",
    "        df['STATIONID']= key\n",
    "        \n",
    "\n",
    "        csprep, report = am_data.prepare(df, data_type=\"eddy\")\n",
    "        am_df[name] = csprep\n",
    "        outlier_report[key] = report\n",
    "\n",
    "    comp_edd_df[key] = pd.concat(am_df,ignore_index=False)\n",
    "    \n",
    "\n",
    "    #comp_edd_df.to_csv(raw_fold / f\"{key}\" / f\"{key}_cs_flux.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99711e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_edd = pd.concat(comp_edd_df)\n",
    "comp_edd.columns = comp_edd.columns.str.upper()\n",
    "comp_edd = comp_edd.dropna(how='all',axis=1)\n",
    "drop_fields = [\n",
    "    \"TS_CS65X_2_1_1\",\n",
    "    \"WS_RSLT\",\n",
    "    \"_229_DEL_TMPR(1)\",\n",
    "    \"_229_DEL_TMPR(2)\",\n",
    "    \"_229_TMPR_T0_1\",\n",
    "    \"_229_TMPR_T0_2\",\n",
    "    \"_229_TMPR_T1_1\",\n",
    "    \"_229_TMPR_T1_2\",\n",
    "    \"_229_TMPR_T30_1\",\n",
    "    \"_229_TMPR_T30_2\",\n",
    "    \"_PANEL_TMPR_T0\",\n",
    "    \"_PANEL_TMPR_T1\",\n",
    "    \"_PANEL_TMPR_T30\",\n",
    "    \"WND_DIR_STD\",\n",
    "    \"WND_DIR_UNIT_VEC\",\n",
    "    \"WND_SPD_AVG\",\n",
    "    \"U_HEATMAX\",\n",
    "    \"U_SEN0\",\n",
    "    \"U_SENAMP\",\n",
    "    \"U_SENMAX\",\n",
    "    \"SONIC_AZIMUTH\",\n",
    "    \"CS65X_EC_2_1_1\"\n",
    "    \"SUN_AZIMUTH\",\n",
    "    \"SUN_DECLINATION\",\n",
    "    \"SUN_ELEVATION\",\n",
    "    \"HEIGHT_AGL\",\n",
    "    \"HOUR_ANGLE\",\n",
    "    \"CS65X_PERM_1_1_1\",\n",
    "    \"DAYTIME\",\n",
    "    \"E\",\n",
    "    \"E1_Q\",\n",
    "    \"ANONYMOUS1\",\n",
    "    \"ANONYMOUS2\",\n",
    "    \"TD_TP01\",\n",
    "    \"AIR_MASS_COEFF\",\n",
    "    \"ROCP_TP01\",\n",
    "    \"Q\"\n",
    "]\n",
    "\n",
    "for field in drop_fields:\n",
    "    if field in comp_edd.columns:\n",
    "        comp_edd = comp_edd.drop(columns=[field],axis=1)\n",
    "\n",
    "rename_fields = {\n",
    "    \"CS65X_EC_1_1_1\":\"EC_1_1_1\",\n",
    "    \"CS65X_EC_1_1_2\":\"EC_1_1_2\",\n",
    "    \"G\":\"G_1_1_A\",\n",
    "    \"ALB\":\"ALB_1_1_1\",\n",
    "    \"H\":\"H_1_1_1\",\n",
    "    \"LE\":\"LE_1_1_1\",\n",
    "    \"LI7700_AMB_PRESS\":\"PA_1_1_2\",\n",
    "    \"LI7700_AMB_TMPR\":\"TA_1_1_5\",\n",
    "    \"LW_IN\":\"LW_IN_1_1_1\",\n",
    "    \"LW_OUT\":\"LW_OUT_1_1_1\",\n",
    "    \"NETRAD\":\"NETRAD_1_1_1\",\n",
    "    \"PA\":\"PA_1_1_1\",\n",
    "    \"SW_IN\":\"SW_IN_1_1_1\",\n",
    "    \"SW_OUT\":\"SW_OUT_1_1_1\",\n",
    "    \"T_SONIC\":\"T_SONIC_1_1_1\",\n",
    "    \"WD\":\"WD_1_1_1\",\n",
    "    \"WS\":\"WS_1_1_1\",\n",
    "    \"WS_MAX\":\"WS_MAX_1_1_1\",\n",
    "    'CO2':'CO2_1_1_1', \n",
    "               'CO2_SIGMA':'CO2_SIGMA_1_1_1', \n",
    "               'H2O':'H2O_1_1_1', \n",
    "               'H2O_SIGMA':'H2O_SIGMA_1_1_1',\n",
    "               'FC':'FC_1_1_1', \n",
    "    }\n",
    "\n",
    "comp_edd = comp_edd.rename(columns=rename_fields)\n",
    "\n",
    "mergefields = {\n",
    "    \"TA_1_1_4\":\"AMB_AIR_TMPR\",\n",
    "    \"E_AMB\": \"AMB_E\",\n",
    "    \"E_SAT_AMB\":\"AMB_E_SAT\",\n",
    "    \"TS_1_1_1\":\"TS_CS65X_1_1_1\",\n",
    "    \"TS_1_1_1\":\"TS_CS65X_1_1_2\",\n",
    "    \"TS110_T_AVG\":\"T_CANOPY\"\n",
    "    }\n",
    "\n",
    "for key, value in mergefields.items():\n",
    "    # Treat -9999 as missing\n",
    "    s1 = comp_edd[key].replace(-9999, np.nan)\n",
    "    s2 = comp_edd[value].replace(-9999, np.nan)\n",
    "    comp_edd[key] = s1.combine_first(s2).fillna(-9999)\n",
    "\n",
    "# Identify duplicate column names\n",
    "duplicate_columns = comp_edd.columns[comp_edd.columns.duplicated()]\n",
    "print(\"Duplicate column names:\", duplicate_columns)\n",
    "\n",
    "\n",
    "\n",
    "comp_edd = comp_edd.reset_index().drop(['level_1','level_2'], axis=1).set_index(['level_0','TIMESTAMP'])\n",
    "\n",
    "\n",
    "comp_edd.to_parquet(raw_fold / \"comp_cs_flux.parquet\")\n",
    "#outlier_report.to_csv(raw_fold / \"outlier_report_csflux.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comp_edd.index.get_level_values(0).unique())\n",
    "for col in sorted(comp_edd.columns):\n",
    "    print(col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884a2e27",
   "metadata": {},
   "source": [
    "# Bring together the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23696546",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps_amfluxfmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d667a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_vs_files = compare_gap_summaries(gaps_easyflux, gaps_amfluxfmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8425aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_vs_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ec9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "def fill_missing_from_other(\n",
    "    df_target: pd.DataFrame,\n",
    "    df_source: pd.DataFrame,\n",
    "    expected_freq: str = \"30min\",\n",
    "    add_missing_timestamps: bool = True,\n",
    "    min_steps: int = 1,\n",
    "    columns: list | None = None,\n",
    "    station_level: str = \"STATIONID\",\n",
    "    time_level: str = \"DATETIME_START\",\n",
    "    return_plan: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fill missing values in `df_target` using `df_source` guided by gap/coverage analysis.\n",
    "\n",
    "    It:\n",
    "      1) runs `summarize_gaps` on target and source\n",
    "      2) runs `compare_gap_summaries` to find fillable segments where SOURCE can fill TARGET\n",
    "      3) (optionally) reindexes target to include any missing timestamps in those segments\n",
    "      4) copies values from source -> target ONLY for the targeted column(s), station, and times\n",
    "         where target is missing (NaN or newly added rows)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_target : pd.DataFrame\n",
    "        MultiIndex (station, datetime) with data to be filled (we call this \"A\" internally).\n",
    "    df_source : pd.DataFrame\n",
    "        MultiIndex (station, datetime) with data to copy from (we call this \"B\").\n",
    "    expected_freq : str, default \"30min\"\n",
    "        Grid frequency (must match both datasets).\n",
    "    add_missing_timestamps : bool, default True\n",
    "        If True, adds missing rows in the target during fillable segments before copying.\n",
    "        If False, only fills NaN cells at timestamps that already exist in target.\n",
    "    min_steps : int, default 1\n",
    "        Only consider fillable segments of at least this many samples.\n",
    "    columns : list[str] | None\n",
    "        Optional subset of columns to fill. By default uses the intersection of\n",
    "        df_target.columns and df_source.columns.\n",
    "    station_level : str, default \"STATIONID\"\n",
    "        Name of station level in MultiIndex.\n",
    "    time_level : str, default \"DATETIME_START\"\n",
    "        Name of time level in MultiIndex.\n",
    "    return_plan : bool, default False\n",
    "        If True, also returns the computed fill plan (B→A only).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    filled : pd.DataFrame\n",
    "        A copy of `df_target` with values filled from `df_source`.\n",
    "    audit : pd.DataFrame\n",
    "        Row-by-row audit of realized fills with columns:\n",
    "          ['STATIONID','COLUMN','FILLABLE_START','FILLABLE_END',\n",
    "           'N_STEPS_PLANNED','N_STEPS_FILLED','HOURS_FILLED']\n",
    "    plan (optional) : pd.DataFrame\n",
    "        The B→A portion of the compare plan (only if return_plan=True).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Requires the helper functions `summarize_gaps` and `compare_gap_summaries` to be defined.\n",
    "    - Only copies the specified column indicated by each plan row (no cross-column filling).\n",
    "    - Never overwrites non-missing target values.\n",
    "    \"\"\"\n",
    "    # --- basic checks ---\n",
    "    if not isinstance(df_target.index, pd.MultiIndex) or not isinstance(df_source.index, pd.MultiIndex):\n",
    "        raise TypeError(\"Both df_target and df_source must have a MultiIndex (station, datetime).\")\n",
    "    if station_level not in df_target.index.names or time_level not in df_target.index.names:\n",
    "        raise KeyError(\"df_target index must include levels: station and time.\")\n",
    "    if station_level not in df_source.index.names or time_level not in df_source.index.names:\n",
    "        raise KeyError(\"df_source index must include levels: station and time.\")\n",
    "\n",
    "    # Decide which columns to work on\n",
    "    if columns is None:\n",
    "        columns = list(set(df_target.columns).intersection(set(df_source.columns)))\n",
    "        if not columns:\n",
    "            raise ValueError(\"No overlapping columns between target and source to fill.\")\n",
    "\n",
    "    # Frequency helpers\n",
    "    freq_td = to_offset(expected_freq).delta\n",
    "    hours_per_step = freq_td / pd.Timedelta(hours=1)\n",
    "\n",
    "    # --- Build plan: B fills A ---\n",
    "    gaps_a = summarize_gaps(df_target, station_level=station_level, time_level=time_level,\n",
    "                            expected_freq=expected_freq, columns=columns)\n",
    "    gaps_b = summarize_gaps(df_source, station_level=station_level, time_level=time_level,\n",
    "                            expected_freq=expected_freq, columns=columns)\n",
    "    plan_all = compare_gap_summaries(gaps_a, gaps_b, expected_freq=expected_freq, min_steps=min_steps)\n",
    "    plan = plan_all[plan_all[\"TARGET_DATASET\"] == \"A\"].copy()\n",
    "\n",
    "    if plan.empty:\n",
    "        # Nothing to do\n",
    "        audit = pd.DataFrame(columns=[\n",
    "            \"STATIONID\",\"COLUMN\",\"FILLABLE_START\",\"FILLABLE_END\",\n",
    "            \"N_STEPS_PLANNED\",\"N_STEPS_FILLED\",\"HOURS_FILLED\"\n",
    "        ])\n",
    "        return (df_target.copy(), audit, plan) if return_plan else (df_target.copy(), audit)\n",
    "\n",
    "    # Optional column filter\n",
    "    plan = plan[plan[\"COLUMN\"].isin(columns)].copy()\n",
    "    if plan.empty:\n",
    "        audit = pd.DataFrame(columns=[\n",
    "            \"STATIONID\",\"COLUMN\",\"FILLABLE_START\",\"FILLABLE_END\",\n",
    "            \"N_STEPS_PLANNED\",\"N_STEPS_FILLED\",\"HOURS_FILLED\"\n",
    "        ])\n",
    "        return (df_target.copy(), audit, plan) if return_plan else (df_target.copy(), audit)\n",
    "\n",
    "    # --- Prepare a working copy of target ---\n",
    "    target = df_target.copy()\n",
    "\n",
    "    # If we need to add missing timestamps, compute per-station union of times from plan\n",
    "    if add_missing_timestamps:\n",
    "        add_times_by_station = {}\n",
    "        for _, r in plan.iterrows():\n",
    "            stn = r[\"STATIONID\"]\n",
    "            times = pd.date_range(r[\"FILLABLE_START\"], r[\"FILLABLE_END\"], freq=expected_freq)\n",
    "            add_times_by_station.setdefault(stn, set()).update(times.to_pydatetime().tolist())\n",
    "\n",
    "        # Reindex per station once with the union of needed times\n",
    "        rebuilt = []\n",
    "        stations = target.index.get_level_values(station_level).unique()\n",
    "        stations_in_plan = set(plan[\"STATIONID\"].unique())\n",
    "        for stn in stations.union(stations_in_plan):\n",
    "            # Slice existing station data if present, else empty\n",
    "            if stn in stations:\n",
    "                sub = target.xs(stn, level=station_level)\n",
    "            else:\n",
    "                # Create empty subframe with all columns if station absent\n",
    "                sub = pd.DataFrame(columns=target.columns, index=pd.DatetimeIndex([], name=time_level))\n",
    "\n",
    "            need_times = pd.DatetimeIndex(sorted(add_times_by_station.get(stn, [])))\n",
    "            if len(need_times) > 0:\n",
    "                new_index = sub.index.union(need_times)\n",
    "                sub = sub.reindex(new_index)\n",
    "\n",
    "            # Return to MultiIndex\n",
    "            sub = sub.copy()\n",
    "            sub[station_level] = stn\n",
    "            sub[time_level] = sub.index\n",
    "            sub = sub.set_index([station_level, time_level]).sort_index()\n",
    "            rebuilt.append(sub)\n",
    "\n",
    "        target = pd.concat(rebuilt).sort_index()\n",
    "\n",
    "    # --- Perform the fill per plan row ---\n",
    "    audit_rows = []\n",
    "    idx = pd.IndexSlice\n",
    "    for _, r in plan.iterrows():\n",
    "        stn = r[\"STATIONID\"]\n",
    "        col = r[\"COLUMN\"]\n",
    "        times = pd.date_range(r[\"FILLABLE_START\"], r[\"FILLABLE_END\"], freq=expected_freq)\n",
    "\n",
    "        # Intersect with indices present in both frames (after optional reindex, target has them;\n",
    "        # still be safe if add_missing_timestamps=False)\n",
    "        try:\n",
    "            t_vals = target.loc[idx[stn, times], col]\n",
    "        except KeyError:\n",
    "            # If none of the times exist in target and we didn't reindex them in, skip\n",
    "            continue\n",
    "\n",
    "        # Source values for those times (skip if missing in source for any reason)\n",
    "        try:\n",
    "            s_vals = df_source.loc[idx[stn, times], col]\n",
    "        except KeyError:\n",
    "            # If source lacks all those times (shouldn't happen per plan), skip\n",
    "            continue\n",
    "\n",
    "        # Only fill where target is NA and source is not NA\n",
    "        to_fill_mask = t_vals.isna() & s_vals.notna()\n",
    "        if not to_fill_mask.any():\n",
    "            # Nothing filled for this segment\n",
    "            audit_rows.append({\n",
    "                \"STATIONID\": stn,\n",
    "                \"COLUMN\": col,\n",
    "                \"FILLABLE_START\": r[\"FILLABLE_START\"],\n",
    "                \"FILLABLE_END\": r[\"FILLABLE_END\"],\n",
    "                \"N_STEPS_PLANNED\": int(r[\"N_STEPS_FILLABLE\"]),\n",
    "                \"N_STEPS_FILLED\": 0,\n",
    "                \"HOURS_FILLED\": 0.0,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Assign\n",
    "        fill_index = to_fill_mask.index[to_fill_mask]\n",
    "        target.loc[idx[stn, fill_index], col] = s_vals.loc[fill_index]\n",
    "\n",
    "        n_filled = int(to_fill_mask.sum())\n",
    "        audit_rows.append({\n",
    "            \"STATIONID\": stn,\n",
    "            \"COLUMN\": col,\n",
    "            \"FILLABLE_START\": r[\"FILLABLE_START\"],\n",
    "            \"FILLABLE_END\": r[\"FILLABLE_END\"],\n",
    "            \"N_STEPS_PLANNED\": int(r[\"N_STEPS_FILLABLE\"]),\n",
    "            \"N_STEPS_FILLED\": n_filled,\n",
    "            \"HOURS_FILLED\": n_filled * hours_per_step,\n",
    "        })\n",
    "\n",
    "    audit = pd.DataFrame(audit_rows, columns=[\n",
    "        \"STATIONID\",\"COLUMN\",\"FILLABLE_START\",\"FILLABLE_END\",\n",
    "        \"N_STEPS_PLANNED\",\"N_STEPS_FILLED\",\"HOURS_FILLED\"\n",
    "    ]).sort_values([\"STATIONID\",\"COLUMN\",\"FILLABLE_START\"]).reset_index(drop=True)\n",
    "\n",
    "    # Done\n",
    "    target = target.sort_index()\n",
    "    if return_plan:\n",
    "        return target, audit, plan\n",
    "    return target, audit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0089eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming summarize_gaps() and compare_gap_summaries() are defined (from earlier),\n",
    "# and df_a (target) and df_b (source) are your MultiIndex DataFrames.\n",
    "\n",
    "df_a = pd.read_parquet(raw_fold / \"easyflux.parquet\")\n",
    "df_b = pd.read_parquet(raw_fold / \"comp_edd.parquet\")\n",
    "filled_a, audit = fill_missing_from_other(\n",
    "    df_target=df_a,\n",
    "    df_source=df_b,\n",
    "    expected_freq=\"30min\",\n",
    "    add_missing_timestamps=True,     # add structurally-missing rows before filling\n",
    "    min_steps=1,                     # ignore super-short segments if you want, e.g., min_steps=2\n",
    "    columns=[\"LE_1_1_1\",\"H_1_1_1\",\"NETRAD_1_1_1\",\n",
    "             \"LW_IN_1_1_1\",\"SW_IN_1_1_1\",\"SW_OUT_1_1_1\",\"LW_OUT_1_1_1\"],         # or None to auto-use shared columns\n",
    "    station_level=\"STATIONID\",\n",
    "    time_level=\"DATETIME_START\",\n",
    ")\n",
    "\n",
    "print(audit.head())\n",
    "# filled_a now contains values copied from df_b wherever plan said B could fill A.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15cd822",
   "metadata": {},
   "source": [
    "## Eddy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40345a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edd = pd.read_parquet(raw_fold /  \"comp_edd.parquet\",).replace(-9999,np.nan)\n",
    "df_edd.index.names = ['STATIONID','DATETIME_START']\n",
    "df_edd['PRIORITY'] = 1\n",
    "\n",
    "df = pd.read_parquet(raw_fold /  \"easyflux.parquet\",).replace(-9999,np.nan)\n",
    "df.index.names = ['STATIONID','DATETIME_START']\n",
    "df['PRIORITY'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c5adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_parquet(raw_fold / \"comp_cs_flux.parquet\")\n",
    "df_merged.index.names = ['STATIONID','DATETIME_START']\n",
    "df_merged['PRIORITY'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb54aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cff8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdb = pd.read_parquet(raw_fold /  \"old_database_eddy.parquet\",).replace(-9999,np.nan)\n",
    "dfdb.columns = dfdb.columns.str.upper()\n",
    "dfdb['DATETIME_START'] = pd.to_datetime(dfdb['DATETIME_START'])\n",
    "#dfdb[\"TIMESTAMP_START\"] = dfdb['datetime_start'].apply(lambda x: f\"{x:%Y%m%d%H%M}\")\n",
    "dfdb = dfdb.set_index(['STATIONID','DATETIME_START'])\n",
    "#df.index.names = ['station','datetime']\n",
    "\n",
    "dfdb.columns = dfdb.columns.str.upper()\n",
    "rename_dict = {'CO2':'CO2_1_1_1', \n",
    "               'CO2_SIGMA':'CO2_SIGMA_1_1_1', \n",
    "               'H2O':'H2O_1_1_1', \n",
    "               'H2O_SIGMA':'H2O_SIGMA_1_1_1',\n",
    "               'FC':'FC_1_1_1', \n",
    "               'FC_SSITC_TEST':'FC_SSITC_TEST_1_1_1', \n",
    "               'LE':'LE_1_1_1',\n",
    "               'LE_SSITC_TEST':'LE_SSITC_TEST_1_1_1', \n",
    "               'ET':'ET_1_1_1',\n",
    "               'ET_SSITC_TEST':'ET_SSITC_TEST_1_1_1', \n",
    "               'H':'H_1_1_1',\n",
    "               'H_SSITC_TEST':'H_SSITC_TEST_1_1_1', \n",
    "               'G':'G_1_1_A',\n",
    "               'G_SSITC_TEST':'G_SSITC_TEST_1_1_1',\n",
    "               'SG':'SG_1_1_1', \n",
    "               'WD':'WD_1_1_1', \n",
    "               'WS':'WS_1_1_1', \n",
    "               'WS_MAX':'WS_MAX_1_1_1',\n",
    "               'PA':'PA_1_1_1', \n",
    "               'VPD':'VPD_1_1_1', \n",
    "               'ALB':'ALB_1_1_1', \n",
    "               'NETRAD':'NETRAD_1_1_1', \n",
    "               'SW_IN':'SW_IN_1_1_1',\n",
    "               'SW_OUT':'SW_OUT_1_1_1', \n",
    "               'LW_IN':'LW_IN_1_1_1', \n",
    "               'LW_OUT':'LW_OUT_1_1_1', \n",
    "               'P':'P_1_1_1', \n",
    "               }\n",
    "\n",
    "dfdb = dfdb.rename(columns=rename_dict)\n",
    "dfdb['ET_1_1_1'].where(dfdb['ET_1_1_1'].between(0,1.1),np.nan)\n",
    "dfdb['PRIORITY'] = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def coalesce_by_priority_multiindex(\n",
    "    df_or_dfs,\n",
    "    priority_col=\"priority\",\n",
    "    ascending=True,\n",
    "    invalid_values=(-9999,),\n",
    "    keep_index=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Column-wise coalesce: for each MultiIndex group (all index levels except `priority_col`),\n",
    "    take the first non-null value per column after sorting by priority.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_or_dfs : DataFrame or list/tuple of DataFrames\n",
    "        Concatenated DataFrame (or list to be concatenated) with a MultiIndex.\n",
    "    priority_col : str\n",
    "        Column name (or index level name) indicating priority. Lower/greater is better\n",
    "        depending on `ascending`.\n",
    "    ascending : bool\n",
    "        Sort so that smaller (True) or larger (False) priority wins.\n",
    "    invalid_values : tuple\n",
    "        Treat these values as missing.\n",
    "    keep_index : bool\n",
    "        Keep the MultiIndex in the result. If False, returns a reset_index frame.\n",
    "    \"\"\"\n",
    "    # 0) Accept list of dfs or a single df\n",
    "    if isinstance(df_or_dfs, (list, tuple)):\n",
    "        df = pd.concat(df_or_dfs, axis=0)\n",
    "    else:\n",
    "        df = df_or_dfs.copy()\n",
    "\n",
    "    # 1) If priority is an index level, move it to a column (so we don't group by it)\n",
    "    if isinstance(df.index, pd.MultiIndex) and priority_col in df.index.names:\n",
    "        df = df.reset_index(level=priority_col)\n",
    "\n",
    "    # 2) Define group levels = all current index levels (MultiIndex) → the \"keys\"\n",
    "    if not isinstance(df.index, pd.MultiIndex):\n",
    "        raise ValueError(\"Expected a MultiIndex index. Set your keys as the DataFrame index first.\")\n",
    "    group_levels = list(df.index.names)\n",
    "\n",
    "    # 3) Value columns = all columns except the priority column\n",
    "    if priority_col not in df.columns:\n",
    "        raise ValueError(f\"'{priority_col}' must be a column or an index level.\")\n",
    "    value_cols = [c for c in df.columns if c != priority_col]\n",
    "\n",
    "    # 4) Treat sentinels as NaN\n",
    "    if invalid_values:\n",
    "        for v in invalid_values:\n",
    "            df[value_cols] = df[value_cols].mask(df[value_cols].eq(v))\n",
    "    df[value_cols] = df[value_cols].where(df[value_cols].notna(), np.nan)\n",
    "\n",
    "    # 5) Sort by priority (best first)\n",
    "    df = df.sort_values(priority_col, ascending=ascending)\n",
    "\n",
    "    # 6) Per group & per column, take the first non-null\n",
    "    def _first_valid(s):\n",
    "        s = s.dropna()\n",
    "        return s.iloc[0] if len(s) else np.nan\n",
    "\n",
    "    out = (\n",
    "        df.groupby(level=group_levels, sort=False)[value_cols]\n",
    "          .agg(_first_valid)\n",
    "    )\n",
    "\n",
    "    return out if keep_index else out.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "result = coalesce_by_priority_multiindex([df,df_edd,df_merged],  \n",
    "                              priority_col=\"PRIORITY\", \n",
    "                              ascending=True, \n",
    "                              invalid_values=(-9999,np.nan,\"NAN\",None))\n",
    "result.to_parquet(raw_fold / \"combined_eddy_dataset_20250905.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d298d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def coalesce_by_priority_multiindex_fast(\n",
    "    df_or_dfs,\n",
    "    priority_col=\"PRIORITY\",\n",
    "    ascending=True,\n",
    "    invalid_values=(-9999,),\n",
    "    keep_index=True,\n",
    "):\n",
    "    # 1) Combine frames\n",
    "    if isinstance(df_or_dfs, (list, tuple)):\n",
    "        df = pd.concat(df_or_dfs, axis=0)\n",
    "    else:\n",
    "        df = df_or_dfs.copy()\n",
    "\n",
    "    # 2) Ensure PRIORITY is an index level (last)\n",
    "    if priority_col in df.columns:\n",
    "        df = df.set_index(priority_col, append=True)\n",
    "    elif not (isinstance(df.index, pd.MultiIndex) and priority_col in df.index.names):\n",
    "        raise ValueError(f\"'{priority_col}' must be a column or an index level.\")\n",
    "    levels = list(df.index.names)\n",
    "    if levels[-1] != priority_col:\n",
    "        levels.remove(priority_col)\n",
    "        levels.append(priority_col)\n",
    "        df = df.reorder_levels(levels).sort_index()\n",
    "\n",
    "    value_cols = list(df.columns)  # all non-index columns\n",
    "\n",
    "    # 3) Normalize invalids → NaN\n",
    "    if invalid_values:\n",
    "        for v in invalid_values:\n",
    "            # Skip np.nan because .eq(np.nan) is always False\n",
    "            if isinstance(v, float) and np.isnan(v):\n",
    "                continue\n",
    "            df[value_cols] = df[value_cols].mask(df[value_cols].eq(v))\n",
    "    df[value_cols] = df[value_cols].where(df[value_cols].notna(), np.nan)\n",
    "\n",
    "    # 4) DEDUP step: collapse duplicates per (keys..., PRIORITY)\n",
    "    #    For each group & column, take the first non-null.\n",
    "    def _first_valid(s):\n",
    "        s = s.dropna()\n",
    "        return s.iloc[0] if len(s) else np.nan\n",
    "\n",
    "    df = (\n",
    "        df.groupby(level=list(df.index.names), sort=False)[value_cols]\n",
    "          .agg(_first_valid)\n",
    "    )\n",
    "\n",
    "    # 5) Unstack PRIORITY and fill across priority dimension (best → worse)\n",
    "    wide = df.unstack(priority_col)  # columns: (value_col, priority)\n",
    "    wide = wide.sort_index(axis=1, level=1, ascending=ascending)\n",
    "    filled = wide.bfill(axis=1)\n",
    "\n",
    "    # 6) Take the first (best) priority slice for each value column\n",
    "    best_priority_label = filled.columns.levels[1][0]\n",
    "    out = filled.xs(best_priority_label, level=1, axis=1)\n",
    "\n",
    "    return out if keep_index else out.reset_index()\n",
    "\n",
    "result2 = coalesce_by_priority_multiindex_fast([df,df_edd,df_merged, dfdb],  \n",
    "                              priority_col=\"PRIORITY\", \n",
    "                              ascending=True, \n",
    "                              invalid_values=(-9999,np.nan,\"NAN\",None))\n",
    "result2.to_parquet(raw_fold / \"combined_eddy_dataset_20250905_v2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2.loc['US-UTV','NETRAD_1_1_1'].sort_index().plot()\n",
    "plt.ylim(0,800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_static_outliers(\n",
    "    df: pd.DataFrame,\n",
    "    thresh: float = 4.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace values that deviate more than `thresh` standard deviations\n",
    "    from the *station-wide* mean (no moving window).\n",
    "\n",
    "    Outlier detection is performed separately for each station (level-0\n",
    "    of the MultiIndex).  Only floating-point columns are filtered.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        MultiIndex DataFrame with outer index = stationid and inner\n",
    "        index = datetime (half-hourly).\n",
    "    thresh : float, default 3.0\n",
    "        Number of σ from the mean that defines an outlier.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Copy of `df` with outliers in float columns replaced by NaN.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid mutating the caller’s DataFrame\n",
    "    df = df.copy()\n",
    "\n",
    "    # Select only float columns (ignore integers, objects, etc.)\n",
    "    float_cols = df.select_dtypes(include=[np.floating]).columns\n",
    "    if float_cols.empty:\n",
    "        return df                        # nothing to do\n",
    "\n",
    "    # Compute station-specific mean and std, broadcast back with transform\n",
    "    grp = df[float_cols].groupby(level=0)\n",
    "    mean  = grp.transform(\"mean\")\n",
    "    std   = grp.transform(\"std\")         # sample std (ddof=1) like pandas default\n",
    "\n",
    "    # Identify outliers and replace with NaN\n",
    "    mask = (df[float_cols] - mean).abs() > thresh * std\n",
    "    df.loc[:, float_cols] = df[float_cols].mask(mask)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo = pd.concat([df,df_edd,df_merged],axis=0)\n",
    "# Remove duplicate station datetime values, keeping the non-na values\n",
    "combo = combo.sort_values(['LE_1_1_1','NETRAD_1_1_1','priority']).sort_index()\n",
    "combo = combo.reset_index().drop_duplicates(subset=['stationid','datetime_start'],keep='first')\n",
    "combo = combo.set_index(['stationid','datetime_start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't run this- drops most precip values\n",
    "# clean_df = filter_static_outliers(combo, thresh=4)  # custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a16176",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo.to_parquet(raw_fold / \"combined_eddy_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131cc93f",
   "metadata": {},
   "source": [
    "## Met Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f480e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_met = pd.read_parquet(raw_fold /  \"comp_met.parquet\",).replace(-9999,np.nan)\n",
    "df_met.index.names = ['stationid','datetime_start']\n",
    "df_met['priority'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c8f2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stmet = pd.read_parquet(raw_fold / \"comp_met_stat.parquet\")\n",
    "stmet['datetime_start'] = pd.to_datetime(stmet['TIMESTAMP_START'],format=\"%Y%m%d%H%M\")\n",
    "stmet = stmet.reset_index()\n",
    "stmet = stmet.rename(columns = {'level_0':'stationid'})\n",
    "stmet = stmet.set_index(['stationid','datetime_start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca9091",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdbm = pd.read_parquet(raw_fold /  \"old_database_met.parquet\",).replace(-9999,np.nan)\n",
    "dfdbm['datetime_start'] = pd.to_datetime(dfdbm['datetime_start'])\n",
    "dfdbm = dfdbm.set_index(['stationid','datetime_start'])\n",
    "#df.index.names = ['station','datetime']\n",
    "\n",
    "dfdbm.columns = dfdbm.columns.str.upper()\n",
    "rename_dict_m = {'CO2':'CO2_1_1_2', \n",
    "               'CO2_SIGMA':'CO2_SIGMA_1_1_2', \n",
    "               'H2O':'H2O_1_1_2', \n",
    "               'H2O_SIGMA':'H2O_SIGMA_1_1_2',\n",
    "               'FC':'FC_1_1_2', \n",
    "               'FC_SSITC_TEST':'FC_SSITC_TEST_1_1_2', \n",
    "               'LE':'LE_1_1_2',\n",
    "               'LE_SSITC_TEST':'LE_SSITC_TEST_1_1_2', \n",
    "               'ET':'ET_1_1_2',\n",
    "               'ET_SSITC_TEST':'ET_SSITC_TEST_1_1_2', \n",
    "               'H':'H_1_1_2',\n",
    "               'H_SSITC_TEST':'H_SSITC_TEST_1_1_2', \n",
    "               'G':'G_1_1_A',\n",
    "               'G_SSITC_TEST':'G_SSITC_TEST_1_1_2',\n",
    "               'SG':'SG_1_1_2', \n",
    "               'WD':'WD_1_1_2', \n",
    "               'WS':'WS_1_1_2', \n",
    "               'WS_MAX':'WS_MAX_1_1_2',\n",
    "               'PA':'PA_1_1_2', \n",
    "               'VPD':'VPD_1_1_2', \n",
    "               'ALB':'ALB_1_1_2', \n",
    "               'NETRAD':'NETRAD_1_1_2', \n",
    "               'SW_IN':'SW_IN_1_1_2',\n",
    "               'SW_OUT':'SW_OUT_1_1_2', \n",
    "               'LW_IN':'LW_IN_1_1_2', \n",
    "               'LW_OUT':'LW_OUT_1_1_2', \n",
    "               'P':'P_1_1_2', \n",
    "               }\n",
    "\n",
    "dfdbm = dfdbm.rename(columns=rename_dict_m)\n",
    "#dfdb['ET_1_1_1'].where(dfdb['ET_1_1_1'].between(0,1.1),np.nan)\n",
    "dfdbm['priority'] = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eed569",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_met = pd.concat([df_met,dfdbm,stmet],axis=0)\n",
    "combo_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328650af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate station datetime values, keeping the non-na values\n",
    "combo_met = combo_met.sort_values(['NETRAD_1_1_2','priority']).sort_index()\n",
    "combo_met = combo_met.reset_index().drop_duplicates(subset=['stationid','datetime_start'],keep='first')\n",
    "combo_met = combo_met.set_index(['stationid','datetime_start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c7fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# may want to revisit whether to run this- caused issues with precip data for the eddy stations\n",
    "# clean_df_met = filter_static_outliers(combo_met, thresh=4)  # custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_met.to_parquet(raw_fold / \"combined_met_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d5549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "met  = pd.read_parquet(raw_fold / \"combined_met_dataset.parquet\")\n",
    "eddy = pd.read_parquet(raw_fold / \"combined_eddy_dataset.parquet\")\n",
    "#met.to_csv(raw_fold / \"combined_met_dataset.csv\")\n",
    "#eddy.to_csv(raw_fold / \"combined_eddy_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281585a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.merge(met, eddy, how='outer', left_index=True, right_index=True,\n",
    "         suffixes=('_met', '_eddy'))\n",
    "\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.loc['US-UTD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d887f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.loc['US-UTD', ['WS','WS_1_1_1']].dropna().plot(kind='scatter',x='WS',y='WS_1_1_1',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_cols = [\"WS\", \n",
    "                \"TA_\", \n",
    "                \"RH_\", \n",
    "                \"LE_\", \n",
    "                \"H_\", \n",
    "                \"VPD\", \n",
    "                \"PA\", \n",
    "                \"WD\", \n",
    "                \"NETRAD\", \n",
    "                \"SW_IN_\", \n",
    "                \"SW_OUT_\", \n",
    "                \"LW_IN_\", \n",
    "                \"LW_OUT_\", \n",
    "                \"ALB\"]\n",
    "\n",
    "matches = {}\n",
    "for i in compare_cols:\n",
    "    values = []\n",
    "    met_col = []\n",
    "    eddy_col = []\n",
    "\n",
    "    for col in met.columns:\n",
    "        if 'MAX' not in col and 'SSITC' not in col:\n",
    "            if col.startswith(i):\n",
    "                values.append(col)\n",
    "                met_col.append(col)\n",
    "\n",
    "    for col in eddy.columns:\n",
    "        if 'MAX' not in col and 'SSITC' not in col:\n",
    "            if col.startswith(i):\n",
    "                values.append(col)\n",
    "                eddy_col.append(col)\n",
    "\n",
    "    matches[i] = values\n",
    "    if len(values) > 1:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        plt.title(f\"Comparison of {i} for US-UTD\")\n",
    "        for j in met_col:\n",
    "            met.loc['US-UTD',j].replace(-9999,np.nan).plot(label=j,ax=ax)\n",
    "        for k in eddy_col:\n",
    "            eddy.loc['US-UTD',k].replace(-9999,np.nan).plot(label=k,ax=ax)\n",
    "        plt.legend()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccca283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from collections import defaultdict\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. LOAD  (needs pyarrow or fastparquet installed)\n",
    "# --------------------------------------------------\n",
    "met  = pd.read_parquet(raw_fold / \"combined_met_dataset.parquet\")\n",
    "eddy = pd.read_parquet(raw_fold / \"combined_eddy_dataset.parquet\")\n",
    "\n",
    "# If not already multi-indexed by (station, timestamp):\n",
    "# met  = met.set_index([\"station_id\", \"timestamp\"]).sort_index()\n",
    "# eddy = eddy.set_index([\"station_id\", \"timestamp\"]).sort_index()\n",
    "\n",
    "# Keep only overlapping station–time rows\n",
    "common_idx = met.index.intersection(eddy.index)\n",
    "met, eddy  = met.loc[common_idx], eddy.loc[common_idx]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. DEFINE THE PREFIXES YOU WANT TO COMPARE\n",
    "#    (fill this list in with your own)\n",
    "# --------------------------------------------------\n",
    "prefixes = [\"WS\", \"TA\", \"RH\", \"LE\", \"H\", \"VPD\", \"PA\", \"WD\", \"NETRAD\", \"SW_IN\", \"SW_OUT\", \"LW_IN\", \"LW_OUT\", \"ALB\"]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. BUILD A MATCH TABLE  {prefix -> [(met_col, eddy_col), …]}\n",
    "# --------------------------------------------------\n",
    "matches = defaultdict(list)\n",
    "\n",
    "for p in prefixes:\n",
    "    # columns that begin with that prefix\n",
    "    met_cols  = [c for c in met.columns  if c.startswith(p)]\n",
    "    eddy_cols = [c for c in eddy.columns if c.startswith(p)]\n",
    "\n",
    "    # simplest strategy: look for *exact* column-name matches\n",
    "    common = set(met_cols).intersection(eddy_cols)\n",
    "    for col in common:\n",
    "        matches[p].append((col, col))\n",
    "\n",
    "    # fallback: if names differ after the prefix, pair by the suffix\n",
    "    if not common:\n",
    "        met_suffix  = {c[len(p):]: c for c in met_cols}\n",
    "        eddy_suffix = {c[len(p):]: c for c in eddy_cols}\n",
    "        for suf in met_suffix.keys() & eddy_suffix.keys():\n",
    "            matches[p].append((met_suffix[suf], eddy_suffix[suf]))\n",
    "\n",
    "# sanity check\n",
    "if not any(matches.values()):\n",
    "    raise ValueError(\"No columns matched with the given prefixes!\")\n",
    "else:\n",
    "    print(f\"Found {len(matches)} prefixes with matches:\")\n",
    "    for p, pairs in matches.items():\n",
    "        print(f\"  {p}: {len(pairs)} pairs\")\n",
    "        for mcol, ecol in pairs:\n",
    "            print(f\"    {mcol} ↔ {ecol}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. COLLECT ALL DIFFERENCES INTO ONE DATAFRAME\n",
    "#    (column names => \"<prefix><suffix>_diff\")\n",
    "# --------------------------------------------------\n",
    "diff_frames = []\n",
    "for p, pairs in matches.items():\n",
    "    for mcol, ecol in pairs:\n",
    "        name = f\"{mcol}_diff\"          # keeps original met name for clarity\n",
    "        diff_frames.append(\n",
    "            (name, met[mcol] - eddy[ecol])\n",
    "        )\n",
    "\n",
    "# combine into a single MultiIndex-friendly DataFrame\n",
    "diff = pd.concat(\n",
    "    {name: series for name, series in diff_frames}, axis=1\n",
    ")\n",
    "\n",
    "abs_diff = diff.abs()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. OUTLIER METHODS\n",
    "# --------------------------------------------------\n",
    "# 5A. Z-score (3σ)\n",
    "z_scores = abs_diff.groupby(level=0).transform(\n",
    "    lambda g: (g - g.mean()) / g.std(ddof=0)\n",
    ")\n",
    "flags_z = z_scores > 3\n",
    "\n",
    "# 5B. MAD (3.5× MAD)\n",
    "def mad_flags(s, k=3.5):\n",
    "    med = s.median()\n",
    "    mad = np.median(np.abs(s - med))\n",
    "    return np.abs(s - med) / (1.4826 * mad + 1e-9) > k\n",
    "\n",
    "flags_mad = abs_diff.groupby(level=0).transform(mad_flags)\n",
    "\n",
    "# 5C. Isolation Forest (multivariate, per station)\n",
    "flags_if = pd.DataFrame(False, index=abs_diff.index, columns=abs_diff.columns)\n",
    "\n",
    "for stn, g in abs_diff.groupby(level=0):\n",
    "    X   = g.values\n",
    "    ok  = np.any(~np.isnan(X), axis=1)\n",
    "    if ok.sum() < 20:                # need enough rows to fit\n",
    "        continue\n",
    "\n",
    "    clf = IsolationForest(\n",
    "        n_estimators=300,\n",
    "        contamination=0.01,\n",
    "        random_state=42,\n",
    "    ).fit(X[ok])\n",
    "\n",
    "    row_out = clf.predict(X[ok]) == -1   # → Boolean vector\n",
    "    # broadcast to all columns\n",
    "    flags_if.loc[g.index[ok], :] = np.repeat(\n",
    "        row_out[:, None], g.shape[1], axis=1\n",
    "    )\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6. QUICK SUMMARY  (how many flags per variable)\n",
    "# --------------------------------------------------\n",
    "summary = (\n",
    "    pd.DataFrame({\n",
    "        \"Zscore\": flags_z.sum(),\n",
    "        \"MAD\":    flags_mad.sum(),\n",
    "        \"IsoF\":   flags_if.sum(),\n",
    "    })\n",
    "    .sort_index()\n",
    ")\n",
    "print(summary.head())\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7. OPTIONAL:  EXPORT OR APPLY MASK\n",
    "# --------------------------------------------------\n",
    "# Example: mask out any value flagged by *any* method\n",
    "combined_flags = flags_z | flags_mad | flags_if\n",
    "clean_met  = met.where(~combined_flags)  # replaces flagged cells with NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29359bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "met_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adfd5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# ---------- 1. LOAD ----------\n",
    "met   = pd.read_parquet(raw_fold /\"combined_met_dataset.parquet\")   # needs pyarrow or fastparquet\n",
    "eddy  = pd.read_parquet(raw_fold /\"combined_eddy_dataset.parquet\")\n",
    "\n",
    "# If your indices aren’t yet a MultiIndex (station, time) do this once:\n",
    "# met  = met.set_index([\"station_id\",\"timestamp\"]).sort_index()\n",
    "# eddy = eddy.set_index([\"station_id\",\"timestamp\"]).sort_index()\n",
    "\n",
    "# Keep only the overlapping stations & times\n",
    "common_idx = met.index.intersection(eddy.index)\n",
    "met  = met.loc[common_idx]\n",
    "eddy = eddy.loc[common_idx]\n",
    "\n",
    "# ---------- 2. IDENTIFY MATCHING VARIABLES ----------\n",
    "common_cols = met.columns.intersection(eddy.columns)\n",
    "if common_cols.empty:\n",
    "    raise ValueError(\"No shared measurement names between the two datasets!\")\n",
    "\n",
    "# Optionally drop columns that are integer-typed (often flags / counters)\n",
    "keep_float = [c for c in common_cols if np.issubdtype(met[c].dtype, np.floating)]\n",
    "met  = met[keep_float]\n",
    "eddy = eddy[keep_float]\n",
    "\n",
    "# ---------- 3. STACK THE TWO SOURCES FOR EZ COMPARISON ----------\n",
    "diff = met - eddy              # sign tells you which source is higher\n",
    "abs_diff = diff.abs()\n",
    "\n",
    "# ---------- 4A. Z-SCORE BASED OUTLIERS ----------\n",
    "z_scores = abs_diff.groupby(level=0).transform(  # compute σ station-by-station\n",
    "    lambda g: (g - g.mean()) / g.std(ddof=0)\n",
    ")\n",
    "outliers_z = z_scores > 3        # boolean DF same shape as diff\n",
    "\n",
    "# ---------- 4B. MAD BASED OUTLIERS ----------\n",
    "def mad_based_flags(series, k=3.5):\n",
    "    med = series.median()\n",
    "    mad = np.median(np.abs(series - med))\n",
    "    # 1.4826 converts MAD to σ for a normal dist.\n",
    "    return np.abs(series - med) / (1.4826 * mad + 1e-9) > k\n",
    "\n",
    "outliers_mad = abs_diff.groupby(level=0).transform(mad_based_flags)\n",
    "\n",
    "# ---------- 4C. ISOLATION FOREST (multivariate) ----------\n",
    "iso_out = {}\n",
    "for stn, g in abs_diff.groupby(level=0):\n",
    "\n",
    "    X = g.values\n",
    "    mask = np.any(~np.isnan(X), axis=1)          # rows with ≥1 real number\n",
    "    flags = pd.DataFrame(False, index=g.index, columns=g.columns)\n",
    "\n",
    "    if mask.sum() >= 20:                         # enough samples to train\n",
    "        clf = IsolationForest(\n",
    "            contamination=0.01,\n",
    "            n_estimators=300,\n",
    "            random_state=42,\n",
    "        ).fit(X[mask])\n",
    "\n",
    "        row_flags = clf.predict(X[mask]) == -1   # 1-D Boolean (outlier rows)\n",
    "\n",
    "        # --- broadcast row_flags to full (n_rows_selected × n_columns) matrix\n",
    "        flags.iloc[mask, :] = np.repeat(\n",
    "            row_flags[:, None], g.shape[1], axis=1\n",
    "        )\n",
    "\n",
    "    iso_out[stn] = flags\n",
    "\n",
    "outliers_iso = pd.concat(iso_out)\n",
    "\n",
    "# ---------- 5. SUMMARIZE ----------\n",
    "summary = (\n",
    "    pd.DataFrame({\n",
    "        \"z_score\":  outliers_z.sum(),\n",
    "        \"MAD\":      outliers_mad.sum(),\n",
    "        \"iForest\":  outliers_iso.sum()\n",
    "    })\n",
    "    .rename_axis(\"variable\")\n",
    ")\n",
    "print(summary.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8792cc9c",
   "metadata": {},
   "source": [
    "Compile files from each station into a a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d61bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.concat(comp_edd_df, axis=0)\n",
    "cdf.index.set_names(['stationid','datetime_start'],inplace=True)\n",
    "#cdf.rename(columns={'level_0':'stationid'},inplace=True)\n",
    "#cdf.to_parquet('../station_data/all_data.parquet')\n",
    "for col in cdf.columns:\n",
    "    cdf.rename(columns={col:col.lower()},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf106f08",
   "metadata": {},
   "source": [
    "Save to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d86dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.to_parquet('../../station_data/all_eddy_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f62bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577962db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comp_met_df = {}\n",
    "root_dir = \"C:/Users/paulinkenbrandt/Documents/GitHub/MicroMet/src/micromet/data/\"\n",
    "config_path = root_dir + \"reformatter_vars.yml\"\n",
    "var_limits_csv = root_dir + \"extreme_values.csv\"\n",
    "am = micromet.AmerifluxDataProcessor(config_path, logger)\n",
    "\n",
    "\n",
    "for key, value in site_folders.items():\n",
    "\n",
    "    print(key)\n",
    "    raw_fold = pathlib.Path('G:/Shared drives/UGS_Flux/Data_Downloads/')\n",
    "    raw_data = am.raw_file_compile(raw_fold, value, search_str = \"*Statistics_AmeriFlux*.dat\")\n",
    "    if raw_data is not None:\n",
    "        am_data = micromet.Reformatter(\n",
    "                                       config_path=config_path,\n",
    "                                       var_limits_csv= var_limits_csv,\n",
    "                                       drop_soil=False,\n",
    "                                       logger=logger,\n",
    "                                       )\n",
    "        am_df = am_data.prepare(raw_data, data_type=\"met\")\n",
    "        #am_df = am_data.et_data\n",
    "        comp_met_df[key] = am_df\n",
    "\n",
    "        #am_df.to_csv(f\"../../station_data/{key}_HH_{am_df['TIMESTAMP_START'].values[0]:}_{am_df['TIMESTAMP_END'].values[-1]:}.csv\")\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64217e3c28f06cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.columns = ddf.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d44a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "soildfs\n",
    "\n",
    "for old_col, new_col in mapping.items():\n",
    "    if str(old_col).lower() in soildfs.columns.str.lower():\n",
    "        if str(new_col).lower() in soildfs.columns.str.lower():\n",
    "            soildfs[new_col.lower()] = soildfs[[old_col.lower(), new_col.lower()]].max(axis=1)\n",
    "            soildfs = soildfs.drop(old_col.lower(), axis=1)\n",
    "        else:\n",
    "            soildfs = soildfs.rename(columns={old_col.lower(): new_col.lower()})\n",
    "    elif str(old_col).lower()+\"_eddy\" in soildfs.columns.str.lower():\n",
    "        print(f\"Found {old_col} eddy column\")\n",
    "        if str(new_col).lower()+\"_eddy\" in soildfs.columns.str.lower():\n",
    "            soildfs[new_col.lower()] = soildfs[[old_col.lower()+\"_eddy\", new_col.lower()+\"_eddy\"]].max(axis=1)\n",
    "            soildfs = soildfs.drop(old_col.lower()+\"_eddy\", axis=1)\n",
    "        else:\n",
    "            soildfs = soildfs.rename(columns={old_col.lower()+\"_eddy\": new_col.lower()})\n",
    "    elif str(new_col).lower()+\"_eddy\" in soildfs.columns.str.lower():\n",
    "        if str(new_col).lower() in soildfs.columns.str.lower():\n",
    "            soildfs[new_col.lower()] = soildfs[[new_col.lower()+\"_eddy\", new_col.lower()+\"_eddy\"]].max(axis=1)\n",
    "            soildfs = soildfs.drop(new_col.lower()+\"_eddy\", axis=1)\n",
    "            print(f\"Found {new_col} eddy column\")\n",
    "        else:\n",
    "            print(f\"Found {new_col} eddy column\")\n",
    "            soildfs = soildfs.rename(columns={new_col.lower()+\"_eddy\": new_col.lower()})\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.concat(comp_met_df, axis=0)\n",
    "ddf.index.set_names(['stationid','datetime_start'],inplace=True)\n",
    "#cdf.rename(columns={'level_0':'stationid'},inplace=True)\n",
    "#cdf.to_parquet('../station_data/all_data.parquet')\n",
    "for col in ddf.columns:\n",
    "    ddf.rename(columns={col:col.lower()},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf042fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[~ddf['vwc_2_7_1'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef3316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.iloc[0:1,:].to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f082e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "soilcols = [col.lower() for col in am_data.MATH_SOILS_V2]\n",
    "pattern = re.compile(r\"2_1_1|1_2_1|1_1_2\")\n",
    "# Print matching columns\n",
    "matching_cols = [col for col in soilcols if pattern.search(col)]\n",
    "# Remove them from the original list\n",
    "soilcols = [col for col in soilcols if not pattern.search(col)]\n",
    "\n",
    "        \n",
    "soildfs = pd.merge(ddf,cdf[soilcols],how='left',on=['stationid','datetime_start'],suffixes=(None,'_eddy'))\n",
    "soildfs\n",
    "\n",
    "for col in cdf.columns:\n",
    "    if col in soilcols:\n",
    "        cdf.drop(columns=col,inplace=True)  # drop the soil columns from the main dataframe\n",
    "\n",
    "cdf.to_parquet('../../station_data/all_eddy_data.parquet')\n",
    "\n",
    "soildfs.to_parquet('../../station_data/all_soil_data.parquet')\n",
    "\n",
    "ddf.to_parquet('../../station_data/all_met_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f9e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.read_parquet('../../station_data/all_eddy_data.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84295da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "soildfs = pd.read_parquet('../../station_data/all_soil_data.parquet')\n",
    "utd_soilt = soildfs.loc['US-UTD'][['ts_3_1_1','ts_3_2_1','ts_3_3_1']].replace(-9999,np.nan)\n",
    "utd_soilt = utd_soilt[utd_soilt.index >= '2024-07-01']#.resample('30T').mean()\n",
    "utd_soilt['ts_3_1_1'].plot()\n",
    "utd_soilt['ts_3_2_1'].shift(-1).plot()\n",
    "utd_soilt['ts_3_3_1'].shift(-5).plot()\n",
    "plt.axvline('2024-07-04 15:00',color='r')\n",
    "#plt.xlim('2024-07-01','2024-07-08')\n",
    "#plt.ylim(10,35)\n",
    "plt.grid(True, which='minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy.signal import correlate\n",
    "\n",
    "# Function to decompose the seasonal component\n",
    "def extract_seasonal(ts, period):\n",
    "    decomposition = seasonal_decompose(ts, model='additive', period=period)\n",
    "    return decomposition.seasonal\n",
    "\n",
    "# Function to calculate lag between two seasonal series using cross-correlation\n",
    "def calculate_lag(seasonal1, seasonal2):\n",
    "    n = len(seasonal1)\n",
    "    correlation = correlate(seasonal1 - np.mean(seasonal1), seasonal2 - np.mean(seasonal2), mode='full')\n",
    "    lags = np.arange(-n + 1, n)\n",
    "    lag = lags[np.argmax(correlation)]\n",
    "    return lag, correlation, lags\n",
    "\n",
    "ts1 = utd_soilt['ts_3_2_1']\n",
    "ts2 = utd_soilt['ts_3_3_1']\n",
    "#utd_soilt['ts_3_3_1'].shift(-5).plot()\n",
    "\n",
    "\n",
    "# Extract seasonal components\n",
    "seasonal1 = extract_seasonal(ts1, period=48)\n",
    "seasonal2 = extract_seasonal(ts2, period=48)\n",
    "\n",
    "# Calculate lag\n",
    "lag, correlation, lags = calculate_lag(seasonal1.dropna(), seasonal2.dropna())\n",
    "\n",
    "# Output\n",
    "print(f\"Calculated lag: {lag/2} hours\")\n",
    "\n",
    "# Plot seasonal components and correlation\n",
    "fig, ax = plt.subplots(3, 1, figsize=(10, 8))\n",
    "\n",
    "seasonal1.plot(ax=ax[0], label='Seasonal Component 1')\n",
    "seasonal2.plot(ax=ax[0], label='Seasonal Component 2')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Seasonal Components')\n",
    "ax[0].set_xlim(pd.to_datetime('2024-07-01'),pd.to_datetime('2024-07-08'))\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(lags, correlation)\n",
    "ax[1].set_title('Cross-Correlation')\n",
    "ax[1].set_xlabel('Lag (hours)')\n",
    "ax[1].set_ylabel('Correlation')\n",
    "ax[1].set_xlim(-10, 10)\n",
    "ax[1].grid(True)\n",
    "\n",
    "ax[2].plot(seasonal1.index, seasonal1, label='Series 1')\n",
    "ax[2].plot(seasonal2.index + pd.Timedelta(hours=lag/2), seasonal2, label='Series 2 (Shifted)')\n",
    "ax[2].legend()\n",
    "ax[2].set_title(f'Series alignment (Lag: {lag/2} hours)')\n",
    "ax[2].set_xlim(pd.to_datetime('2024-07-01'),pd.to_datetime('2024-07-08'))\n",
    "ax[2].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04ee994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc536c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.read_parquet('../../station_data/all_eddy_data.parquet')\n",
    "ddf = pd.read_parquet('../../station_data/all_met_data.parquet')\n",
    "\n",
    "for col in cdf.columns:\n",
    "    if col in ddf.columns:\n",
    "        print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head(10).to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549094d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = ddf.loc['US-UTD','t_si111_body'].replace(-9999,np.nan)\n",
    "series.plot()\n",
    "series.diff().plot()\n",
    "new_series = series[series.diff()<2].diff().cumsum()\n",
    "new_series.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddcebdfd6a7b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read('../../secrets/config.ini')\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse\n",
    "host = config['DEFAULT']['ip']\n",
    "pw = config['DEFAULT']['pw']\n",
    "user = config['DEFAULT']['login']\n",
    "\n",
    "encoded_password = urllib.parse.quote_plus(pw)\n",
    "\n",
    "def postconn_et(encoded_password, host='localhost',user='postgres',port='5432',db='groundwater', schema = 'groundwater'):\n",
    "    connection_text = \"postgresql+psycopg2://{:}:{:}@{:}:{:}/{:}?gssencmode=disable\".format(user,encoded_password,host,port,db)\n",
    "    return create_engine(connection_text, connect_args={'options': '-csearch_path={}'.format(schema)})\n",
    "\n",
    "\n",
    "engine = postconn_et(encoded_password, host=host, user=user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ce788442a9680",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.to_sql(name = 'amfluxeddy',\n",
    "           schema='groundwater',\n",
    "           con=engine,\n",
    "           if_exists='replace',\n",
    "           chunksize=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f92cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in soildfs.columns:\n",
    "    print(f\"amfluxmet.{col},\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5819ddd94230e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "soildfs.to_sql(name = 'amfluxmet',\n",
    "           schema='groundwater',\n",
    "           con=engine,\n",
    "           if_exists='replace',\n",
    "           chunksize=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MicroMet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
