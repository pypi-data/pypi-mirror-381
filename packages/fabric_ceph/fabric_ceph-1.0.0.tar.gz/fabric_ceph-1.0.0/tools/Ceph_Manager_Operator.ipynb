{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58430ca2",
   "metadata": {},
   "source": [
    "\n",
    "# Ceph Manager — Operator Notebook\n",
    "\n",
    "Use this notebook to **add/delete CephX users** and **add/resize/delete CephFS subvolumes**, either **one at a time** or **in bulk from CSV files**.\n",
    "\n",
    "This notebook expects the Python client `CephManagerClient` to be importable (from your package or as a module).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1ec180dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T17:54:34.097435Z",
     "start_time": "2025-10-02T17:54:34.016174Z"
    }
   },
   "source": [
    "\n",
    "# Optional: uncomment to install dependencies in this environment\n",
    "# !pip install requests pandas urllib3\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# If your client is in your repo path, you can add it like this:\n",
    "# import sys, os\n",
    "# sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "try:\n",
    "    from fabric_ceph_client import CephManagerClient, ApiError\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Could not import fabric_ceph_client. Ensure it's on PYTHONPATH or installed.\") from e\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mdataclasses\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m dataclass\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mjson\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpd\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m# If your client is in your repo path, you can add it like this:\u001B[39;00m\n\u001B[32m     10\u001B[39m \u001B[38;5;66;03m# import sys, os\u001B[39;00m\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m# sys.path.append(os.path.abspath(\"..\"))\u001B[39;00m\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'pandas'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "6e76e006",
   "metadata": {},
   "source": [
    "\n",
    "## Configure connection\n",
    "Set your service URL, token (if required), and optional default cluster try-order via `X-Cluster`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4337a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_URL = \"http://localhost:11000\"   # e.g., \"https://mgr.example.org/api\"\n",
    "TOKEN = None                           # paste a JWT if your service requires it, else None\n",
    "DEFAULT_X_CLUSTER = \"europe,lab\"       # optional: influence cluster try order\n",
    "VERIFY_TLS = True                      # set False only for testing self-signed certs\n",
    "\n",
    "client = CephManagerClient(\n",
    "    base_url=BASE_URL,\n",
    "    token=TOKEN,\n",
    "    default_x_cluster=DEFAULT_X_CLUSTER,\n",
    "    verify=VERIFY_TLS,\n",
    "    timeout=60,\n",
    ")\n",
    "print(\"Client ready:\", client.base_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16805d92",
   "metadata": {},
   "source": [
    "\n",
    "## CephX user helpers\n",
    "Single operations and CSV batch upload.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3daabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upsert_user(user_entity: str, capabilities: List[Dict[str, str]], x_cluster: Optional[str]=None) -> Dict[str, Any]:\n",
    "    \"\"\"Create or update a CephX user. Server may implement upsert; this calls POST first, then falls back to PUT.\"\"\"\n",
    "    try:\n",
    "        try:\n",
    "            return client.create_user(user_entity, capabilities, x_cluster=x_cluster)\n",
    "        except ApiError as e:\n",
    "            # if user exists or POST is restricted, try PUT\n",
    "            return client.update_user(user_entity, capabilities, x_cluster=x_cluster)\n",
    "    except ApiError as e:\n",
    "        print(\"API error during upsert_user:\", e)\n",
    "        raise\n",
    "\n",
    "def delete_user(entity: str, x_cluster: Optional[str]=None) -> Dict[str, Any]:\n",
    "    try:\n",
    "        return client.delete_user(entity, x_cluster=x_cluster)\n",
    "    except ApiError as e:\n",
    "        print(\"API error during delete_user:\", e)\n",
    "        raise\n",
    "\n",
    "def export_keyrings(entities: List[str], x_cluster: Optional[str]=None) -> str:\n",
    "    try:\n",
    "        return client.export_users(entities, x_cluster=x_cluster)\n",
    "    except ApiError as e:\n",
    "        print(\"API error during export_keyrings:\", e)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4398175",
   "metadata": {},
   "source": [
    "\n",
    "## CephFS subvolume helpers\n",
    "Create/resize/delete subvolumes and check/get info. `mode` is used on **create**; `size` sets quota in bytes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818c8ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensure_group(vol_name: str, group_name: str, x_cluster: Optional[str]=None) -> Dict[str, Any]:\n",
    "    try:\n",
    "        return client.create_subvolume_group(vol_name, group_name, x_cluster=x_cluster)\n",
    "    except ApiError as e:\n",
    "        print(\"API error during ensure_group:\", e)\n",
    "        raise\n",
    "\n",
    "def create_or_resize_subvol(vol_name: str, subvol_name: str, *, group_name: Optional[str]=None,\n",
    "                            size_bytes: Optional[int]=None, mode: Optional[str]=None,\n",
    "                            x_cluster: Optional[str]=None) -> Dict[str, Any]:\n",
    "    try:\n",
    "        return client.create_or_resize_subvolume(vol_name, subvol_name, group_name=group_name,\n",
    "                                                 size=size_bytes, mode=mode, x_cluster=x_cluster)\n",
    "    except ApiError as e:\n",
    "        print(\"API error during create_or_resize_subvol:\", e)\n",
    "        raise\n",
    "\n",
    "def get_subvol_info(vol_name: str, subvol_name: str, *, group_name: Optional[str]=None,\n",
    "                    x_cluster: Optional[str]=None) -> Dict[str, Any]:\n",
    "    try:\n",
    "        return client.get_subvolume_info(vol_name, subvol_name, group_name=group_name, x_cluster=x_cluster)\n",
    "    except ApiError as e:\n",
    "        print(\"API error during get_subvol_info:\", e)\n",
    "        raise\n",
    "\n",
    "def subvol_exists(vol_name: str, subvol_name: str, *, group_name: Optional[str]=None,\n",
    "                  x_cluster: Optional[str]=None) -> bool:\n",
    "    try:\n",
    "        return client.subvolume_exists(vol_name, subvol_name, group_name=group_name, x_cluster=x_cluster)\n",
    "    except ApiError as e:\n",
    "        print(\"API error during subvol_exists:\", e)\n",
    "        raise\n",
    "\n",
    "def delete_subvol(vol_name: str, subvol_name: str, *, group_name: Optional[str]=None, force: bool=False,\n",
    "                  x_cluster: Optional[str]=None) -> Dict[str, Any]:\n",
    "    try:\n",
    "        return client.delete_subvolume(vol_name, subvol_name, group_name=group_name, force=force, x_cluster=x_cluster)\n",
    "    except ApiError as e:\n",
    "        print(\"API error during delete_subvol:\", e)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accc10c3",
   "metadata": {},
   "source": [
    "\n",
    "## One-at-a-time examples\n",
    "Uncomment and run what you need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc22791",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- CephX user: upsert\n",
    "# caps = [\n",
    "#     {\"entity\": \"mon\", \"cap\": \"allow r\"},\n",
    "#     {\"entity\": \"mds\", \"cap\": \"allow rw fsname=CEPH-FS-01 path=/volumes/_nogroup/demo\"},\n",
    "#     {\"entity\": \"osd\", \"cap\": \"allow rw tag cephfs data=CEPH-FS-01\"},\n",
    "#     {\"entity\": \"osd\", \"cap\": \"allow rw tag cephfs metadata=CEPH-FS-01\"},\n",
    "# ]\n",
    "# upsert_user(\"client.demo\", caps)\n",
    "\n",
    "# ---- CephX user: export keyring(s)\n",
    "# print(export_keyrings([\"client.demo\"]))\n",
    "\n",
    "# ---- CephX user: delete\n",
    "# delete_user(\"client.demo\")\n",
    "\n",
    "# ---- CephFS: ensure group, create, info, delete\n",
    "# ensure_group(\"CEPH-FS-01\", \"fabric_staff\")\n",
    "# create_or_resize_subvol(\"CEPH-FS-01\", \"alice\", group_name=\"fabric_staff\", size_bytes=10 * 1024**3, mode=\"0777\")\n",
    "# info = get_subvol_info(\"CEPH-FS-01\", \"alice\", group_name=\"fabric_staff\"); info\n",
    "# delete_subvol(\"CEPH-FS-01\", \"alice\", group_name=\"fabric_staff\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74040600",
   "metadata": {},
   "source": [
    "\n",
    "## Batch from CSV — CephX users\n",
    "\n",
    "**CSV format (`users.csv`)**\n",
    "\n",
    "| user_entity | capabilities_json |\n",
    "|-------------|-------------------|\n",
    "| client.demo | [{ \"entity\":\"mon\",\"cap\":\"allow r\" }, { \"entity\":\"mds\",\"cap\":\"allow rw\" }] |\n",
    "\n",
    "- `capabilities_json` is a JSON array of objects with `entity` and `cap` keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from caas_jupyter_tools import display_dataframe_to_user\n",
    "\n",
    "def process_users_csv(csv_path: str, *, action: str=\"upsert\", x_cluster: Optional[str]=None) -> pd.DataFrame:\n",
    "    \"\"\"action: 'upsert' or 'delete'\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    results = []\n",
    "    for _, row in df.iterrows():\n",
    "        entity = str(row[\"user_entity\"]).strip()\n",
    "        try:\n",
    "            if action == \"upsert\":\n",
    "                caps = json.loads(row[\"capabilities_json\"])\n",
    "                _ = upsert_user(entity, caps, x_cluster=x_cluster)\n",
    "                results.append({\"user_entity\": entity, \"action\": \"upsert\", \"status\": \"ok\"})\n",
    "            elif action == \"delete\":\n",
    "                _ = delete_user(entity, x_cluster=x_cluster)\n",
    "                results.append({\"user_entity\": entity, \"action\": \"delete\", \"status\": \"ok\"})\n",
    "            else:\n",
    "                results.append({\"user_entity\": entity, \"action\": action, \"status\": \"unsupported\"})\n",
    "        except ApiError as e:\n",
    "            results.append({\"user_entity\": entity, \"action\": action, \"status\": f\"error: {e.message}\"})\n",
    "    out = pd.DataFrame(results)\n",
    "    display_dataframe_to_user(\"users_batch_results\", out)\n",
    "    return out\n",
    "\n",
    "# Example usage:\n",
    "# process_users_csv(\"/path/to/users.csv\", action=\"upsert\")\n",
    "# process_users_csv(\"/path/to/users.csv\", action=\"delete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef222d",
   "metadata": {},
   "source": [
    "\n",
    "## Batch from CSV — CephFS subvolumes\n",
    "\n",
    "**CSV format (`subvols.csv`)**\n",
    "\n",
    "| vol_name   | subvol_name | group_name   | action  | size_bytes | mode  |\n",
    "|------------|-------------|--------------|---------|------------|-------|\n",
    "| CEPH-FS-01 | alice       | fabric_staff | create  | 10737418240| 0777  |\n",
    "| CEPH-FS-01 | alice       | fabric_staff | resize  | 536870912  |       |\n",
    "| CEPH-FS-01 | alice       | fabric_staff | delete  |            |       |\n",
    "\n",
    "- `action` is one of `create`, `resize`, `delete`\n",
    "- `size_bytes` is optional for `create` (omit → unlimited); required for `resize`\n",
    "- `mode` is only used on create\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301110d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_subvols_csv(csv_path: str, x_cluster: Optional[str]=None) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    results = []\n",
    "    for _, row in df.iterrows():\n",
    "        fs = str(row[\"vol_name\"]).strip()\n",
    "        name = str(row[\"subvol_name\"]).strip()\n",
    "        group = None if pd.isna(row.get(\"group_name\")) else str(row.get(\"group_name\")).strip()\n",
    "        action = str(row[\"action\"]).strip().lower()\n",
    "        size = row.get(\"size_bytes\")\n",
    "        size = int(size) if (pd.notna(size)) else None\n",
    "        mode = row.get(\"mode\")\n",
    "        mode = None if pd.isna(mode) or str(mode).strip()==\"\" else str(mode).strip()\n",
    "\n",
    "        try:\n",
    "            if action in (\"create\", \"resize\"):\n",
    "                if group:\n",
    "                    ensure_group(fs, group, x_cluster=x_cluster)\n",
    "                _ = create_or_resize_subvol(fs, name, group_name=group, size_bytes=size, mode=mode, x_cluster=x_cluster)\n",
    "                info = get_subvol_info(fs, name, group_name=group, x_cluster=x_cluster)\n",
    "                path = info.get(\"path\", \"\")\n",
    "                results.append({\"vol_name\": fs, \"subvol_name\": name, \"action\": action, \"status\": \"ok\", \"path\": path})\n",
    "            elif action == \"delete\":\n",
    "                _ = delete_subvol(fs, name, group_name=group, x_cluster=x_cluster)\n",
    "                results.append({\"vol_name\": fs, \"subvol_name\": name, \"action\": action, \"status\": \"ok\"})\n",
    "            else:\n",
    "                results.append({\"vol_name\": fs, \"subvol_name\": name, \"action\": action, \"status\": \"unsupported\"})\n",
    "        except ApiError as e:\n",
    "            results.append({\"vol_name\": fs, \"subvol_name\": name, \"action\": action, \"status\": f\"error: {e.message}\"})\n",
    "    out = pd.DataFrame(results)\n",
    "    from caas_jupyter_tools import display_dataframe_to_user\n",
    "    display_dataframe_to_user(\"subvols_batch_results\", out)\n",
    "    return out\n",
    "\n",
    "# Example usage:\n",
    "# process_subvols_csv(\"/path/to/subvols.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
