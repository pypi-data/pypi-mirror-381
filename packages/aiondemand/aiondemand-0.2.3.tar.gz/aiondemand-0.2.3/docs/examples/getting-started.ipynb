{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d91e3db7-0445-4bcc-9645-995c9eb6d18b",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "This notebook gives a minimal example on how to use the `aiod` package.\n",
    "For information on how to register new metadata in the catalogue, see our [Sharing](sharing.ipynb) example.\n",
    "\n",
    "üßë‚Äçüíª You can try this notebook interactively on [Google Colab](https://colab.research.google.com/github/aiondemand/aiondemand/blob/develop/docs/examples/getting-started.ipynb) or [Binder](https://mybinder.org/v2/gh/aiondemand/aiondemand/develop?urlpath=%2Fdoc%2Ftree%2Fdocs%2Fexamples%2Fgetting-started.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87fdd7e-52b9-4f17-8c0a-d2377ca6f448",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Install the package in a [virtual environment](https://docs.python.org/3/library/venv.html) with your favorite package manager, for example with pip or [uv](https://docs.astral.sh/uv/) by running:\n",
    "```bash\n",
    "python -m venv venv\n",
    "source venv/bin/activate  # on Windows: ./Scripts/activate.bat\n",
    "python -m pip install aiondemand\n",
    "```\n",
    "\n",
    "If you are running this notebook on Colab or Binder, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a4089f-3bf0-4f58-9258-318263cf92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aiondemand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b448fb07-9a73-4e42-8d7e-7895f5c459b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You installed version 0.1.0b1\n"
     ]
    }
   ],
   "source": [
    "import aiod\n",
    "print(f\"You installed version {aiod.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8e1b3a-2684-40f2-bd73-f988538bccaf",
   "metadata": {},
   "source": [
    "# Browsing Assets\n",
    "You can browse through all assets of a certain type by using the `get_list` function. You can choose whether you want the original JSON response or a formatted pandas dataframe (the default). You can even limit results by platform, e.g., metadata for assets on [Hugging Face](https://www.huggingface.co/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70168fb1-788d-4e29-9033-aa0e23f0d450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platform</th>\n",
       "      <th>platform_resource_identifier</th>\n",
       "      <th>name</th>\n",
       "      <th>date_published</th>\n",
       "      <th>same_as</th>\n",
       "      <th>is_accessible_for_free</th>\n",
       "      <th>ai_asset_identifier</th>\n",
       "      <th>ai_resource_identifier</th>\n",
       "      <th>aiod_entry</th>\n",
       "      <th>alternate_name</th>\n",
       "      <th>...</th>\n",
       "      <th>keyword</th>\n",
       "      <th>license</th>\n",
       "      <th>media</th>\n",
       "      <th>note</th>\n",
       "      <th>relevant_link</th>\n",
       "      <th>relevant_resource</th>\n",
       "      <th>relevant_to</th>\n",
       "      <th>research_area</th>\n",
       "      <th>scientific_domain</th>\n",
       "      <th>identifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huggingface</td>\n",
       "      <td>621ffdd236468d709f181d58</td>\n",
       "      <td>amirveyseh/acronym_identification</td>\n",
       "      <td>2022-03-02T23:29:22</td>\n",
       "      <td>https://huggingface.co/datasets/amirveyseh/acr...</td>\n",
       "      <td>True</td>\n",
       "      <td>data_rPQvKrL8cgXhtL4HEHijXSiC</td>\n",
       "      <td>data_rPQvKrL8cgXhtL4HEHijXSiC</td>\n",
       "      <td>{'editor': [], 'status': 'published', 'date_mo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[source_datasets:original, annotations_creator...</td>\n",
       "      <td>mit</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>data_rPQvKrL8cgXhtL4HEHijXSiC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>huggingface</td>\n",
       "      <td>621ffdd236468d709f181d59</td>\n",
       "      <td>ade-benchmark-corpus/ade_corpus_v2</td>\n",
       "      <td>2022-03-02T23:29:22</td>\n",
       "      <td>https://huggingface.co/datasets/ade-benchmark-...</td>\n",
       "      <td>True</td>\n",
       "      <td>data_rizpBPVK6WL8dn2owGXtsgox</td>\n",
       "      <td>data_rizpBPVK6WL8dn2owGXtsgox</td>\n",
       "      <td>{'editor': [], 'status': 'published', 'date_mo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[source_datasets:original, annotations_creator...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>data_rizpBPVK6WL8dn2owGXtsgox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huggingface</td>\n",
       "      <td>621ffdd236468d709f181d5a</td>\n",
       "      <td>UCLNLP/adversarial_qa</td>\n",
       "      <td>2022-03-02T23:29:22</td>\n",
       "      <td>https://huggingface.co/datasets/UCLNLP/adversa...</td>\n",
       "      <td>True</td>\n",
       "      <td>data_67H4UGoHxvoFnHz8i8XIsSg3</td>\n",
       "      <td>data_67H4UGoHxvoFnHz8i8XIsSg3</td>\n",
       "      <td>{'editor': [], 'status': 'published', 'date_mo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[source_datasets:original, language:en, size_c...</td>\n",
       "      <td>cc-by-sa-4.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>data_67H4UGoHxvoFnHz8i8XIsSg3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>huggingface</td>\n",
       "      <td>621ffdd236468d709f181d5b</td>\n",
       "      <td>Yale-LILY/aeslc</td>\n",
       "      <td>2022-03-02T23:29:22</td>\n",
       "      <td>https://huggingface.co/datasets/Yale-LILY/aeslc</td>\n",
       "      <td>True</td>\n",
       "      <td>data_RBdcDETh6MWYBGnCcTYIqI07</td>\n",
       "      <td>data_RBdcDETh6MWYBGnCcTYIqI07</td>\n",
       "      <td>{'editor': [], 'status': 'published', 'date_mo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[source_datasets:original, language:en, size_c...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>data_RBdcDETh6MWYBGnCcTYIqI07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>huggingface</td>\n",
       "      <td>621ffdd236468d709f181d5c</td>\n",
       "      <td>nwu-ctext/afrikaans_ner_corpus</td>\n",
       "      <td>2022-03-02T23:29:22</td>\n",
       "      <td>https://huggingface.co/datasets/nwu-ctext/afri...</td>\n",
       "      <td>True</td>\n",
       "      <td>data_YTetTfxkvKsPPpvA2fE93q2e</td>\n",
       "      <td>data_YTetTfxkvKsPPpvA2fE93q2e</td>\n",
       "      <td>{'editor': [], 'status': 'published', 'date_mo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[source_datasets:original, annotations_creator...</td>\n",
       "      <td>Other</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>data_YTetTfxkvKsPPpvA2fE93q2e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      platform platform_resource_identifier  \\\n",
       "0  huggingface     621ffdd236468d709f181d58   \n",
       "1  huggingface     621ffdd236468d709f181d59   \n",
       "2  huggingface     621ffdd236468d709f181d5a   \n",
       "3  huggingface     621ffdd236468d709f181d5b   \n",
       "4  huggingface     621ffdd236468d709f181d5c   \n",
       "\n",
       "                                 name       date_published  \\\n",
       "0   amirveyseh/acronym_identification  2022-03-02T23:29:22   \n",
       "1  ade-benchmark-corpus/ade_corpus_v2  2022-03-02T23:29:22   \n",
       "2               UCLNLP/adversarial_qa  2022-03-02T23:29:22   \n",
       "3                     Yale-LILY/aeslc  2022-03-02T23:29:22   \n",
       "4      nwu-ctext/afrikaans_ner_corpus  2022-03-02T23:29:22   \n",
       "\n",
       "                                             same_as  is_accessible_for_free  \\\n",
       "0  https://huggingface.co/datasets/amirveyseh/acr...                    True   \n",
       "1  https://huggingface.co/datasets/ade-benchmark-...                    True   \n",
       "2  https://huggingface.co/datasets/UCLNLP/adversa...                    True   \n",
       "3    https://huggingface.co/datasets/Yale-LILY/aeslc                    True   \n",
       "4  https://huggingface.co/datasets/nwu-ctext/afri...                    True   \n",
       "\n",
       "             ai_asset_identifier         ai_resource_identifier  \\\n",
       "0  data_rPQvKrL8cgXhtL4HEHijXSiC  data_rPQvKrL8cgXhtL4HEHijXSiC   \n",
       "1  data_rizpBPVK6WL8dn2owGXtsgox  data_rizpBPVK6WL8dn2owGXtsgox   \n",
       "2  data_67H4UGoHxvoFnHz8i8XIsSg3  data_67H4UGoHxvoFnHz8i8XIsSg3   \n",
       "3  data_RBdcDETh6MWYBGnCcTYIqI07  data_RBdcDETh6MWYBGnCcTYIqI07   \n",
       "4  data_YTetTfxkvKsPPpvA2fE93q2e  data_YTetTfxkvKsPPpvA2fE93q2e   \n",
       "\n",
       "                                          aiod_entry alternate_name  ...  \\\n",
       "0  {'editor': [], 'status': 'published', 'date_mo...             []  ...   \n",
       "1  {'editor': [], 'status': 'published', 'date_mo...             []  ...   \n",
       "2  {'editor': [], 'status': 'published', 'date_mo...             []  ...   \n",
       "3  {'editor': [], 'status': 'published', 'date_mo...             []  ...   \n",
       "4  {'editor': [], 'status': 'published', 'date_mo...             []  ...   \n",
       "\n",
       "                                             keyword       license media note  \\\n",
       "0  [source_datasets:original, annotations_creator...           mit    []   []   \n",
       "1  [source_datasets:original, annotations_creator...       Unknown    []   []   \n",
       "2  [source_datasets:original, language:en, size_c...  cc-by-sa-4.0    []   []   \n",
       "3  [source_datasets:original, language:en, size_c...       Unknown    []   []   \n",
       "4  [source_datasets:original, annotations_creator...         Other    []   []   \n",
       "\n",
       "  relevant_link relevant_resource relevant_to research_area scientific_domain  \\\n",
       "0            []                []          []            []                []   \n",
       "1            []                []          []            []                []   \n",
       "2            []                []          []            []                []   \n",
       "3            []                []          []            []                []   \n",
       "4            []                []          []            []                []   \n",
       "\n",
       "                      identifier  \n",
       "0  data_rPQvKrL8cgXhtL4HEHijXSiC  \n",
       "1  data_rizpBPVK6WL8dn2owGXtsgox  \n",
       "2  data_67H4UGoHxvoFnHz8i8XIsSg3  \n",
       "3  data_RBdcDETh6MWYBGnCcTYIqI07  \n",
       "4  data_YTetTfxkvKsPPpvA2fE93q2e  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = aiod.datasets.get_list(limit=25, platform=\"huggingface\")\n",
    "datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714672c8-1078-44b3-9af6-763d23794e6c",
   "metadata": {},
   "source": [
    "You can also use search queries to search through the registered assets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fced447b-14ea-4730-9adb-00a5f1373dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>platform</th>\n",
       "      <th>platform_resource_identifier</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_1wbB6zdEFY1R7I7h8bZ4e3JN</td>\n",
       "      <td>zenodo</td>\n",
       "      <td>zenodo.org:7115068</td>\n",
       "      <td>National Forest Cover of Indonesia Dataset by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_5uFWrj7MULGFUi4F2Qzje4Po</td>\n",
       "      <td>zenodo</td>\n",
       "      <td>zenodo.org:10510543</td>\n",
       "      <td>Linked collectors and determiners for: US Fore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data_BgtQPmf16SzYqYFY2XZDOIKj</td>\n",
       "      <td>zenodo</td>\n",
       "      <td>zenodo.org:11056979</td>\n",
       "      <td>Linked collectors and determiners for: US Fore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data_HlxWk9XDSfypDdWhwa7rxDCM</td>\n",
       "      <td>zenodo</td>\n",
       "      <td>zenodo.org:11231199</td>\n",
       "      <td>Linked collectors and determiners for: US Fore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data_RWJ17ZTeoANOpxIb7ndm1ljO</td>\n",
       "      <td>zenodo</td>\n",
       "      <td>zenodo.org:10714007</td>\n",
       "      <td>Linked collectors and determiners for: US Fore...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      identifier platform platform_resource_identifier  \\\n",
       "0  data_1wbB6zdEFY1R7I7h8bZ4e3JN   zenodo           zenodo.org:7115068   \n",
       "1  data_5uFWrj7MULGFUi4F2Qzje4Po   zenodo          zenodo.org:10510543   \n",
       "2  data_BgtQPmf16SzYqYFY2XZDOIKj   zenodo          zenodo.org:11056979   \n",
       "3  data_HlxWk9XDSfypDdWhwa7rxDCM   zenodo          zenodo.org:11231199   \n",
       "4  data_RWJ17ZTeoANOpxIb7ndm1ljO   zenodo          zenodo.org:10714007   \n",
       "\n",
       "                                                name  \n",
       "0  National Forest Cover of Indonesia Dataset by ...  \n",
       "1  Linked collectors and determiners for: US Fore...  \n",
       "2  Linked collectors and determiners for: US Fore...  \n",
       "3  Linked collectors and determiners for: US Fore...  \n",
       "4  Linked collectors and determiners for: US Fore...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_datasets = aiod.datasets.search(\"National Forest\")\n",
    "forest_datasets[[\"identifier\", \"platform\", \"platform_resource_identifier\", \"name\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d40783-8eb8-433c-84d7-c3d056260fcd",
   "metadata": {},
   "source": [
    "Each asset has a unique identifier on AI-on-Demand. It is always a prefix (of 3 or 4 letters), followed by an alphanumeric string, separated by an underscore. For example, \"data_HlxWk9XDSfypDdWhwa7rxDCM\" is an AI-on-Demand identifier. For assets which are imported from other platforms, such as [Zenodo](https://www.zenodo.org/), the identifier under which the asset is known with the original platform is also stored as the \"platform resource identifier\", for example \"zenodo.org:11231199\". These can also be used to fetch the asset from AI-on-Demand, which is especially useful if you only know how the asset can be found on the original platform!\n",
    "\n",
    "We can use those identifiers to request metadata for the asset directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9ff33b0-3ada-40bf-a999-9ad032f173b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "platform                                                                   zenodo\n",
       "platform_resource_identifier                                  zenodo.org:11231199\n",
       "name                            Linked collectors and determiners for: US Fore...\n",
       "date_published                                                2024-05-21T00:00:00\n",
       "same_as                                   https://zenodo.org/api/records/11231199\n",
       "ai_asset_identifier                                 data_HlxWk9XDSfypDdWhwa7rxDCM\n",
       "ai_resource_identifier                              data_HlxWk9XDSfypDdWhwa7rxDCM\n",
       "aiod_entry                      {'editor': [], 'status': 'published', 'date_mo...\n",
       "alternate_name                                                                 []\n",
       "application_area                                                               []\n",
       "citation                                                                       []\n",
       "contact                                                                        []\n",
       "contacts                                                                       []\n",
       "creator                                                                        []\n",
       "description                     {'plain': 'Natural history specimen data linke...\n",
       "distribution                    [{'checksum': '32683ecc1cc35315d8b282eecb19844...\n",
       "funder                                                                         []\n",
       "has_part                                                                       []\n",
       "industrial_sector                                                              []\n",
       "is_part_of                                                                     []\n",
       "keyword                                     [natural history, taxonomy, specimen]\n",
       "license                                      creative commons zero v1.0 universal\n",
       "media                                                                          []\n",
       "note                                                                           []\n",
       "relevant_link                                                                  []\n",
       "relevant_resource                                                              []\n",
       "relevant_to                                                                    []\n",
       "research_area                                                                  []\n",
       "scientific_domain                                                              []\n",
       "identifier                                          data_HlxWk9XDSfypDdWhwa7rxDCM\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiod.datasets.get_asset(identifier=\"data_HlxWk9XDSfypDdWhwa7rxDCM\")\n",
    "# alternatively, the call below gets the same asset:\n",
    "# aiod.datasets.get_asset_from_platform(platform=\"zenodo\", platform_identifier=\"zenodo.org:11231199\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a7f6a-9b1e-4941-a4ba-d05a118cf961",
   "metadata": {},
   "source": [
    "For datasets, we can even fetch the underlying data files themselves in a unified way.\n",
    "However, loading them requires some additional code since there is no universal data loader.\n",
    "Let's take a dataset from [OpenML](https://www.openml.org):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05da4a7c-de99-4428-ad48-6c98a8876e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = aiod.datasets.get_asset_from_platform(platform_identifier=\"300\", platform=\"openml\")\n",
    "raw_data = aiod.datasets.get_content(identifier=dataset.identifier).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e028522-6080-4a30-8385-3729037fbfe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['% 1. Title of Database: ISOLET (Isolated Letter Speech Recognition)',\n",
       " '%',\n",
       " '% 2. Sources:',\n",
       " '%   (a) Creators: Ron Cole and Mark Fanty',\n",
       " '%       Department of Computer Science and Engineering,']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.splitlines()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc49530-bffa-4857-a711-93b5cae0e55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e39b91d1-66c1-4f5d-96c3-b27a9828caf4",
   "metadata": {},
   "source": [
    "The downloaded dataset from OpenML is an [ARFF](https://waikato.github.io/weka-wiki/formats_and_processing/arff/) file, above you can see part of the header and part of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2d9379-7041-4b92-b1e1-5898b596a30f",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "In the last example, we will show you how to load the dataset above into a pandas dataframe and use it to train a model with scikit-learn. These packages are not included with `aiondemand` by default, so they need to be installed separately. Alternatively, you could use the [openml](https://pypi.org/project/openml/) package directly after finding the dataset's platform resource identifier on AI-on-Demand (and similar for other platforms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cbc40f0-c99e-485a-96e8-0b910d029a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-macosx_12_0_arm64.whl (8.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.22.0 in /Users/pietergijsbers/repositories/aiod-py-sdk/jvenv/lib/python3.11/site-packages (from scikit-learn) (2.3.2)\n",
      "Collecting scipy>=1.8.0\n",
      "  Downloading scipy-1.16.2-cp311-cp311-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m308.4/308.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.7.2 scipy-1.16.2 threadpoolctl-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting liac-arff\n",
      "  Using cached liac-arff-2.5.0.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: liac-arff\n",
      "\u001b[33m  DEPRECATION: liac-arff is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py install for liac-arff ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed liac-arff-2.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install liac-arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b200552-69ba-4a8b-98d2-824e13fd1419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f609</th>\n",
       "      <th>f610</th>\n",
       "      <th>f611</th>\n",
       "      <th>f612</th>\n",
       "      <th>f613</th>\n",
       "      <th>f614</th>\n",
       "      <th>f615</th>\n",
       "      <th>f616</th>\n",
       "      <th>f617</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.4394</td>\n",
       "      <td>-0.0930</td>\n",
       "      <td>0.1718</td>\n",
       "      <td>0.4620</td>\n",
       "      <td>0.6226</td>\n",
       "      <td>0.4704</td>\n",
       "      <td>0.3578</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>-0.1184</td>\n",
       "      <td>-0.2310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4102</td>\n",
       "      <td>0.2052</td>\n",
       "      <td>0.3846</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>0.5898</td>\n",
       "      <td>0.3334</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.5898</td>\n",
       "      <td>-0.4872</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.4348</td>\n",
       "      <td>-0.1198</td>\n",
       "      <td>0.2474</td>\n",
       "      <td>0.4036</td>\n",
       "      <td>0.5026</td>\n",
       "      <td>0.6328</td>\n",
       "      <td>0.4948</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>-0.0520</td>\n",
       "      <td>-0.1302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2954</td>\n",
       "      <td>0.2046</td>\n",
       "      <td>0.4772</td>\n",
       "      <td>0.0454</td>\n",
       "      <td>0.2046</td>\n",
       "      <td>0.4318</td>\n",
       "      <td>0.4546</td>\n",
       "      <td>-0.0910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.2330</td>\n",
       "      <td>0.2124</td>\n",
       "      <td>0.5014</td>\n",
       "      <td>0.5222</td>\n",
       "      <td>-0.3422</td>\n",
       "      <td>-0.5840</td>\n",
       "      <td>-0.7168</td>\n",
       "      <td>-0.6342</td>\n",
       "      <td>-0.8614</td>\n",
       "      <td>-0.8318</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.0476</td>\n",
       "      <td>-0.1746</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>-0.0476</td>\n",
       "      <td>0.1112</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.1588</td>\n",
       "      <td>-0.4762</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.3808</td>\n",
       "      <td>-0.0096</td>\n",
       "      <td>0.2602</td>\n",
       "      <td>0.2554</td>\n",
       "      <td>-0.4290</td>\n",
       "      <td>-0.6746</td>\n",
       "      <td>-0.6868</td>\n",
       "      <td>-0.6650</td>\n",
       "      <td>-0.8410</td>\n",
       "      <td>-0.9614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0504</td>\n",
       "      <td>-0.0360</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>0.1366</td>\n",
       "      <td>0.2950</td>\n",
       "      <td>0.0792</td>\n",
       "      <td>-0.0072</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>-0.1510</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.3412</td>\n",
       "      <td>0.0946</td>\n",
       "      <td>0.6082</td>\n",
       "      <td>0.6216</td>\n",
       "      <td>-0.1622</td>\n",
       "      <td>-0.3784</td>\n",
       "      <td>-0.4324</td>\n",
       "      <td>-0.4358</td>\n",
       "      <td>-0.4966</td>\n",
       "      <td>-0.5406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>0.3124</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>-0.0938</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>0.3124</td>\n",
       "      <td>0.3124</td>\n",
       "      <td>0.2188</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 618 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       f1      f2      f3      f4      f5      f6      f7      f8      f9  \\\n",
       "0 -0.4394 -0.0930  0.1718  0.4620  0.6226  0.4704  0.3578  0.0478 -0.1184   \n",
       "1 -0.4348 -0.1198  0.2474  0.4036  0.5026  0.6328  0.4948  0.0338 -0.0520   \n",
       "2 -0.2330  0.2124  0.5014  0.5222 -0.3422 -0.5840 -0.7168 -0.6342 -0.8614   \n",
       "3 -0.3808 -0.0096  0.2602  0.2554 -0.4290 -0.6746 -0.6868 -0.6650 -0.8410   \n",
       "4 -0.3412  0.0946  0.6082  0.6216 -0.1622 -0.3784 -0.4324 -0.4358 -0.4966   \n",
       "\n",
       "      f10  ...    f609    f610    f611    f612    f613    f614    f615  \\\n",
       "0 -0.2310  ...  0.4102  0.2052  0.3846  0.3590  0.5898  0.3334  0.6410   \n",
       "1 -0.1302  ...  0.0000  0.2954  0.2046  0.4772  0.0454  0.2046  0.4318   \n",
       "2 -0.8318  ... -0.1112 -0.0476 -0.1746  0.0318 -0.0476  0.1112  0.2540   \n",
       "3 -0.9614  ... -0.0504 -0.0360 -0.1224  0.1366  0.2950  0.0792 -0.0072   \n",
       "4 -0.5406  ...  0.1562  0.3124  0.2500 -0.0938  0.1562  0.3124  0.3124   \n",
       "\n",
       "     f616    f617  class  \n",
       "0  0.5898 -0.4872      1  \n",
       "1  0.4546 -0.0910      1  \n",
       "2  0.1588 -0.4762      2  \n",
       "3  0.0936 -0.1510      2  \n",
       "4  0.2188 -0.2500      3  \n",
       "\n",
       "[5 rows x 618 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import arff\n",
    "import pandas as pd\n",
    "parsed_data = arff.load(raw_data)\n",
    "columns = [attr[0] for attr in parsed_data['attributes']]\n",
    "df = pd.DataFrame(parsed_data[\"data\"], columns=columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff0df4-e5f1-451b-9b3e-463a2c20262f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e127a8b-456c-4d63-867e-3ff9fea189c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.932012432012432\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Model accuracy:\",clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb206ce-6417-4ef0-abf4-36d2e68bf770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
