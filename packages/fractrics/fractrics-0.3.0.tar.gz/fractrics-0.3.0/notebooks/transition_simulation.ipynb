{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25cbc884",
   "metadata": {},
   "source": [
    "## The forward algorithm\n",
    "\n",
    "This section explains the factorized transition used for the MSM model, and provides a simulation of what happens in the actual implementation.\n",
    "\n",
    "For a Hidden Markov Model (HMM) let:  \n",
    "  - $\\Pi_t$ be the joint distribution of latent components $M_t$.  \n",
    "  - $f(x_t | M_t)$ be the emission/data likelihood of the model.  \n",
    "  - $A$: the transition matrix.  \n",
    "\n",
    "Then each step of the forward algorithm is of the form:  \n",
    "$$\n",
    "\\Pi_{t+1}=\\frac{f(x_t | M_t)\\odot \\Pi_t A}{f(x_t | M_t)* \\Pi_t A}\n",
    "$$  \n",
    "\n",
    "where $\\odot$ is the Hadamard product, and $*$ is the inner product. For notational convenience we also introduce the predictive distribution $Q_{t+1}= \\Pi_t A$, so that the step equation becomes:\n",
    "\n",
    "$$\n",
    "\\Pi_{t+1}=\\frac{f(x_t | M_t)\\odot Q_{t+1}}{f(x_t | M_t)* Q_{t+1}}\n",
    "$$\n",
    "\n",
    "Below follows an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "045c4724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06666667 0.13333334 0.20000002 0.26666668 0.33333334]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "pi = jnp.full(5, 0.2)\n",
    "f = jnp.arange(1, 6)/10\n",
    "A = jnp.full([5, 5], 0.2)\n",
    "Q = jnp.dot(pi, A)\n",
    "num = f * Q\n",
    "pi_tp1 = num/jnp.sum(num)\n",
    "\n",
    "print(pi_tp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a46d91",
   "metadata": {},
   "source": [
    "When a lot of steps are involved, multiplications of probabilities tend to underflow, therefore it is convenient to work in the log-space instead. Recall that:\n",
    "\n",
    "  - for $\\vec{c} = \\vec{a} \\odot \\vec{b} \\implies \\log \\vec{c} = \\log \\vec{a} + \\log \\vec{b}$.  \n",
    "  - for $d=\\vec{a}*\\vec{b} \\implies \\log d = \\log \\left( \\sum_{i=1}^n \\exp (\\log a_i + \\log b_i) \\right)=\\text{logsumexp}(\\log \\vec{a} + \\log \\vec{b})=\\text{logsumexp}(\\vec{c})$\n",
    "  - for $y=\\vec{v}\\mathbf M \\implies \\log y = \\log \\sum_{i=1}^n \\exp (\\log v_i+\\log \\mathbf M_{i,:})$\n",
    "\n",
    "Then the forward update step can be made in the log. space:\n",
    "\n",
    "$$\n",
    "\\log \\Pi_{t+1} = \\log f(x_t | M_t) + \\log \\sum_{i=1}^n \\exp (\\log \\Pi_{t,i}+\\log A_{i,:}) - \\text{logsumexp} \\left( \\log f(x_t | M_t) + \\log \\sum_{i=1}^n \\exp (\\log \\Pi_{t,i}+\\log A_{i,:}) \\right)\n",
    "$$\n",
    "\n",
    "Or equivalently, using the predictive distribution notation: $\\log Q_{t+1}=\\log \\sum_{i=1}^n \\exp (\\log \\Pi_{t,i}+\\log A_{i,:})$\n",
    "  \n",
    "$$\n",
    "\\log \\Pi_{t+1} = \\log f(x_t | M_t) + \\log Q_{t+1} - \\text{logsumexp} \\left( \\log f(x_t | M_t) + \\log Q_{t+1} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfabbf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06666666 0.13333333 0.2        0.26666665 0.3333333 ]\n"
     ]
    }
   ],
   "source": [
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "log_f = jnp.log(f)\n",
    "log_pi = jnp.log(pi)\n",
    "log_A = jnp.log(A)\n",
    "\n",
    "def joint_predictive(log_pi, log_A):\n",
    "    return logsumexp(log_pi[:, None] + log_A, axis=0)\n",
    "\n",
    "log_Q = joint_predictive(pi, A)\n",
    "\n",
    "#consistency check\n",
    "if log_Q.all() != jnp.log(Q).all(): print(\"Values are different\")\n",
    "\n",
    "# Note the None index adds 1 dimension/column during broadcasting\n",
    "num = log_f + log_Q\n",
    "log_pi_tp1 = num - logsumexp(num)\n",
    "\n",
    "# besides small approximation errors, the resulting distribution is the same as in the base space\n",
    "print(jnp.exp(log_pi_tp1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6941fc",
   "metadata": {},
   "source": [
    "As shown in the example, the 2 algorithms produce the same result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84259bab",
   "metadata": {},
   "source": [
    "## A simplified case: factorized latent transitions.\n",
    "\n",
    "The computations in the previous section have the drawback that scale exponentially with the number of latent variables $M_i$. In particular, consider the homogenous case in which there are $K$ latent variables which can assume the same $j$ possible values. Then at each forward step, $\\Pi_t$ is a distribution vector of $j^k$ elements that is updated by multiplying it with a $(j^K \\times j^K)$ transition matrix $A$, thus the total complexity at each step is $\\mathcal O(j^{2K})$.\n",
    "\n",
    "The complexity can be reduced to $\\mathcal O(Kj^{K+1})$ for HMMs in which the latent variables evolve independently. This reduction is possible because $A$ can be represented as the kronecker product of $K$ marginal transition matrices $A^{(k)}= P(M_{k,t} | M_{k, t-1})$:  \n",
    "$$A = P(M_t | M_{t-1})=\\otimes_{k=0}^K A^{(k)}$$\n",
    "  where $\\otimes$ represents the sequentially applied kronecker products.  \n",
    "\n",
    "It is thus possible to make the forward transitions independently, and aggregate them toghether only for the emission adjustment. This constructin of the transition tensor corresponds to a tucker tensor decomposition with a trivial core tensor composed only by 1s.\n",
    "\n",
    "Let $\\tilde \\Pi_{t}=\\text{reshape}(\\Pi_t) \\in \\mathbb R^{\\otimes_K j}$ be the joint distribution at time $t$ reshaped as a tensor of $K$ dimensions, each of length $j$. Then the transition can be done using:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{apply\\_transitions}\\bigl(\\tilde \\Pi_{t},\\{A^{(k)}\\}\\bigr)\n",
    "&=\n",
    "\\underbrace{\\Bigl(\\bigl(\\bigl(\\tilde \\Pi_{t}\n",
    "   \\times_{0}(A^{(0)})^{\\!\\top}\\bigr)\n",
    "   \\times_{1}(A^{(1)})^{\\!\\top}\\bigr)\n",
    "   \\times_{2}\\cdots\\Bigr)}_{K\\ \\text{modeâ€‘products}} \n",
    "   \\times_{K-1}(A^{(K-1)})^{\\!\\top}, \\\\[6pt]\n",
    "\\end{align*}\n",
    "\n",
    "Where for a tensor $\\mathcal X$ of order $K$ and a transition matrix $A \\in \\mathbb R^{J_k \\times I_k}$ the mode-$k$ product $\\times_k$ is defined by:\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "(\\mathcal X\\times_k A)_{i_0\\cdots i_{i-1}\\,j\\,i_{k+1}\\cdots i_{K-1}}\n",
    "&=\n",
    "\\sum_{i_k=1}^{I_K}\n",
    "A_{\\,j\\,i_k}\\;\n",
    "\\mathcal X_{i_0\\,i_1\\,\\dots\\,i_{K-1}}.\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "where $I_k$ are the possible values that the $k$-th latent state can take.\n",
    "\n",
    "What follows are different implementation of this factorized algoirhm. From tests, an implementation using only JAX constructs proves challenging, due to tracing, therefore the implementation currently relies on python loops. The first one works with normalized probabilities, while the other 2 versions use log-probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324cf58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match in log-space: True\n"
     ]
    }
   ],
   "source": [
    "def make_apply_scaled(*As):\n",
    "    def apply(p):\n",
    "        # p:    shape (S, S, ..., S), sums to 1\n",
    "        # emissions: same shape, non-negative\n",
    "        for axis, A in enumerate(As):\n",
    "            # static moveaxis / tensordot / moveaxis\n",
    "            p = jnp.moveaxis(p, axis, -1)\n",
    "            p = jnp.tensordot(p, A, axes=([-1], [0]))\n",
    "            p = jnp.moveaxis(p, -1, axis)\n",
    "\n",
    "        return p\n",
    "\n",
    "    return apply\n",
    "\n",
    "# A0 = jnp.array([[0.7, 0.3], [0.4, 0.6]])\n",
    "# A1 = jnp.array([[0.1, 0.6, 0.3], [0.3, 0.4, 0.3], [0.5, 0.1, 0.4]])\n",
    "# A2 = jnp.array([[0.9, 0.1], [0.2, 0.8]])\n",
    "\n",
    "A0 = jnp.array([[0.7, 0.3], [0.4, 0.6]])\n",
    "A1 = jnp.array([[0.1, 0.9], [0.6, 0.4]])\n",
    "A2 = jnp.array([[0.9, 0.1], [0.2, 0.8]])\n",
    "\n",
    "\n",
    "# prior = jnp.array([\n",
    "#     0.02, 0.03, 0.05, 0.10, 0.07, 0.08,\n",
    "#     0.04, 0.06, 0.12, 0.13, 0.10, 0.20\n",
    "# ])\n",
    "\n",
    "prior = jnp.array([\n",
    "    0.06, 0.09, 0.17, 0.23, 0.07, 0.08, 0.10, 0.20\n",
    "])\n",
    "\n",
    "A_tensor = (A0, A1, A2)\n",
    "apply_fn = make_apply_scaled(*A_tensor)\n",
    "\n",
    "dims = tuple(A.shape[0] for A in A_tensor)\n",
    "#reshape(n_latent * [len(marg_prob)])\n",
    "\n",
    "\n",
    "prior_tensor = prior.reshape(*dims)\n",
    "pred_tensor = apply_fn(prior_tensor)\n",
    "\n",
    "pred_fast = pred_tensor.reshape(-1)\n",
    "\n",
    "log_pred_fast = jnp.log(pred_fast)\n",
    "\n",
    "\n",
    "# Full-joint predictive and its log for reference\n",
    "A_joint = jnp.kron(jnp.kron(A0, A1), A2)\n",
    "pred_full = prior @ A_joint\n",
    "log_pred_full = jnp.log(pred_full)\n",
    "\n",
    "\n",
    "# print(\"Log full-joint predictive:\", log_pred_full)\n",
    "# print(\"Log fast-space predictive:\", log_pred_fast)\n",
    "print(\"Match in log-space:\", jnp.allclose(log_pred_full, log_pred_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e462a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "def apply_transitions(prior_tensor, transition_matrices):\n",
    "    result = prior_tensor\n",
    "    for axis, A in enumerate(transition_matrices):\n",
    "        # Move axis to front\n",
    "        result = jnp.moveaxis(result, axis, 0)\n",
    "        # Apply transition matrix along this axis\n",
    "        result = jnp.tensordot(A.T, result, axes=1)\n",
    "        # Move axis back to original position\n",
    "        result = jnp.moveaxis(result, 0, axis)\n",
    "    return result\n",
    "\n",
    "# Transition matrices for 3 latent variables with different number of states\n",
    "A0 = jnp.array([[0.7, 0.3],\n",
    "                [0.4, 0.6]])\n",
    "A1 = jnp.array([[0.1, 0.6, 0.3],\n",
    "                [0.3, 0.4, 0.3],\n",
    "                [0.5, 0.1, 0.4]])\n",
    "A2 = jnp.array([[0.9, 0.1],\n",
    "                [0.2, 0.8]])\n",
    "\n",
    "# Prior over joint states (2 x 3 x 2 = 12 states)\n",
    "prior = jnp.array([\n",
    "    0.02, 0.03, 0.05, 0.10, 0.07, 0.08,\n",
    "    0.04, 0.06, 0.12, 0.13, 0.10, 0.20\n",
    "])\n",
    "prior_tensor = prior.reshape(2, 3, 2)\n",
    "\n",
    "# Apply transitions (using the function you already have)\n",
    "predictive_tensor = apply_transitions(prior_tensor, [A0, A1, A2])\n",
    "predictive_fast = predictive_tensor.reshape(-1)\n",
    "\n",
    "# Full joint transition via kron product for reference\n",
    "A_joint = jnp.kron(jnp.kron(A0, A1), A2)\n",
    "predictive_full = prior @ A_joint\n",
    "\n",
    "print(\"Match:\", jnp.allclose(predictive_full, predictive_fast))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f0dcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match in log-space: True\n"
     ]
    }
   ],
   "source": [
    "def apply_transitions(prior_log_tensor, log_transition_matrices):\n",
    "    \"\"\"\n",
    "    Applies each axis-wise transition in log-space using an optimized broadcasted log-sum-exp.\n",
    "\n",
    "    prior_log_tensor: log-prob tensor of shape (d0, d1, ..., dK)\n",
    "    log_transition_matrices: list of K matrices, where\n",
    "      log_transition_matrices[k] is (d_k, d_k').\n",
    "    \"\"\"\n",
    "    result = prior_log_tensor\n",
    "    for axis, logA in enumerate(log_transition_matrices):\n",
    "        # 1) Move the k-th latent axis to the last position\n",
    "        r = jnp.moveaxis(result, axis, -1)  # shape (..., old_dim)\n",
    "\n",
    "        # 2) Broadcast-add logA: r[..., :, None] has shape (..., old_dim, 1)\n",
    "        #    logA[None, ...] has shape (1, old_dim, new_dim)\n",
    "        #    result t has shape (..., old_dim, new_dim)\n",
    "        t = r[..., :, None] + logA[None, :, :]\n",
    "\n",
    "        # 3) log-sum-exp over the old state dimension (axis -2)\n",
    "        s = logsumexp(t, axis=-2)  # shape (..., new_dim)\n",
    "\n",
    "        # 4) Move the new state axis back to its original position\n",
    "        result = jnp.moveaxis(s, -1, axis)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Example usage to verify equivalence\n",
    "if __name__ == '__main__':\n",
    "    # Factorized transition mats\n",
    "    A0 = jnp.array([[0.7, 0.3],\n",
    "                    [0.4, 0.6]])\n",
    "    A1 = jnp.array([[0.1, 0.6, 0.3],\n",
    "                    [0.3, 0.4, 0.3],\n",
    "                    [0.5, 0.1, 0.4]])\n",
    "    A2 = jnp.array([[0.9, 0.1],\n",
    "                    [0.2, 0.8]])\n",
    "    logA0, logA1, logA2 = jnp.log(A0), jnp.log(A1), jnp.log(A2)\n",
    "\n",
    "    # Prior over 2Ã—3Ã—2 joint grid\n",
    "    prior = jnp.array([\n",
    "        0.02, 0.03, 0.05, 0.10, 0.07, 0.08,\n",
    "        0.04, 0.06, 0.12, 0.13, 0.10, 0.20\n",
    "    ])\n",
    "    prior_tensor = prior.reshape(2, 3, 2)\n",
    "    log_prior_tensor = jnp.log(prior_tensor)\n",
    "\n",
    "    # Fast log-space predictive\n",
    "    log_pred_tensor = apply_transitions(log_prior_tensor, [logA0, logA1, logA2])\n",
    "    log_pred_fast = log_pred_tensor.reshape(-1)\n",
    "\n",
    "    # Full-joint predictive and its log for reference\n",
    "    A_joint = jnp.kron(jnp.kron(A0, A1), A2)\n",
    "    pred_full = prior @ A_joint\n",
    "    log_pred_full = jnp.log(pred_full)\n",
    "\n",
    "    # Compare results\n",
    "    print(\"Match in log-space:\", jnp.allclose(log_pred_full, log_pred_fast))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de2040e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match in log-space: True\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "# Factory to create a JIT-ed function with transitions embedded in closure\n",
    "def make_apply_transitions(*log_transition_matrices):\n",
    "    \"\"\"\n",
    "    Returns a JIT-ed function with log_transition_matrices baked into the closure.\n",
    "    This avoids static_argnums hashing issues and is fully compatible with JAX.\n",
    "    \"\"\"\n",
    "    @jax.jit\n",
    "    def apply(prior_log_tensor):\n",
    "        result = prior_log_tensor\n",
    "        for axis, logA in enumerate(log_transition_matrices):\n",
    "            r = jnp.moveaxis(result, axis, -1)\n",
    "            t = r[..., :, None] + logA[None, :, :]\n",
    "            s = logsumexp(t, axis=-2)\n",
    "            result = jnp.moveaxis(s, -1, axis)\n",
    "        return result\n",
    "    return apply\n",
    "\n",
    "# Example usage with full test:\n",
    "if __name__ == '__main__':\n",
    "    A0 = jnp.array([[0.7, 0.3], [0.4, 0.6]])\n",
    "    A1 = jnp.array([[0.1, 0.6, 0.3], [0.3, 0.4, 0.3], [0.5, 0.1, 0.4]])\n",
    "    A2 = jnp.array([[0.9, 0.1], [0.2, 0.8]])\n",
    "    logA0, logA1, logA2 = jnp.log(A0), jnp.log(A1), jnp.log(A2)\n",
    "\n",
    "    # Create the apply function with static mats\n",
    "    apply_transitions = make_apply_transitions(logA0, logA1, logA2)\n",
    "\n",
    "    # Prior over 2Ã—3Ã—2 joint grid\n",
    "    prior = jnp.array([\n",
    "        0.02, 0.03, 0.05, 0.10, 0.07, 0.08,\n",
    "        0.04, 0.06, 0.12, 0.13, 0.10, 0.20\n",
    "    ])\n",
    "    prior_tensor = prior.reshape(2, 3, 2)\n",
    "    log_prior_tensor = jnp.log(prior_tensor)\n",
    "\n",
    "    # Fast log-space predictive\n",
    "    log_pred_tensor = apply_transitions(log_prior_tensor)\n",
    "    log_pred_fast = log_pred_tensor.reshape(-1)\n",
    "\n",
    "    # Full-joint predictive and its log for reference\n",
    "    A_joint = jnp.kron(jnp.kron(A0, A1), A2)\n",
    "    pred_full = prior @ A_joint\n",
    "    log_pred_full = jnp.log(pred_full)\n",
    "\n",
    "    print(\"Match in log-space:\", jnp.allclose(log_pred_full, log_pred_fast))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a690c8",
   "metadata": {},
   "source": [
    "Trying to avoid python loops entirely creates tracing problems. The issue is that JAX loop operators work with traced indexes, which cannot be put inside functions that require static arguements. Below, an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3a0af8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot create weak reference to 'staticmethod' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mmoveaxis(predictive_tensor, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, axis)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fori_loop(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(transition_matrices), step, prior_tensor)\n\u001b[0;32m---> 24\u001b[0m pr_fn \u001b[38;5;241m=\u001b[39m make_predictive(prior_reshaped, A0, A1, A2)\n",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m, in \u001b[0;36mmake_predictive\u001b[0;34m(prior_tensor, *transition_matrices)\u001b[0m\n\u001b[1;32m     19\u001b[0m     predictive_tensor \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mtensordot(predictive_tensor, factor_transition_matrix, axes\u001b[38;5;241m=\u001b[39m([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mmoveaxis(predictive_tensor, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, axis)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fori_loop(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(transition_matrices), step, prior_tensor)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/usami/lib/python3.11/site-packages/jax/_src/lax/control_flow/loops.py:2058\u001b[0m, in \u001b[0;36mfori_loop\u001b[0;34m(lower, upper, body_fun, init_val, unroll)\u001b[0m\n\u001b[1;32m   2053\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mdisable_jit\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mand\u001b[39;00m length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2054\u001b[0m     \u001b[38;5;66;03m# non-jit implementation of scan does not support length=0\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m init_val\n\u001b[1;32m   2057\u001b[0m   (_, result), _ \u001b[38;5;241m=\u001b[39m scan(\n\u001b[0;32m-> 2058\u001b[0m       _fori_scan_body_fun(body_fun),\n\u001b[1;32m   2059\u001b[0m       (lower_, init_val),\n\u001b[1;32m   2060\u001b[0m       \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2061\u001b[0m       length\u001b[38;5;241m=\u001b[39mlength,\n\u001b[1;32m   2062\u001b[0m       unroll\u001b[38;5;241m=\u001b[39munroll,\n\u001b[1;32m   2063\u001b[0m   )\n\u001b[1;32m   2064\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   2065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unroll \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot create weak reference to 'staticmethod' object"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax.lax import fori_loop\n",
    "\n",
    "A0 = jnp.array([[0.7, 0.3], [0.4, 0.6]])\n",
    "A1 = jnp.array([[0.6, 0.7], [0.6, 0.4]])\n",
    "A2 = jnp.array([[0.9, 0.1], [0.2, 0.8]])\n",
    "\n",
    "#make reshape outside of looped section, have the tensor shape as input\n",
    "dims = tuple(A.shape[0] for A in (A0, A1, A2))\n",
    "prior = jnp.array([0.27, 0.08, 0.04, 0.06, 0.12, 0.13, 0.10, 0.20])\n",
    "prior_reshaped = prior.reshape(*dims)\n",
    "\n",
    "def make_predictive(prior_tensor:jnp.ndarray, *transition_matrices):\n",
    "    stacked = jnp.stack(transition_matrices)\n",
    "    def step(axis, prior_tensor):\n",
    "        factor_transition_matrix = jnp.take(stacked, axis, axis=0)\n",
    "        predictive_tensor = jnp.moveaxis(prior_tensor, axis, -1)\n",
    "        predictive_tensor = jnp.tensordot(predictive_tensor, factor_transition_matrix, axes=([-1], [0]))\n",
    "        return jnp.moveaxis(predictive_tensor, -1, axis)\n",
    "    \n",
    "    return fori_loop(0, len(transition_matrices), step, prior_tensor)\n",
    "\n",
    "pr_fn = make_predictive(prior_reshaped, A0, A1, A2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usami",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
