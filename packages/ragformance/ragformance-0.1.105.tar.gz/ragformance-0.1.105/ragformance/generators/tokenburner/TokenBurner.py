import litellm
from ragformance.generators.utils.pdf_utils import load_pdf_documents

# /!\ Trigger Warning: this is a first commit the code is not supposed to work in the library at the moment
# updates in progress

import uuid  # For generating unique IDs
from typing import List, Tuple, Dict, Any  # For type hinting

# Global constants for LLM parameters are removed.
# They will be passed as arguments to functions.
# Default temperature can be a local const or default arg.
DEFAULT_TEMPERATURE = 0.0


# Smoke-test function will take parameters
def _run_smoke_test(
    model_name: str,
    api_key: str,
    base_url: str,
    temperature: float = DEFAULT_TEMPERATURE,
):
    """Quick check to verify chat connectivity with OpenRouter LLM."""
    if not api_key:
        print("âš ï¸ API_KEY not set for smoke test. Skipping.")
        return
    try:
        test_prompt = "Hi there! Can you confirm you're alive?"
        response = litellm.completion(
            model="openai/" + model_name,
            messages=[{"role": "user", "content": test_prompt}],
            api_key=api_key,
            base_url=base_url,
            temperature=temperature,
        )
        print(
            f"ðŸ”¹ Chat OK with {model_name} â†’ {response.choices[0].message.content[:80]}â€¦"
        )
    except Exception as e:
        # Softer failure for smoke test, as it's not critical path for generator
        print(f"âš ï¸ Chat smoke-test failed for {model_name}: {e}")


# Layout extraction function, now takes LLM params
def extract_layout(
    pdf_path: str,
    model_name: str,
    api_key: str,
    base_url: str,
    temperature: float = DEFAULT_TEMPERATURE,
) -> str:
    """
    Extract the document's structural layout (sections and subsections with page numbers)
    using a single LLM call with the full context.

    Args:
        pdf_path (str): Path to the PDF file.
        model_name (str): Name of the LLM model.
        api_key (str): API key for the LLM.
        base_url (str): Base URL for the LLM API.
        temperature (float): LLM temperature setting.

    Returns:
        str: Document layout description generated by the LLM.
    """
    if not api_key:
        raise ValueError(
            "API_KEY is not set. Cannot make LLM calls for layout extraction."
        )

    full_text = "\n".join(load_pdf_documents(pdf_path))

    # Build prompt for single-shot layout extraction
    prompt_content = (
        "Extrait la structure du document en listant les sections et sous-sections "
        "avec leurs numÃ©ros de page. PrÃ©sente le rÃ©sultat sous forme de liste hiÃ©rarchique. "
        "Sois prÃ©cis et n'invente pas d'information :\n" + full_text
    )
    response = litellm.completion(
        model="openai/" + model_name,
        messages=[{"role": "user", "content": prompt_content}],
        api_key=api_key,
        base_url=base_url,
        temperature=temperature,
    )
    return response.choices[0].message.content.strip()


def analyze_pdf_unified(
    pdf_path: str,
    layout_str: str,
    model_name: str,
    api_key: str,
    base_url: str,
    temperature: float = DEFAULT_TEMPERATURE,
) -> str:
    """
    Perform all types of analysis on a PDF in a single LLM call using a unified prompt.

    This includes:
      1. Structured summary (by domain topic)
      2. Hierarchical summary (by layout depth)
      3. Entity extraction + Mermaid graph
      4. Concept extraction + Mermaid graph

    Args:
        pdf_path (str): Path to the PDF file.
        layout_str (str): Pre-extracted layout (as plain text).
        model_name (str): Name of the LLM model.
        api_key (str): API key for the LLM.
        base_url (str): Base URL for the LLM API.
        temperature (float): LLM temperature setting.

    Returns:
        str: Unified analysis response
    """
    if not api_key:
        raise ValueError("API_KEY is not set. Cannot make LLM calls for PDF analysis.")

    full_text = "\n".join(load_pdf_documents(pdf_path))

    # Construct unified prompt
    prompt = f"""
    Vous allez recevoir la structure du document et son contenu intÃ©gral. Votre mission est d'en produire une analyse complÃ¨te, en une seule rÃ©ponse, structurÃ©e selon les instructions suivantes.

    ---

    ## EntrÃ©es

    **Structure :**
    {layout_str}

    **Contenu complet :**
    {full_text}

    ---

    ## Sortie attendue

    ### 1. RÃ©sumÃ© structurÃ© (par thÃ©matique fonctionnelle)
    Pour chacune des rubriques suivantes :
      1. Contexte et objectifs
      2. Analyse opÃ©rationnelle
      3. Analyse systÃ¨me
      4. Architecture logique
      5. Architecture physique
      6. SÃ©curitÃ© et perspectives
    - RÃ©sumez en 1 phrase l'objectif global de chaque rubrique
    - Puis ajoutez 1 phrase par paragraphe important
    - Ajoutez tout autre point saillant si pertinent

    ### 2. RÃ©sumÃ© hiÃ©rarchique prÃ©cis (par structure)
    Respectez prÃ©cisÃ©ment chaque niveau indiquÃ© dans la structure (ex : 1, 1.1, 1.1.1...) :
    - Pour chaque niveau :
      - RÃ©sumÃ© global : 1 phrase
      - RÃ©sumÃ© dÃ©taillÃ© : 1 phrase par paragraphe

    ### 3. Extraction hiÃ©rarchique des entitÃ©s et tags
    Pour chaque niveau (section, sous-section, sous-sous-section exactement comme indiquÃ© dans la structure) :
    - Listez les entitÃ©s nommÃ©es clairement identifiÃ©es (personnes, organisations, lieux, technologies)
    - Proposez des tags pertinents spÃ©cifiques Ã  chaque niveau

    GÃ©nÃ©rez ensuite un diagramme Mermaid hiÃ©rarchisÃ© clair, par exemple :

    ```mermaid
    graph TD
      1["1. Titre Section"] --> 1_ENT1["Organisation: ACTIA"]
      1 --> 1_TAG1["Tag: Technologie"]
      1 --> 1.1["1.1 Titre Sous-section"]
      1.1 --> 1.1_ENT1["Personne: Jean Dupont"]
    ```

    ### 4. Extraction hiÃ©rarchique des concepts
        Pour chaque niveau indiquÃ© dans la structure :
        - Listez les concepts-clÃ©s (mÃ©thodologies, composants, technologies, principes techniques) qui se trouvent dans les paragraphes

    GÃ©nÃ©rez un diagramme Mermaid hiÃ©rarchisÃ© correspondant :

    ```mermaid
    graph TD
          1["1. Titre Section"] --> 1_CPT1["Concept: Machine Learning"]
          1 --> 1.1["1.1 Titre Sous-section"]
      1.1 --> 1.1_CPT1["Concept: GPT-4"]
    ```

    ---

    RÃ©pondez dans lâ€™ordre exact ci-dessus. Soyez clair, synthÃ©tique, prÃ©cis et strictement fidÃ¨le au texte, sans inventer d'information.
    """

    response = litellm.completion(
        model="openai/" + model_name,
        messages=[{"role": "user", "content": prompt}],
        api_key=api_key,
        base_url=base_url,
        temperature=temperature,
    )
    return response.choices[0].message.content.strip()


def find_relevant_sections_detailed(
    analysis_result: str,
    layout_str: str,
    user_query: str,
    model_name: str,
    api_key: str,
    base_url: str,
    temperature: float = DEFAULT_TEMPERATURE,
) -> str:
    prompt = f"""
    Ã€ partir de l'analyse complÃ¨te suivante du document, incluant les sections, les entitÃ©s, les tags, et les concepts (prÃ©sentÃ©s en format Mermaid), identifie prÃ©cisÃ©ment les sections pertinentes pour rÃ©pondre Ã  la question suivante:

    Analyse complÃ¨te avec graph Mermaid :
    {analysis_result}

    Structure du document :
    {layout_str}

    Question :
    {user_query}

    Utilise explicitement la hiÃ©rarchie (chapitres, sections, jusqu'aux sous-sections), les rÃ©sumÃ©s ainsi que ce qui a Ã©tÃ© trouvÃ© en MERMAID: les noms, les rÃ©sumÃ©s, les tags, entitÃ©s et concepts pour identifier et justifier les sections pertinentes. RÃ©ponds avec les numÃ©ros et titres exacts des sections pertinentes ainsi qu'une courte justification explicite de leur pertinence basÃ©e sur les tags, entitÃ©s, concepts et rÃ©sumÃ©s fournis.

    produit un mermaid des liens trouvÃ©s entre les rÃ©sumÃ©s/chapitres, sections, sous sections, noms, concepts tags, entitÃ©s et concepts afin d'exprimer ton raisonement

    Ã  la fin liste les sections et sous-sections utilisÃ©es, il est important d'avoir les 2.
    """
    if not api_key:
        raise ValueError(
            "API_KEY is not set. Cannot make LLM calls for finding relevant sections."
        )

    response = litellm.completion(
        model="openai/" + model_name,
        messages=[{"role": "user", "content": prompt}],
        api_key=api_key,
        base_url=base_url,
        temperature=temperature,
    )
    return response.choices[0].message.content.strip()


def extract_raw_text_sections_llm(
    pdf_path: str,
    sections: str,
    model_name: str,
    api_key: str,
    base_url: str,
    temperature: float = DEFAULT_TEMPERATURE,
) -> str:
    full_text = "\n".join(load_pdf_documents(pdf_path))

    prompt = f"""
    Extrait prÃ©cisÃ©ment le texte brut des sections et sous-sections suivantes Ã  partir du document fourni :

    Sections et sous-sections :
    {sections}

    Texte complet :
    {full_text}

    Retourne uniquement le texte brut exact des sections demandÃ©es, sans ajouter ni modifier d'information.
    """
    if not api_key:
        raise ValueError(
            "API_KEY is not set. Cannot make LLM calls for extracting raw text."
        )

    response = litellm.completion(
        model="openai/" + model_name,
        messages=[{"role": "user", "content": prompt}],
        api_key=api_key,
        base_url=base_url,
        temperature=temperature,
    )
    return response.choices[0].message.content.strip()


def generate_questions_from_sections(
    final_sections: str,
    analysis_result: str,
    user_query: str,
    categories_filepath: str,
    model_name: str,
    api_key: str,
    base_url: str,
    temperature: float = DEFAULT_TEMPERATURE,
) -> str:
    with open(categories_filepath, encoding="utf-8") as file:
        categories_definition = file.read()

    prompt = f"""
    Ã€ partir du texte extrait des sections suivantes :
    {final_sections}

    et de l'analyse complÃ¨te (incluant le graphe Mermaid) :
    {analysis_result}

    CatÃ©gories de questions :
    {categories_definition}

    GÃ©nÃ¨re des questions pertinentes pour chaque section et sous-section en lien direct avec la requÃªte utilisateur suivante :
    {user_query}

    Ensuite, utilise les liens indiquÃ©s dans le graphe Mermaid pour crÃ©er des questions additionnelles reliant ces sections, concepts, entitÃ©s entre elles.

    Structure clairement chaque sÃ©rie de questions par section, entitÃ©s, concepts et indique explicitement les liens utilisÃ©s pour gÃ©nÃ©rer les questions additionnelles.
    Pour chaque question prÃ©cise les concepts, entitÃ©s utilisÃ©es.

    enfin de toute ces questions essais d'en gÃ©nÃ©rer des plus globales toujours en liant sections, entitÃ©s, concepts.

    n'oublie pas de classifier chaque question selon les catÃ©gories fournies dans le fichier

    remember those question must be related to concepts and entities of the user input

    n'oublie pas de gÃ©nÃ©rer aussi les rÃ©ponses Ã  chaque fois!
    """
    if not api_key:
        raise ValueError(
            "API_KEY is not set. Cannot make LLM calls for question generation."
        )

    response = litellm.completion(
        model="openai/" + model_name,
        messages=[{"role": "user", "content": prompt}],
        api_key=api_key,
        base_url=base_url,
        temperature=temperature,
    )
    return response.choices[0].message.content.strip()


def refine_questions(
    user_query: str,
    questions_generated: str,
    categories_filepath: str,
    model_name: str,
    api_key: str,
    base_url: str,
    temperature: float = DEFAULT_TEMPERATURE,
) -> str:
    with open(categories_filepath, encoding="utf-8") as file:
        categories_definition = file.read()

    prompt = f"""
    Question utilisateur initiale :
    {user_query}

    Questions gÃ©nÃ©rÃ©es :
    {questions_generated}

    CatÃ©gories de questions :
    {categories_definition}

    SÃ©lectionne les 5 questions les plus pertinentes par rapport Ã  la question utilisateur initiale, en tenant compte de la proximitÃ© sÃ©mantique, des concepts clÃ©s abordÃ©s, les entitÃ©s et en assurant une diversitÃ© des catÃ©gories reprÃ©sentÃ©es.
    Les questions doivent avoir une proximitÃ© des concepts et entitÃ©s avec les questions gÃ©nÃ©rÃ©es.
    Retourne ces 5 questions raffinÃ©es clairement classifiÃ©es selon les catÃ©gories fournies ainsi que les rÃ©ponses Ã  ne pas oublier.
    """
    if not api_key:
        raise ValueError(
            "API_KEY is not set. Cannot make LLM calls for refining questions."
        )

    response = litellm.completion(
        model="openai/" + model_name,
        messages=[{"role": "user", "content": prompt}],
        api_key=api_key,
        base_url=base_url,
        temperature=temperature,
    )
    return response.choices[0].message.content.strip()


# Main processing function to be called by the generator
def run_tokenburner_processing(
    pdf_file_path: str,
    categories_file_path: str,
    model_name: str,
    api_key: str,
    base_url: str,
    user_questions: List[str],
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    _run_smoke_test(
        model_name, api_key, base_url
    )  # Optional: run smoke test with provided creds

    # Step 1: Extract layout
    print(f"Extracting layout from {pdf_file_path} using {model_name}...")
    extracted_layout = extract_layout(pdf_file_path, model_name, api_key, base_url)
    print("Layout extracted.")

    # Step 2: Unified analysis
    print(f"Analyzing PDF {pdf_file_path} using {model_name}...")
    analysis_result = analyze_pdf_unified(
        pdf_file_path, extracted_layout, model_name, api_key, base_url
    )
    print("PDF analysis complete.")

    all_refined_questions_with_answers = []  # Will store final Q&A dicts
    corpus_docs_map = {}  # To store unique corpus sections: {section_text: corpus_id}
    corpus_id_counter = 0

    for user_query in user_questions:
        print(f"\nProcessing user query: {user_query}")

        # Step 3: Find relevant sections
        print("Finding relevant sections...")
        relevant_sections_description = find_relevant_sections_detailed(
            analysis_result, extracted_layout, user_query, model_name, api_key, base_url
        )
        print(
            "Relevant sections identified (description):", relevant_sections_description
        )  # This is a description, not the raw text

        # Step 4: Extract raw text for these sections
        # The 'relevant_sections_description' might need parsing to get actual section titles/numbers
        # to pass to 'extract_raw_text_sections_llm'.
        # For now, assuming 'relevant_sections_description' IS the list of sections string. This might need refinement.
        print("Extracting raw text for relevant sections...")
        final_extracted_sections_text = extract_raw_text_sections_llm(
            pdf_file_path, relevant_sections_description, model_name, api_key, base_url
        )
        print("Raw text of relevant sections extracted.")

        # Add extracted sections to corpus if not already present
        # This is a simplified corpus creation. A more robust approach would parse section titles.
        if final_extracted_sections_text not in corpus_docs_map:
            corpus_id = f"doc_{pdf_file_path}_{corpus_id_counter}"
            corpus_docs_map[final_extracted_sections_text] = corpus_id
            corpus_id_counter += 1
        current_section_corpus_id = corpus_docs_map[final_extracted_sections_text]

        # Step 5: Generate questions from these sections
        print("Generating questions from sections...")
        questions_generated_text = generate_questions_from_sections(
            final_extracted_sections_text,
            analysis_result,
            user_query,
            categories_file_path,
            model_name,
            api_key,
            base_url,
        )
        print("Questions generated from sections.")

        # Step 6: Refine questions
        print("Refining questions...")
        refined_questions_text_blob = refine_questions(
            user_query,
            questions_generated_text,
            categories_file_path,
            model_name,
            api_key,
            base_url,
        )
        print("Questions refined.")

        # The 'refined_questions_text_blob' needs to be parsed into individual Q&A pairs.
        # This is a CRITICAL missing piece. The LLM output format for this is not defined.
        # Assuming it's a list of Q&A pairs in a parsable string format (e.g., JSON-like or specific structure).
        # For now, a placeholder:
        # TODO: Implement robust parsing for the output of refine_questions
        try:
            # This is a guess. The actual output format from the LLM needs to be known.
            # Example: "Question: Q1\nAnswer: A1\n\nQuestion: Q2\nAnswer: A2"
            parsed_qas = []
            # Simple parsing based on "Question:" and "Answer:"
            # This will need to be much more robust based on actual LLM output.
            entries = refined_questions_text_blob.strip().split("Question:")[
                1:
            ]  # Split by "Question:" and remove first empty if any
            for entry in entries:
                if "Answer:" in entry:
                    q_part, a_part = entry.split("Answer:", 1)
                    parsed_qas.append(
                        {
                            "question": q_part.strip(),
                            "answer": a_part.strip()
                            .split("\n\n")[0]
                            .strip(),  # Take only first paragraph of answer
                        }
                    )

            for qa_pair in parsed_qas:
                all_refined_questions_with_answers.append(
                    {
                        "_id": str(uuid.uuid4()),
                        "query_text": qa_pair["question"],
                        "ref_answer": qa_pair["answer"],
                        "relevant_document_ids": [
                            {"corpus_id": current_section_corpus_id, "score": 1.0}
                        ],
                        "metadata": {"original_user_query": user_query},
                    }
                )
        except Exception as e:
            print(
                f"Error parsing refined questions output: {e}. Content: {refined_questions_text_blob}"
            )
            # Add a placeholder if parsing fails
            all_refined_questions_with_answers.append(
                {
                    "_id": str(uuid.uuid4()),
                    "query_text": f"Placeholder for query related to: {user_query}",
                    "ref_answer": "Error in parsing LLM response for refined questions.",
                    "relevant_document_ids": [
                        {"corpus_id": current_section_corpus_id, "score": 1.0}
                    ],
                    "metadata": {
                        "original_user_query": user_query,
                        "parsing_error": str(e),
                    },
                }
            )

    # Format corpus_docs_map for return
    final_corpus_list = []
    for text, doc_id in corpus_docs_map.items():
        final_corpus_list.append(
            {
                "_id": doc_id,
                # Title extraction is complex here. Using a placeholder.
                "title": f"Extracted Section for {pdf_file_path}",
                "text": text,
            }
        )

    return final_corpus_list, all_refined_questions_with_answers
