import litellm
from ragformance.generators.utils.pdf_utils import load_pdf_documents

# /!\ Trigger Warning: this is a first commit the code is not supposed to work in the library at the moment
# updates in progress

import uuid  # For generating unique IDs
from typing import List, Tuple, Dict, Any  # For type hinting

# Global constants for LLM parameters are removed.
# They will be passed as arguments to functions.
# Default temperature can be a local const or default arg.
DEFAULT_TEMPERATURE = 0.0


# Smoke-test function will take parameters
def _run_smoke_test(
    model_name: str,
    api_key: str,
    base_url: str,
    temperature: float = DEFAULT_TEMPERATURE,
):
    """Quick check to verify chat connectivity with OpenRouter LLM."""
    if not api_key:
        print("‚ö†Ô∏è API_KEY not set for smoke test. Skipping.")
        return
    try:
        test_prompt = "Hi there! Can you confirm you're alive?"
        response = litellm.completion(
            model="openai/" + model_name,
            messages=[{"role": "user", "content": test_prompt}],
            api_key=api_key,
            base_url=base_url,
            temperature=temperature,
        )
        print(
            f"üîπ Chat OK with {model_name} ‚Üí {response.choices[0].message.content[:80]}‚Ä¶"
        )
    except Exception as e:
        # Softer failure for smoke test, as it's not critical path for generator
        print(f"‚ö†Ô∏è Chat smoke-test failed for {model_name}: {e}")


# Layout extraction function, now takes LLM params
def extract_layout(
    pdf_path: str,
    model_name: str,
    api_key: str,
    base_url: str,
    temperature: float = DEFAULT_TEMPERATURE,
) -> str:
    """
    Extract the document's structural layout (sections and subsections with page numbers)
    using a single LLM call with the full context.

    Args:
        pdf_path (str): Path to the PDF file.
        model_name (str): Name of the LLM model.
        api_key (str): API key for the LLM.
        base_url (str): Base URL for the LLM API.
        temperature (float): LLM temperature setting.

    Returns:
        str: Document layout description generated by the LLM.
    """
    if not api_key:
        raise ValueError(
            "API_KEY is not set. Cannot make LLM calls for layout extraction."
        )

    full_text = "\n".join(load_pdf_documents(pdf_path))

    # Build prompt for single-shot layout extraction
    prompt_content = (
        "Extrait la structure du document en listant les sections et sous-sections "
        "avec leurs num√©ros de page. Pr√©sente le r√©sultat sous forme de liste hi√©rarchique. "
        "Sois pr√©cis et n'invente pas d'information :\n" + full_text
    )
    response = litellm.completion(
        model="openai/" + model_name,
        messages=[{"role": "user", "content": prompt_content}],
        api_key=api_key,
        base_url=base_url,
        temperature=temperature,
    )
    return response.choices[0].message.content.strip()


def analyze_pdf_unified(
    pdf_path: str,
    layout_str: str,
    model_name: str,
    api_key: str,
    base_url: str,
    temperature: float = DEFAULT_TEMPERATURE,
) -> str:
    """
    Perform all types of analysis on a PDF in a single LLM call using a unified prompt.

    This includes:
      1. Structured summary (by domain topic)
      2. Hierarchical summary (by layout depth)
      3. Entity extraction + Mermaid graph
      4. Concept extraction + Mermaid graph

    Args:
        pdf_path (str): Path to the PDF file.
        layout_str (str): Pre-extracted layout (as plain text).
        model_name (str): Name of the LLM model.
        api_key (str): API key for the LLM.
        base_url (str): Base URL for the LLM API.
        temperature (float): LLM temperature setting.

    Returns:
        str: Unified analysis response
    """
    if not api_key:
        raise ValueError("API_KEY is not set. Cannot make LLM calls for PDF analysis.")

    full_text = "\n".join(load_pdf_documents(pdf_path))

    # Construct unified prompt
    prompt = f"""
    Vous allez recevoir la structure du document et son contenu int√©gral. Votre mission est d'en produire une analyse compl√®te, en une seule r√©ponse, structur√©e selon les instructions suivantes.

    ---

    ## Entr√©es

    **Structure :**
    {layout_str}

    **Contenu complet :**
    {full_text}

    ---

    ## Sortie attendue

    ### 1. R√©sum√© structur√© (par th√©matique fonctionnelle)
    Pour chacune des rubriques suivantes :
      1. Contexte et objectifs
      2. Analyse op√©rationnelle
      3. Analyse syst√®me
      4. Architecture logique
      5. Architecture physique
      6. S√©curit√© et perspectives
    - R√©sumez en 1 phrase l'objectif global de chaque rubrique
    - Puis ajoutez 1 phrase par paragraphe important
    - Ajoutez tout autre point saillant si pertinent

    ### 2. R√©sum√© hi√©rarchique pr√©cis (par structure)
    Respectez pr√©cis√©ment chaque niveau indiqu√© dans la structure (ex : 1, 1.1, 1.1.1...) :
    - Pour chaque niveau :
      - R√©sum√© global : 1 phrase
      - R√©sum√© d√©taill√© : 1 phrase par paragraphe

    ### 3. Extraction hi√©rarchique des entit√©s et tags
    Pour chaque niveau (section, sous-section, sous-sous-section exactement comme indiqu√© dans la structure) :
    - Listez les entit√©s nomm√©es clairement identifi√©es (personnes, organisations, lieux, technologies)
    - Proposez des tags pertinents sp√©cifiques √† chaque niveau

    G√©n√©rez ensuite un diagramme Mermaid hi√©rarchis√© clair, par exemple :

    ```mermaid
    graph TD
      1["1. Titre Section"] --> 1_ENT1["Organisation: ACTIA"]
      1 --> 1_TAG1["Tag: Technologie"]
      1 --> 1.1["1.1 Titre Sous-section"]
      1.1 --> 1.1_ENT1["Personne: Jean Dupont"]
    ```

    ### 4. Extraction hi√©rarchique des concepts
        Pour chaque niveau indiqu√© dans la structure :
        - Listez les concepts-cl√©s (m√©thodologies, composants, technologies, principes techniques) qui se trouvent dans les paragraphes

    G√©n√©rez un diagramme Mermaid hi√©rarchis√© correspondant :

    ```mermaid
    graph TD
          1["1. Titre Section"] --> 1_CPT1["Concept: Machine Learning"]
          1 --> 1.1["1.1 Titre Sous-section"]
      1.1 --> 1.1_CPT1["Concept: GPT-4"]
    ```

    ---

    R√©pondez dans l‚Äôordre exact ci-dessus. Soyez clair, synth√©tique, pr√©cis et strictement fid√®le au texte, sans inventer d'information.
    """

    response = litellm.completion(
        model="openai/" + model_name,
        messages=[{"role": "user", "content": prompt}],
        api_key=api_key,
        base_url=base_url,
        temperature=temperature,
    )
    return response.choices[0].message.content.strip()


def find_relevant_sections_detailed(
    analysis_result: str,
    layout_str: str,
    user_query: str,
    model_name: str,
    api_key: str,
    base_url: str,
    temperature: float = DEFAULT_TEMPERATURE,
) -> str:
    prompt = f"""
    √Ä partir de l'analyse compl√®te suivante du document, incluant les sections, les entit√©s, les tags, et les concepts (pr√©sent√©s en format Mermaid), identifie pr√©cis√©ment les sections pertinentes pour r√©pondre √† la question suivante:

    Analyse compl√®te avec graph Mermaid :
    {analysis_result}

    Structure du document :
    {layout_str}

    Question :
    {user_query}

    Utilise explicitement la hi√©rarchie (chapitres, sections, jusqu'aux sous-sections), les r√©sum√©s ainsi que ce qui a √©t√© trouv√© en MERMAID: les noms, les r√©sum√©s, les tags, entit√©s et concepts pour identifier et justifier les sections pertinentes. R√©ponds avec les num√©ros et titres exacts des sections pertinentes ainsi qu'une courte justification explicite de leur pertinence bas√©e sur les tags, entit√©s, concepts et r√©sum√©s fournis.

    produit un mermaid des liens trouv√©s entre les r√©sum√©s/chapitres, sections, sous sections, noms, concepts tags, entit√©s et concepts afin d'exprimer ton raisonement

    √† la fin liste les sections et sous-sections utilis√©es, il est important d'avoir les 2.
    """
    if not api_key:
        raise ValueError(
            "API_KEY is not set. Cannot make LLM calls for finding relevant sections."
        )

    response = litellm.completion(
        model="openai/" + model_name,
        messages=[{"role": "user", "content": prompt}],
        api_key=api_key,
        base_url=base_url,
        temperature=temperature,
    )
    return response.choices[0].message.content.strip()


def extract_raw_text_sections_llm(
    pdf_path: str,
    sections: str,
    model_name: str,
    api_key: str,
    base_url: str,
    temperature: float = DEFAULT_TEMPERATURE,
) -> str:
    full_text = "\n".join(load_pdf_documents(pdf_path))

    prompt = f"""
    Extrait pr√©cis√©ment le texte brut des sections et sous-sections suivantes √† partir du document fourni :

    Sections et sous-sections :
    {sections}

    Texte complet :
    {full_text}

    Retourne uniquement le texte brut exact des sections demand√©es, sans ajouter ni modifier d'information.
    """
    if not api_key:
        raise ValueError(
            "API_KEY is not set. Cannot make LLM calls for extracting raw text."
        )

    response = litellm.completion(
        model="openai/" + model_name,
        messages=[{"role": "user", "content": prompt}],
        api_key=api_key,
        base_url=base_url,
        temperature=temperature,
    )
    return response.choices[0].message.content.strip()


def generate_questions_from_sections(
    final_sections: str,
    analysis_result: str,
    user_query: str,
    categories_filepath: str,
    model_name: str,
    api_key: str,
    base_url: str,
    temperature: float = DEFAULT_TEMPERATURE,
) -> str:
    with open(categories_filepath, encoding="utf-8") as file:
        categories_definition = file.read()

    prompt = f"""
    √Ä partir du texte extrait des sections suivantes :
    {final_sections}

    et de l'analyse compl√®te (incluant le graphe Mermaid) :
    {analysis_result}

    Cat√©gories de questions :
    {categories_definition}

    G√©n√®re des questions pertinentes pour chaque section et sous-section en lien direct avec la requ√™te utilisateur suivante :
    {user_query}

    Ensuite, utilise les liens indiqu√©s dans le graphe Mermaid pour cr√©er des questions additionnelles reliant ces sections, concepts, entit√©s entre elles.

    Structure clairement chaque s√©rie de questions par section, entit√©s, concepts et indique explicitement les liens utilis√©s pour g√©n√©rer les questions additionnelles.
    Pour chaque question pr√©cise les concepts, entit√©s utilis√©es.

    enfin de toute ces questions essais d'en g√©n√©rer des plus globales toujours en liant sections, entit√©s, concepts.

    n'oublie pas de classifier chaque question selon les cat√©gories fournies dans le fichier

    remember those question must be related to concepts and entities of the user input

    n'oublie pas de g√©n√©rer aussi les r√©ponses √† chaque fois!
    """
    if not api_key:
        raise ValueError(
            "API_KEY is not set. Cannot make LLM calls for question generation."
        )

    response = litellm.completion(
        model="openai/" + model_name,
        messages=[{"role": "user", "content": prompt}],
        api_key=api_key,
        base_url=base_url,
        temperature=temperature,
    )
    return response.choices[0].message.content.strip()


def refine_questions(
    user_query: str,
    questions_generated: str,
    categories_filepath: str,
    model_name: str,
    api_key: str,
    base_url: str,
    temperature: float = DEFAULT_TEMPERATURE,
) -> str:
    with open(categories_filepath, encoding="utf-8") as file:
        categories_definition = file.read()

    prompt = f"""
    Question utilisateur initiale :
    {user_query}

    Questions g√©n√©r√©es :
    {questions_generated}

    Cat√©gories de questions :
    {categories_definition}

    S√©lectionne les 5 questions les plus pertinentes par rapport √† la question utilisateur initiale, en tenant compte de la proximit√© s√©mantique, des concepts cl√©s abord√©s, les entit√©s et en assurant une diversit√© des cat√©gories repr√©sent√©es.
    Les questions doivent avoir une proximit√© des concepts et entit√©s avec les questions g√©n√©r√©es.
    Retourne ces 5 questions raffin√©es clairement classifi√©es selon les cat√©gories fournies ainsi que les r√©ponses √† ne pas oublier.
    """
    if not api_key:
        raise ValueError(
            "API_KEY is not set. Cannot make LLM calls for refining questions."
        )

    response = litellm.completion(
        model="openai/" + model_name,
        messages=[{"role": "user", "content": prompt}],
        api_key=api_key,
        base_url=base_url,
        temperature=temperature,
    )
    return response.choices[0].message.content.strip()


# Main processing function to be called by the generator
def run_tokenburner_processing(
    pdf_file_path: str,
    categories_file_path: str,
    model_name: str,
    api_key: str,
    base_url: str,
    user_questions: List[str],
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    _run_smoke_test(
        model_name, api_key, base_url
    )  # Optional: run smoke test with provided creds

    # Step 1: Extract layout
    print(f"Extracting layout from {pdf_file_path} using {model_name}...")
    extracted_layout = extract_layout(pdf_file_path, model_name, api_key, base_url)
    print("Layout extracted.")

    # Step 2: Unified analysis
    print(f"Analyzing PDF {pdf_file_path} using {model_name}...")
    analysis_result = analyze_pdf_unified(
        pdf_file_path, extracted_layout, model_name, api_key, base_url
    )
    print("PDF analysis complete.")

    all_refined_questions_with_answers = []  # Will store final Q&A dicts
    corpus_docs_map = {}  # To store unique corpus sections: {section_text: corpus_id}
    corpus_id_counter = 0

    for user_query in user_questions:
        print(f"\nProcessing user query: {user_query}")

        # Step 3: Find relevant sections
        print("Finding relevant sections...")
        relevant_sections_description = find_relevant_sections_detailed(
            analysis_result, extracted_layout, user_query, model_name, api_key, base_url
        )
        print(
            "Relevant sections identified (description):", relevant_sections_description
        )  # This is a description, not the raw text

        # Step 4: Extract raw text for these sections
        # The 'relevant_sections_description' might need parsing to get actual section titles/numbers
        # to pass to 'extract_raw_text_sections_llm'.
        # For now, assuming 'relevant_sections_description' IS the list of sections string. This might need refinement.
        print("Extracting raw text for relevant sections...")
        final_extracted_sections_text = extract_raw_text_sections_llm(
            pdf_file_path, relevant_sections_description, model_name, api_key, base_url
        )
        print("Raw text of relevant sections extracted.")

        # Add extracted sections to corpus if not already present
        # This is a simplified corpus creation. A more robust approach would parse section titles.
        if final_extracted_sections_text not in corpus_docs_map:
            corpus_id = f"doc_{pdf_file_path}_{corpus_id_counter}"
            corpus_docs_map[final_extracted_sections_text] = corpus_id
            corpus_id_counter += 1
        current_section_corpus_id = corpus_docs_map[final_extracted_sections_text]

        # Step 5: Generate questions from these sections
        print("Generating questions from sections...")
        questions_generated_text = generate_questions_from_sections(
            final_extracted_sections_text,
            analysis_result,
            user_query,
            categories_file_path,
            model_name,
            api_key,
            base_url,
        )
        print("Questions generated from sections.")

        # Step 6: Refine questions
        print("Refining questions...")
        refined_questions_text_blob = refine_questions(
            user_query,
            questions_generated_text,
            categories_file_path,
            model_name,
            api_key,
            base_url,
        )
        print("Questions refined.")

        # The 'refined_questions_text_blob' needs to be parsed into individual Q&A pairs.
        # This is a CRITICAL missing piece. The LLM output format for this is not defined.
        # Assuming it's a list of Q&A pairs in a parsable string format (e.g., JSON-like or specific structure).
        # For now, a placeholder:
        # TODO: Implement robust parsing for the output of refine_questions
        try:
            # This is a guess. The actual output format from the LLM needs to be known.
            # Example: "Question: Q1\nAnswer: A1\n\nQuestion: Q2\nAnswer: A2"
            parsed_qas = []
            # Simple parsing based on "Question:" and "Answer:"
            # This will need to be much more robust based on actual LLM output.
            entries = refined_questions_text_blob.strip().split("Question:")[
                1:
            ]  # Split by "Question:" and remove first empty if any
            for entry in entries:
                if "Answer:" in entry:
                    q_part, a_part = entry.split("Answer:", 1)
                    parsed_qas.append(
                        {
                            "question": q_part.strip(),
                            "answer": a_part.strip()
                            .split("\n\n")[0]
                            .strip(),  # Take only first paragraph of answer
                        }
                    )

            for qa_pair in parsed_qas:
                all_refined_questions_with_answers.append(
                    {
                        "_id": str(uuid.uuid4()),
                        "query_text": qa_pair["question"],
                        "ref_answer": qa_pair["answer"],
                        "relevant_document_ids": [
                            {"corpus_id": current_section_corpus_id, "score": 1.0}
                        ],
                        "metadata": {"original_user_query": user_query},
                    }
                )
        except Exception as e:
            print(
                f"Error parsing refined questions output: {e}. Content: {refined_questions_text_blob}"
            )
            # Add a placeholder if parsing fails
            all_refined_questions_with_answers.append(
                {
                    "_id": str(uuid.uuid4()),
                    "query_text": f"Placeholder for query related to: {user_query}",
                    "ref_answer": "Error in parsing LLM response for refined questions.",
                    "relevant_document_ids": [
                        {"corpus_id": current_section_corpus_id, "score": 1.0}
                    ],
                    "metadata": {
                        "original_user_query": user_query,
                        "parsing_error": str(e),
                    },
                }
            )

    # Format corpus_docs_map for return
    final_corpus_list = []
    for text, doc_id in corpus_docs_map.items():
        final_corpus_list.append(
            {
                "_id": doc_id,
                # Title extraction is complex here. Using a placeholder.
                "title": f"Extracted Section for {pdf_file_path}",
                "text": text,
            }
        )

    return final_corpus_list, all_refined_questions_with_answers
