{
  "captioning_master": {
    "comment": [
      "Master prompt: emits a runnable captioning model with strict API. The LLM chooses decoder type (Transformer, LSTM, GRU) and hidden sizes/heads based on the classification inspirations and encoder.",
      "Ensures the model is trained using teacher forcing, returns proper shapes, and avoids undefined variables or untested custom attention modules.",
      "Uses a fixed skeleton so that the final model always implements the required API. The LLM may only modify the marked TODO sections in the skeleton."
    ],
    "input_list": [
      {"para": "nn_code", "value": "nn_code"}
    ],
    "addon_list": [
      {"para": "addon_nn_code_1", "value": "nn_code"},
      {"para": "addon_nn_code_2", "value": "nn_code"},
      {"para": "addon_nn_code_3", "value": "nn_code"},
      {"para": "addon_nn_code_4", "value": "nn_code"},
      {"para": "addon_nn_code_5", "value": "nn_code"}
    ],
    "task": "img-captioning",
    "addon_task": "img-classification",
    "prompt": [
      "SYSTEM: Output exactly one fenced Python code block and nothing else.  Do not include prose or multiple code blocks.",
      "",
      "GOAL: Your task is to generate a high-performance image captioning model by taking inspiration from classification model code blocks, and by making safe, meaningful structural tweaks to the target captioning model.",
      "Do NOT output a classifier.  Produce exactly one full, runnable Python file that conforms to the API below.",
      "The emitted Python file must compile without syntax errors. Ensure all parentheses and brackets are balanced and all nn.Linear layers specify both input and output dimensions.",
      "We prioritize high BLEU on the COCO dataset while keeping the model simple, reliable, and easy to train.  Model size and speed are constraints.",
      "",
      "MANDATORY API (must pass validation):",
      "- def supported_hyperparameters(): return {{'lr','momentum'}}",
      "- class Net(nn.Module) with methods:",
      "    - __init__(self, in_shape: tuple, out_shape: tuple, prm: dict, device: torch.device)",
      "    - train_setup(self, prm)",
      "    - learn(self, train_data)",
      "    - forward(self, images, captions=None, hidden_state=None) -> (logits, hidden_state).",
      "- Expose the decoder as self.rnn and implement:",
      "    init_zero_hidden(batch: int, device: torch.device) -> (h0, c0).  For Transformer decoders return two empty tensors for compatibility.",
      "    forward(inputs, hidden_state, features=None) -> (logits, hidden_state).",
      "",
      "REUSE THE ENCODER BACKBONE:",
      "- Remove the classification head from the chosen classification blocks.  Keep the convolutional backbone for the encoder.  Produce a feature tensor [B,1,H] via AdaptiveAvgPool2d and a Linear layer.  Hidden dimension H must be ≥640 (e.g. 640 or 768). You may instead return all patch tokens from a ViT-style encoder to provide the decoder with multiple memory tokens.",
      "",
      "DECODER CHOICE:",
      "- YOU decide which decoder architecture best suits your chosen encoder and classification inspiration: use either nn.LSTM/GRU or nn.TransformerDecoder(batch_first=True).",
      "- Use an LSTM or GRU with an embedding layer and hidden size ≥640 (e.g. 640 or 768).  Initialise hidden state from the encoder features or pass features at each step.  You may include additive or dot-product attention on the encoder feature.",
      "- OR For a Transformer decoder, use nn.TransformerDecoderLayer and nn.TransformerDecoder (batch_first=True); only use nn.MultiheadAttention(embed_dim, num_heads, batch_first=True) (note the lowercase h); choose num_heads dividing hidden_size (e.g. hidden_size=768, num_heads=8 or 12). Do not pass vocab_size into the decoder constructor. This layer takes a query, key and value tensor, and returns the attention output and weights. Don’t define your own Self‑Attention or MultiHeadAttention classes. Use a simple positional encoding (sinusoidal or learnable embedding).  Cross-attend the decoder to the encoder memory (multi-token memory is encouraged). No custom cross‑attention.",
      "- You MUST decide the best hidden size and number of heads yourself; do not expect these values in the prompt.",
      "",
      "TEACHER FORCING & SHAPES:",
      "- If captions.ndim==3, set captions=captions[:,0,:].",
      "- inputs = captions[:,:-1]; targets = captions[:,1:].",
      "- forward() must return (logits, hidden_state) where logits has shape [B,T-1,vocab_size].  The first element of the returned tuple MUST be a Tensor.",
      "- Shape Sentry: assert images.dim()==4; assert logits.shape[1]==inputs.shape[1]; assert logits.shape[-1]==vocab_size.",
      "- in_channels = int(in_shape[1]); vocab_size = int(out_shape[0]).",
      "- forward() must return a tuple, where the first element is a Tensor of shape [B, T-1, vocab_size] (for teacher forcing) and the second element is the hidden state. Do not wrap logits in another tuple or return more than two values.",
      "",
      "TRAINING SETUP:",
      "- In train_setup(): call self.to(self.device); set self.criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1); use torch.optim.AdamW(self.parameters(), lr=max(float(prm.get('lr',1e-3)), 3e-4), betas=(min(max(float(prm.get('momentum',0.9)),0.7),0.99), 0.999)). always use the momentum parameter.",
      "- In learn(): implement a loop over train_data; call logits,_ = self.forward(images,captions,None); compute loss as CrossEntropyLoss on logits.reshape(-1,V) and targets.reshape(-1); zero grad, backward, clip grads, step optimizer.  No scheduler unless simple (e.g. ReduceLROnPlateau).",
      "",
      "SAFE EDITS ONLY:",
      "- Reuse the base captioning skeleton.  Only modify: encoder blocks (e.g. bottleneck 1x1-3x3-1x1, SE/CBAM), decoder type and hidden dims/head counts, and simple attention on encoder features.",
      "- The model must implement supported_hyperparameters(), __init__, train_setup, learn, and forward exactly as shown in the API skeleton below. You may modify encoder/decoder initialization and their forward calls, but do not remove these functions or alter their signatures.",
      "- Do NOT create complex custom cross-attention or positional encodings.  Use nn.MultiheadAttention and PositionalEncoding or a simple embedding for positions.",
      "- Do NOT import torchvision or pretrained weights.  Define all layers in this file.",
      "- Use only valid PyTorch APIs from torch, torch.nn, torch.nn.functional. Do not invent modules (e.g., nn.SelfAttention is invalid).",
      "- Do not create or use custom attention modules (e.g. nn.SelfAttention or nn.MultiHeadAttention). Use only nn.MultiheadAttention or stick to LSTM/GRU.",
      "- Avoid referencing undefined variables or copying classification variables that are irrelevant.",
      "- The file must compile: no missing parentheses/brackets, every nn.Linear has two size args, no trailing characters.",
      "",
      "DIVERSITY REQUIREMENTS:",
      "- You MUST make at least three structural changes relative to the original captioning model: change the encoder block type, choose a different decoder family, vary hidden dimensions (≥640), add SE/CBAM or depthwise blocks, etc.",
      "- Randomise your choices per run to encourage a wide variety of models.  But keep within the safe modifications above.",
      "",
      "BLEU-ORIENTED GUIDANCE:",
      "- Larger hidden sizes and multi-head attention (when using Transformer) generally improve BLEU.  Hidden sizes in {{640, 768}} and num_heads in {{8, 12}} are good starting points.",
      "- Adding Squeeze-and-Excitation (SE) or CBAM to the encoder can help while staying within budget.",
      "- Keep dropout modest (0.1–0.3) in the decoder.  Clip gradients at 3.0.",
      "- Keep parameter count around 2 million.  Do not add untested modules.",
      "",
      "API SKELETON:",
      "The following skeleton defines the mandatory API functions and method structure. You MUST build your model by modifying only the sections marked TODO. Do not remove or rename any functions or variables outside the TODO sections. The resulting file must compile and adhere to the API requirements.",
      "```python",
      "# === Begin Skeleton ===",
      "import math",
      "import torch",
      "import torch.nn as nn",
      "import torch.nn.functional as F",
      "from typing import Optional",
      "",
      "def supported_hyperparameters():",
      "    return {{'lr','momentum'}}",
      "",
      "class Net(nn.Module):",
      "    def __init__(self, in_shape: tuple, out_shape: tuple, prm: dict, device: torch.device):",
      "        super().__init__()",
      "        self.device = device",
      "        self.vocab_size = int(out_shape[0])",
      "        in_channels = int(in_shape[1])",
      "        # TODO: Replace the encoder with your custom encoder producing memory [B,S,H] (H≥640).",
      "        self.encoder = nn.Identity()",
      "        # Hidden dimension (≥640) used by the decoder. Modify if needed.",
      "        self.hidden_dim = 640",
      "        # TODO: Replace the decoder with your custom decoder. It must be assigned to self.rnn and implement forward(inputs, hidden_state, features) -> (logits, hidden_state).",
      "        self.rnn = nn.Identity()",
      "        self.criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)",
      "        self.optimizer = None",
      "",
      "    def init_zero_hidden(self, batch: int, device: torch.device):",
      "        return torch.empty(0, device=device), torch.empty(0, device=device)",
      "",
      "    def train_setup(self, prm: dict):",
      "        self.to(self.device)",
      "        lr = max(float(prm.get('lr', 1e-3)), 3e-4)",
      "        beta1 = min(max(float(prm.get('momentum', 0.9)), 0.7), 0.99)",
      "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=lr, betas=(beta1, 0.999))",
      "        self.criterion = self.criterion.to(self.device)",
      "",
      "    def learn(self, train_data):",
      "        self.train()",
      "        for images, captions in train_data:",
      "            images = images.to(self.device, dtype=torch.float32)",
      "            caps = captions[:,0,:].long().to(self.device) if captions.ndim == 3 else captions.long().to(self.device)",
      "            inputs = caps[:, :-1]",
      "            targets = caps[:, 1:]",
      "            memory = self.encoder(images)",
      "            logits, _ = self.rnn(inputs, None, memory)",
      "            assert images.dim() == 4",
      "            assert logits.shape[1] == inputs.shape[1]",
      "            assert logits.shape[-1] == self.vocab_size",
      "            loss = self.criterion(logits.reshape(-1, self.vocab_size), targets.reshape(-1))",
      "            self.optimizer.zero_grad(set_to_none=True)",
      "            loss.backward()",
      "            torch.nn.utils.clip_grad_norm_(self.parameters(), 3.0)",
      "            self.optimizer.step()",
      "",
      "    def forward(self, images: torch.Tensor, captions: Optional[torch.Tensor] = None, hidden_state=None):",
      "        images = images.to(self.device, dtype=torch.float32)",
      "        memory = self.encoder(images)",
      "        if captions is not None:",
      "            caps = captions[:,0,:].long().to(self.device) if captions.ndim == 3 else captions.long().to(self.device)",
      "            inputs = caps[:, :-1]",
      "            logits, hidden_state = self.rnn(inputs, hidden_state, memory)",
      "            return logits, hidden_state",
      "        else:",
      "            # TODO: Implement greedy or beam search decoding using self.rnn and memory to return token ids [B, <=max_len].",
      "            raise NotImplementedError()",
      "",
      "# === End Skeleton ===",
      "```",
      "",
      "INSPIRATION CLASSIFICATION MODELS:",
      "```python",
      "{addon_nn_code_1}",
      "```",
      "```python",
      "{addon_nn_code_2}",
      "```",
      "```python",
      "{addon_nn_code_3}",
      "```",
      "```python",
      "{addon_nn_code_4}",
      "```",
      "```python",
      "{addon_nn_code_5}",
      "```",
      "",
      "ORIGINAL CAPTIONING CODE (reference only):",
      "```python",
      "{nn_code}",
      "```",
      "",
      "WRITE THE FINAL RUNNABLE FILE NOW.  Do not include extra commentary."
    ]
  }
}