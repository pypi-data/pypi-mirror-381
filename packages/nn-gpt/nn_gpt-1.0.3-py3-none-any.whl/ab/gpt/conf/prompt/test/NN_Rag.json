{
    "synthesize_classification": {
    "task": "synthesis",
    "input_list": [],
    "addon_list": [],
    "prompt": [
    "SYSTEM: You are DeepSeek-Coder-7B-Instruct. Follow every rule precisely.",
    "",
    "RULES:",
    "1. You will receive one PyTorch building block delimited by triple-back-ticks.",
    "2. Produce exactly one Python file that:",
    " 2.1 defines supported_hyperparameters() returning {'lr','momentum'};",
    " 2.2 implements class Net(nn.Module) with constructor __init__(self, in_shape: tuple, out_shape: tuple, prm: dict, device: torch.device);",
    " 2.3 builds a feature extractor that uses the supplied block at least once (you may duplicate it);",
    " 2.4 finishes with nn.AdaptiveAvgPool2d((1,1)) and an nn.Linear mapping to out_shape[0] classes;",
    " 2.5 implements forward, train_setup(prm), and learn(train_data) exactly as in the template;",
    " 2.6 uses torch.optim.SGD, learning-rate and momentum from prm, and clips gradients with max_norm=3 via nn.utils.clip_grad_norm_ after loss.backward() (do not pass max_norm to the optimizer);",
    " 2.7 Memory guard: if you call ResNestBottleneck or SplAtConv2d, force groups=1, radix=1, bottleneck_width=16.",
    " 2.8 Whole network must stay under 2 000 000 trainable parameters.",
    " 2.9 Do not allocate large tensors in __init__; postpone to forward() and avoid meshgrid, unfold, or quadratic memory ops over H×W. If you must call torch.meshgrid, pass indexing='ij'.",
    " 2.10 When instantiating the provided block, pass only arguments that are present in its __init__ signature. If rule 2.7 applies but the constructor lacks those keywords, instantiate normally and set the attributes afterwards.",
    " 2.11 2D-only ops: use Conv2d/BatchNorm2d/MaxPool2d/AvgPool2d etc. Do not use Conv1d or Conv3d. Always start with a stem nn.Conv2d(self.in_channels, 32, 3, padding=1, bias=False) + BN + ReLU to ensure channel compatibility from RGB.",
    " 2.12 If you use GroupNorm, ensure num_channels % num_groups == 0; otherwise prefer BatchNorm2d.",
    " 2.13 Avoid LayerNorm directly on NCHW. If you use it, first permute to NHWC, set normalized_shape to the last dimension (C), then permute back.",
    " 2.14 Do not use nn.MultiheadAttention unless the embedding dim is divisible by num_heads and the input is [L, N, E]. Prefer avoiding MHA altogether.",
    " 2.15 Use only one final nn.Linear(self._last_channels, self.num_classes) after global pooling. If you insert extra Linear layers, make them lightweight or use nn.LazyLinear to avoid shape bugs.",
    " 2.16 Keep spatial size generic; the model must accept inputs ≥ 128×128.",
    " 2.17 Adapter rule for token-based blocks: If the provided block’s forward expects 3D tokens (B*, N, C) and returns (B*, N, C) (typical for windowed attention), wrap it in a small adapter that: (a) receives NCHW, (b) pads H/W to multiples of the window if needed, (c) converts to NHWC and partitions into nH*nW windows of size (Wh, Ww) producing (B*nH*nW, Wh*Ww, C), (d) calls the block, (e) merges windows back to NHWC, unpads, and returns NCHW. Name this wrapper WinAttnAdapter and pass only constructor args that exist in the block’s __init__.",
    "",
    "TEMPLATE (only edit inside build_features(); you may define tiny helper modules/classes above it like WinAttnAdapter if needed):",
    "python", "import torch", "import torch.nn as nn", "import torch.nn.functional as F", "", "def supported_hyperparameters():", " return {'lr','momentum'}", "", "class Net(nn.Module):", " def __init__(self, in_shape: tuple, out_shape: tuple, prm: dict, device: torch.device) -> None:", " super().__init__()", " self.device = device", " self.in_channels = in_shape[1]", " self.image_size = in_shape[2]", " self.num_classes = out_shape[0]", " self.learning_rate = prm['lr']", " self.momentum = prm['momentum']", "", " self.features = self.build_features()", " self.avgpool = nn.AdaptiveAvgPool2d((1, 1))", " self.classifier = nn.Linear(self._last_channels, self.num_classes)", "", " def build_features(self):", " layers = []", " # Stable 2D stem to avoid channel/shape mismatches", " layers += [", " nn.Conv2d(self.in_channels, 32, kernel_size=3, padding=1, bias=False),", " nn.BatchNorm2d(32),", " nn.ReLU(inplace=True),", " ]", "", " # ← insert the provided block here; repeat/adapt as needed.", " # If the block expects (B*, N, C) tokens, create a WinAttnAdapter(dim=32, window_size=(7,7), num_heads=4, block_cls=ProvidedBlock, ...)", " # If the block is 2D (NCHW->NCHW), just append it after the stem.", "", " # Example patterns you may use (choose ONE appropriate to the block):", " # layers += [ProvidedBlock(in_ch=32, out_ch=32)]", " # layers += [WinAttnAdapter(dim=32, window_size=(7,7), num_heads=4, block_cls=ProvidedBlock)]", "", " # Keep under parameter budget and end with a known channel count:", " self._last_channels = 32", " return nn.Sequential(*layers)", "", " def forward(self, x):", " x = self.features(x)", " x = self.avgpool(x)", " x = torch.flatten(x, 1)", " return self.classifier(x)", "", " def train_setup(self, prm):", " self.to(self.device)", " self.criteria = nn.CrossEntropyLoss().to(self.device)", " self.optimizer = torch.optim.SGD(", " self.parameters(), lr=self.learning_rate, momentum=self.momentum)", "", " def learn(self, train_data):", " self.train()", " for inputs, labels in train_data:", " inputs, labels = inputs.to(self.device), labels.to(self.device)", " self.optimizer.zero_grad()", " outputs = self(inputs)", " loss = self.criteria(outputs, labels)", " loss.backward()", " nn.utils.clip_grad_norm_(self.parameters(), 3)", " self.optimizer.step()", "",
    "",
    "{block}"
    ]
    },
    "synthesize_classification_rag": {
    "task": "synthesis",
    "input_list": [],
    "addon_list": [],
    "prompt": [
    "SYSTEM: You are DeepSeek-Coder-7B-Instruct. Follow every rule precisely.",
    "",
    "RULES:",
    "1. You will receive one PyTorch building block delimited by triple-back-ticks.",
    "2. Produce exactly one Python file that:",
    " 2.1 defines supported_hyperparameters() returning {'lr','momentum'};",
    " 2.2 implements class Net(nn.Module) with constructor __init__(self, in_shape: tuple, out_shape: tuple, prm: dict, device: torch.device);",
    " 2.3 builds a feature extractor that uses the supplied block at least once;",
    " 2.4 finishes with nn.AdaptiveAvgPool2d((1,1)) and an nn.Linear mapping to out_shape[0] classes;",
    " 2.5 implements forward, train_setup(prm), and learn(train_data) exactly as in the template;",
    " 2.6 uses torch.optim.SGD with learning-rate and momentum from prm, clips gradients with max_norm=3;",
    " 2.7 Memory guard: for ResNestBottleneck/SplAtConv2d, force groups=1, radix=1, bottleneck_width=16;",
    " 2.8 Keep under 2M trainable parameters, avoid large tensors in __init__;",
    " 2.9 Use only 2D ops (Conv2d/BatchNorm2d/MaxPool2d), start with stem Conv2d(32) + BN + ReLU;",
    " 2.10 Pass only existing arguments to block constructor, use exact class names from provided block;",
    " 2.11 Add marker '# ---- <BlockClass> (target) ----' at first line, wrap output in ```python```;",
    "",
    "CRITICAL ERROR PREVENTION (based on 400+ implementations):",
    " 2.23 MISSING IMPORTS: Add 'import torch', 'import torch.nn as nn', 'import torch.nn.functional as F', 'import torchvision', 'import numpy as np', 'import math', 'import os' at top.",
    " 2.24 MISSING DEPENDENCIES: Create minimal implementations for undefined classes: LayerNorm2d (2D layer norm), MODELS (add get() method), get_device() (return torch.device), _Reduction (import from F), EdgeIndex (placeholder), cp (import checkpoint).",
    " 2.25 MEMORY OPTIMIZATION: For attention blocks use num_heads=1-2, spatial_range=1-3, position_embedding_dim=4-8, kv_stride=4-8, add MaxPool2d before attention.",
    " 2.26 SHAPE COMPATIBILITY: Add adapters for different input shapes, generate synthetic coordinates for point cloud blocks, create proper indices for graph blocks.",
    " 2.27 CHANNEL COMPATIBILITY: Verify channel counts match, update self._last_channels when blocks change channels, use channel reducers when needed.",
    " 2.28 ACTIVATION FIXES: Use inplace=False to avoid gradient issues, ensure proper forward/backward compatibility.",
    " 2.29 STATIC METHODS: Add @staticmethod decorator to methods without self, fix argument count errors.",
    " 2.30 DEVICE COMPATIBILITY: Ensure all tensors use same device, use .to(device) for components.",
    " 2.31 ENV VARIABLES: Use os.environ.get('VAR', 'default') instead of direct access.",
    " 2.32 REGISTRY FIXES: Add fallbacks when registry lookups fail, use standard PyTorch layers as fallbacks.",
    " 2.33 DETECTION BLOCKS: Use appropriate anchors for CIFAR-10 scale, handle multi-scale outputs properly.",
    "",
    "TEMPLATE (add missing imports/classes at top, edit build_features(), add top marker per 2.18):",
    "python", "import torch", "import torch.nn as nn", "import torch.nn.functional as F", "import torchvision", "import numpy as np", "import math", "import os", "", "# Add missing classes if needed (LayerNorm2d, MODELS, get_device, etc.)", "", "def supported_hyperparameters():", " return {'lr','momentum'}", "", "class Net(nn.Module):", " def __init__(self, in_shape: tuple, out_shape: tuple, prm: dict, device: torch.device) -> None:", " super().__init__()", " self.device = device", " self.in_channels = in_shape[1]", " self.image_size = in_shape[2]", " self.num_classes = out_shape[0]", " self.learning_rate = prm['lr']", " self.momentum = prm['momentum']", "", " self.features = self.build_features()", " self.avgpool = nn.AdaptiveAvgPool2d((1, 1))", " self.classifier = nn.Linear(self._last_channels, self.num_classes)", "", " def build_features(self):", " layers = []", " layers += [", " nn.Conv2d(self.in_channels, 32, kernel_size=3, padding=1, bias=False),", " nn.BatchNorm2d(32),", " nn.ReLU(inplace=False),", " ]", "", " # Add MaxPool2d for memory-intensive blocks", " # layers += [nn.MaxPool2d(kernel_size=2, stride=2)]", "", " # Insert provided block with reduced parameters for attention blocks", " # layers += [ProvidedBlock(32, 32, num_heads=1, spatial_range=1)]", "", " self._last_channels = 32", " return nn.Sequential(*layers)", "", " def forward(self, x):", " x = self.features(x)", " x = self.avgpool(x)", " x = torch.flatten(x, 1)", " return self.classifier(x)", "", " def train_setup(self, prm):", " self.to(self.device)", " self.criteria = nn.CrossEntropyLoss().to(self.device)", " self.optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate, momentum=self.momentum, weight_decay=5e-4)", "", " def learn(self, train_data):", " self.train()", " for inputs, labels in train_data:", " inputs, labels = inputs.to(self.device), labels.to(self.device)", " self.optimizer.zero_grad()", " outputs = self(inputs)", " loss = self.criteria(outputs, labels)", " loss.backward()", " nn.utils.clip_grad_norm_(self.parameters(), 3)", " self.optimizer.step()", "",
    "BLOCK (read-only; delimited by triple backticks):",
    "```python",
    "{block}",
    "```"
    ]
    },
    "synthesize_padding_blocks": {
    "task": "synthesis",
    "input_list": [],
    "addon_list": [],
    "prompt": [
    "SYSTEM: You are DeepSeek-Coder-7B-Instruct. Handle padding blocks specifically.",
    "",
    "RULES for Padding Blocks (_CircularPadNd, _ConstantPadNd, _ReflectionPadNd, _ReplicationPadNd):",
    "1. Add missing _check_input_dim method: if input.dim() < 3: raise ValueError(f'Expected input with at least 3 dimensions, got {input.dim()}')",
    "2. Account for spatial dimension changes: 4x4 becomes 6x6 with [1,1,1,1] padding",
    "3. Update classifier input size: 64 * 6 * 6 = 2304 instead of 64 * 4 * 4 = 1024",
    "4. Use the padding block after adaptive pooling but before classification",
    "",
    "TEMPLATE:",
    "python",
    "import torch",
    "import torch.nn as nn",
    "import torch.nn.functional as F",
    "",
    "def supported_hyperparameters():",
    "    return {'lr','momentum'}",
    "",
    "class Net(nn.Module):",
    "    def __init__(self, in_shape: tuple, out_shape: tuple, prm: dict, device: torch.device) -> None:",
    "        super().__init__()",
    "        self.device = device",
    "        self.in_channels = in_shape[1]",
    "        self.image_size = in_shape[2]",
    "        self.num_classes = out_shape[0]",
    "        self.learning_rate = prm['lr']",
    "        self.momentum = prm['momentum']",
    "        self.features = self.build_features()",
    "        self.padding_block = ProvidedBlock()",
    "        self.padding_block.padding = [1, 1, 1, 1]",
    "        self.classifier = nn.Linear(64 * 6 * 6, self.num_classes)",
    "",
    "    def build_features(self):",
    "        layers = []",
    "        layers += [",
    "            nn.Conv2d(self.in_channels, 32, kernel_size=3, padding=1, bias=False),",
    "            nn.BatchNorm2d(32),",
    "            nn.ReLU(inplace=True),",
    "            nn.MaxPool2d(kernel_size=2, stride=2),",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),",
    "            nn.BatchNorm2d(64),",
    "            nn.ReLU(inplace=True),",
    "            nn.MaxPool2d(kernel_size=2, stride=2)",
    "        ]",
    "        return nn.Sequential(*layers)",
    "",
    "    def forward(self, x):",
    "        x = self.features(x)",
    "        x = F.adaptive_avg_pool2d(x, (4, 4))",
    "        x = self.padding_block(x)",
    "        x = x.view(x.size(0), -1)",
    "        return self.classifier(x)",
    "",
    "    def train_setup(self, prm):",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate, momentum=self.momentum)",
    "        self.criterion = nn.CrossEntropyLoss()",
    "",
    "    def learn(self, data_roll):",
    "        for data, target in data_roll:",
    "            data, target = data.to(self.device), target.to(self.device)",
    "            self.optimizer.zero_grad()",
    "            output = self.forward(data)",
    "            loss = self.criterion(output, target)",
    "            loss.backward()",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)",
    "            self.optimizer.step()",
    "",
    "BLOCK:",
    "```python",
    "{block}",
    "```"
    ]
    },
    "synthesize_attention_blocks": {
    "task": "synthesis",
    "input_list": [],
    "addon_list": [],
    "prompt": [
    "SYSTEM: You are DeepSeek-Coder-7B-Instruct. Handle attention blocks specifically.",
    "",
    "RULES for Attention Blocks (XCA, XCABlock, SwinSelfAttention, etc.):",
    "1. Reduce parameters: num_heads=1-2, window_size=4x4, smaller dimensions",
    "2. Add F.adaptive_avg_pool2d before attention to reduce memory usage",
    "3. Handle device mismatches in mask creation: ensure tensors are on correct device",
    "4. Reshape inputs properly for sequence processing: (B,C,H,W) -> (B,H*W,C)",
    "5. Use os.environ.get('TIMM_FUSED_ATTN', '0') for environment variables",
    "",
    "TEMPLATE:",
    "python",
    "import torch",
    "import torch.nn as nn",
    "import torch.nn.functional as F",
    "import os",
    "",
    "def supported_hyperparameters():",
    "    return {'lr','momentum'}",
    "",
    "class Net(nn.Module):",
    "    def __init__(self, in_shape: tuple, out_shape: tuple, prm: dict, device: torch.device) -> None:",
    "        super().__init__()",
    "        self.device = device",
    "        self.in_channels = in_shape[1]",
    "        self.image_size = in_shape[2]",
    "        self.num_classes = out_shape[0]",
    "        self.learning_rate = prm['lr']",
    "        self.momentum = prm['momentum']",
    "        self.features = self.build_features()",
    "        self.attention_block = ProvidedBlock(dim=64, num_heads=2, window_size=4)",
    "        self.classifier = nn.Linear(64, self.num_classes)",
    "",
    "    def build_features(self):",
    "        layers = []",
    "        layers += [",
    "            nn.Conv2d(self.in_channels, 32, kernel_size=3, padding=1, bias=False),",
    "            nn.BatchNorm2d(32),",
    "            nn.ReLU(inplace=True),",
    "            nn.MaxPool2d(kernel_size=2, stride=2),",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),",
    "            nn.BatchNorm2d(64),",
    "            nn.ReLU(inplace=True),",
    "            nn.MaxPool2d(kernel_size=2, stride=2)",
    "        ]",
    "        return nn.Sequential(*layers)",
    "",
    "    def forward(self, x):",
    "        x = self.features(x)",
    "        B, C, H, W = x.shape",
    "        x = F.adaptive_avg_pool2d(x, (4, 4))",
    "        B, C, H, W = x.shape",
    "        x = x.view(B, C, H * W).transpose(1, 2)",
    "        x = self.attention_block(x)",
    "        x = x.mean(dim=1)",
    "        return self.classifier(x)",
    "",
    "    def train_setup(self, prm):",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate, momentum=self.momentum)",
    "        self.criterion = nn.CrossEntropyLoss()",
    "",
    "    def learn(self, data_roll):",
    "        for data, target in data_roll:",
    "            data, target = data.to(self.device), target.to(self.device)",
    "            self.optimizer.zero_grad()",
    "            output = self.forward(data)",
    "            loss = self.criterion(output, target)",
    "            loss.backward()",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)",
    "            self.optimizer.step()",
    "",
    "BLOCK:",
    "```python",
    "{block}",
    "```"
    ]
    },
    "synthesize_transformer_blocks": {
    "task": "synthesis",
    "input_list": [],
    "addon_list": [],
    "prompt": [
    "SYSTEM: You are DeepSeek-Coder-7B-Instruct. Handle transformer blocks specifically.",
    "",
    "RULES for Transformer Blocks (TransformerBlock, TFDecoderLayer, etc.):",
    "1. Reshape CNN features to sequence format: (B,C,H,W) -> (B,H*W,C)",
    "2. Use smaller dimensions: d_model=32, d_inner=64, n_head=2",
    "3. Add spatial pooling before transformer processing",
    "4. Handle positional encodings correctly",
    "5. Create proper attention masks",
    "",
    "TEMPLATE:",
    "python",
    "import torch",
    "import torch.nn as nn",
    "import torch.nn.functional as F",
    "",
    "def supported_hyperparameters():",
    "    return {'lr','momentum'}",
    "",
    "class Net(nn.Module):",
    "    def __init__(self, in_shape: tuple, out_shape: tuple, prm: dict, device: torch.device) -> None:",
    "        super().__init__()",
    "        self.device = device",
    "        self.in_channels = in_shape[1]",
    "        self.image_size = in_shape[2]",
    "        self.num_classes = out_shape[0]",
    "        self.learning_rate = prm['lr']",
    "        self.momentum = prm['momentum']",
    "        self.features = self.build_features()",
    "        self.transformer_block = ProvidedBlock(d_model=32, d_inner=64, n_head=2)",
    "        self.classifier = nn.Linear(32, self.num_classes)",
    "",
    "    def build_features(self):",
    "        layers = []",
    "        layers += [",
    "            nn.Conv2d(self.in_channels, 32, kernel_size=3, padding=1, bias=False),",
    "            nn.BatchNorm2d(32),",
    "            nn.ReLU(inplace=True),",
    "            nn.MaxPool2d(kernel_size=2, stride=2),",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),",
    "            nn.BatchNorm2d(64),",
    "            nn.ReLU(inplace=True),",
    "            nn.MaxPool2d(kernel_size=2, stride=2)",
    "        ]",
    "        return nn.Sequential(*layers)",
    "",
    "    def forward(self, x):",
    "        x = self.features(x)",
    "        B, C, H, W = x.shape",
    "        x = F.adaptive_avg_pool2d(x, (4, 4))",
    "        B, C, H, W = x.shape",
    "        x = x.view(B, C, H * W).transpose(1, 2)",
    "        x = self.transformer_block(x)",
    "        x = x.mean(dim=1)",
    "        return self.classifier(x)",
    "",
    "    def train_setup(self, prm):",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate, momentum=self.momentum)",
    "        self.criterion = nn.CrossEntropyLoss()",
    "",
    "    def learn(self, data_roll):",
    "        for data, target in data_roll:",
    "            data, target = data.to(self.device), target.to(self.device)",
    "            self.optimizer.zero_grad()",
    "            output = self.forward(data)",
    "            loss = self.criterion(output, target)",
    "            loss.backward()",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)",
    "            self.optimizer.step()",
    "",
    "BLOCK:",
    "```python",
    "{block}",
    "```"
    ]
    },
    "synthesize_loss_blocks": {
    "task": "synthesis",
    "input_list": [],
    "addon_list": [],
    "prompt": [
    "SYSTEM: You are DeepSeek-Coder-7B-Instruct. Handle loss blocks specifically.",
    "",
    "RULES for Loss Blocks (SIoULoss, TverskyLoss, etc.):",
    "1. Create appropriate dummy targets for loss functions",
    "2. Handle different loss function signatures and tensor shapes",
    "3. Use only CrossEntropyLoss for actual training",
    "4. Keep loss blocks for demonstration purposes",
    "5. Handle tensor shape mismatches and broadcasting issues",
    "",
    "TEMPLATE:",
    "python",
    "import torch",
    "import torch.nn as nn",
    "import torch.nn.functional as F",
    "import numpy as np",
    "",
    "def supported_hyperparameters():",
    "    return {'lr','momentum'}",
    "",
    "class Net(nn.Module):",
    "    def __init__(self, in_shape: tuple, out_shape: tuple, prm: dict, device: torch.device) -> None:",
    "        super().__init__()",
    "        self.device = device",
    "        self.in_channels = in_shape[1]",
    "        self.image_size = in_shape[2]",
    "        self.num_classes = out_shape[0]",
    "        self.learning_rate = prm['lr']",
    "        self.momentum = prm['momentum']",
    "        self.features = self.build_features()",
    "        self.loss_block = ProvidedBlock()",
    "        self.classifier = nn.Linear(64, self.num_classes)",
    "",
    "    def build_features(self):",
    "        layers = []",
    "        layers += [",
    "            nn.Conv2d(self.in_channels, 32, kernel_size=3, padding=1, bias=False),",
    "            nn.BatchNorm2d(32),",
    "            nn.ReLU(inplace=True),",
    "            nn.MaxPool2d(kernel_size=2, stride=2),",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),",
    "            nn.BatchNorm2d(64),",
    "            nn.ReLU(inplace=True),",
    "            nn.MaxPool2d(kernel_size=2, stride=2)",
    "        ]",
    "        return nn.Sequential(*layers)",
    "",
    "    def forward(self, x):",
    "        x = self.features(x)",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))",
    "        x = x.view(x.size(0), -1)",
    "        return self.classifier(x)",
    "",
    "    def train_setup(self, prm):",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate, momentum=self.momentum)",
    "        self.criterion = nn.CrossEntropyLoss()",
    "",
    "    def learn(self, data_roll):",
    "        for data, target in data_roll:",
    "            data, target = data.to(self.device), target.to(self.device)",
    "            self.optimizer.zero_grad()",
    "            output = self.forward(data)",
    "            loss = self.criterion(output, target)",
    "            loss.backward()",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)",
    "            self.optimizer.step()",
    "",
    "BLOCK:",
    "```python",
    "{block}",
    "```"
    ]
    },
    "synthesize_complex_architectures": {
    "task": "synthesis",
    "input_list": [],
    "addon_list": [],
    "prompt": [
    "SYSTEM: You are DeepSeek-Coder-7B-Instruct. Handle complex architectures specifically.",
    "",
    "RULES for Complex Architectures (Xception, VGG, etc.):",
    "1. Use the complete architecture directly without additional CNN backbone",
    "2. Ensure proper input channel compatibility",
    "3. Handle the full forward pass correctly",
    "4. Create custom feature extractors if needed",
    "5. Maintain the original architecture's design",
    "",
    "TEMPLATE:",
    "python",
    "import torch",
    "import torch.nn as nn",
    "import torch.nn.functional as F",
    "",
    "def supported_hyperparameters():",
    "    return {'lr','momentum'}",
    "",
    "class Net(nn.Module):",
    "    def __init__(self, in_shape: tuple, out_shape: tuple, prm: dict, device: torch.device) -> None:",
    "        super().__init__()",
    "        self.device = device",
    "        self.in_channels = in_shape[1]",
    "        self.image_size = in_shape[2]",
    "        self.num_classes = out_shape[0]",
    "        self.learning_rate = prm['lr']",
    "        self.momentum = prm['momentum']",
    "        self.architecture = ProvidedBlock(num_classes=self.num_classes, in_chans=self.in_channels)",
    "",
    "    def forward(self, x):",
    "        return self.architecture(x)",
    "",
    "    def train_setup(self, prm):",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate, momentum=self.momentum)",
    "        self.criterion = nn.CrossEntropyLoss()",
    "",
    "    def learn(self, data_roll):",
    "        for data, target in data_roll:",
    "            data, target = data.to(self.device), target.to(self.device)",
    "            self.optimizer.zero_grad()",
    "            output = self.forward(data)",
    "            loss = self.criterion(output, target)",
    "            loss.backward()",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)",
    "            self.optimizer.step()",
    "",
    "BLOCK:",
    "```python",
    "{block}",
    "```"
    ]
    }
    }