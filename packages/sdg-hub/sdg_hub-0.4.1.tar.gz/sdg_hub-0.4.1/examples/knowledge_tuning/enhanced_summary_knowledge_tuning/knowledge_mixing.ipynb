{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee50d9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhishek/rotation_project/sdg_hub/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "import polars as pl\n",
    "from transformers import AutoTokenizer\n",
    "from tabulate import tabulate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from knowledge_mixing_utils import sample_doc_qa, generate_knowledge_qa_dataset, count_len_in_tokens, get_avg_summaries_per_raw_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a87eb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment folder: output_data\n",
      "Student model: meta-llama/Llama-3.1-8B-Instruct\n",
      "GPT OSS format: False\n",
      "Cut sizes: [3, 5, 7]\n",
      "Q&A pairs per document: 3\n",
      "Input data directory: output_data\n",
      "Output directory: output_data/training_mix\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Load configuration from environment variables\n",
    "exp_folder = os.getenv('OUTPUT_DATA_FOLDER', 'generated_output_data')\n",
    "student_model = os.getenv('STUDENT_MODEL', 'meta-llama/Llama-3.1-8B-Instruct')\n",
    "save_gpt_oss_format = os.getenv('SAVE_GPT_OSS_FORMAT', 'false').lower() == 'true'\n",
    "\n",
    "# Parse cut sizes from environment variable\n",
    "cut_sizes_str = os.getenv('CUT_SIZES', '10,20')\n",
    "cuts = [int(x.strip()) for x in cut_sizes_str.split(',')]\n",
    "\n",
    "# Get Q&A pairs per document\n",
    "qa_per_doc = int(os.getenv('QA_PER_DOC', '3'))\n",
    "\n",
    "# Define input and output paths relative to exp_folder\n",
    "input_data_dir = os.path.join(exp_folder)\n",
    "output_dir = os.path.join(exp_folder, 'training_mix')\n",
    "\n",
    "print(f\"Experiment folder: {exp_folder}\")\n",
    "print(f\"Student model: {student_model}\")\n",
    "print(f\"GPT OSS format: {save_gpt_oss_format}\")\n",
    "print(f\"Cut sizes: {cuts}\")\n",
    "print(f\"Q&A pairs per document: {qa_per_doc}\")\n",
    "print(f\"Input data directory: {input_data_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68bdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: meta-llama/Llama-3.1-8B-Instruct\n",
      "Loading extractive_summary from: output_data/extractive_summary\n",
      "  Loaded extractive_summary: 451 samples\n",
      "Loading detailed_summary from: output_data/detailed_summary\n",
      "  Loaded detailed_summary: 508 samples\n",
      "Loading key_facts_summary from: output_data/key_facts_summary\n",
      "  Loaded key_facts_summary: 418 samples\n",
      "\n",
      "‚úÖ Successfully loaded 3 summary datasets\n"
     ]
    }
   ],
   "source": [
    "def load_tokenizer(student_model):\n",
    "    \"\"\"Initialize and return tokenizer.\"\"\"\n",
    "    print(f\"Loading tokenizer: {student_model}\")\n",
    "    return AutoTokenizer.from_pretrained(student_model, trust_remote_code=True)\n",
    "\n",
    "\n",
    "def filter_gpt_oss_dataset(ds):\n",
    "    \"\"\"Apply GPT OSS format filtering to dataset.\"\"\"\n",
    "    original_size = len(ds)\n",
    "    \n",
    "    # Filter out problematic questions\n",
    "    ds = ds.filter(\n",
    "        lambda x: '...' not in x['question'] \n",
    "        and '<question>' not in x['question'] \n",
    "        and '<Insert question here>' not in x['question']\n",
    "    )\n",
    "    \n",
    "    # Clean response text\n",
    "    ds = ds.map(\n",
    "        lambda x: {\n",
    "            'response': x['response'].replace('[ANSWER]', '').replace('[END]', '').strip()\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    filtered_size = len(ds)\n",
    "    print(f\"  Filtered {original_size - filtered_size} samples (kept {filtered_size})\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def load_summary_dataset(summary_type):\n",
    "    \"\"\"Load a single summary dataset.\"\"\"\n",
    "    file_path = os.path.join(input_data_dir, f\"{summary_type}\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(file_path).exists():\n",
    "        print(f\"‚ö†Ô∏è  Warning: File not found: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loading {summary_type} from: {file_path}\")\n",
    "    ds = load_dataset('json', data_dir=file_path, split=\"train\")\n",
    "    \n",
    "    # Apply filtering if needed\n",
    "    if save_gpt_oss_format:\n",
    "        ds = filter_gpt_oss_dataset(ds)\n",
    "    \n",
    "    print(f\"  Loaded {summary_type}: {len(ds)} samples\")\n",
    "    return ds.to_polars()\n",
    "\n",
    "\n",
    "def load_all_summary_datasets():\n",
    "    \"\"\"Load all summary type datasets.\"\"\"\n",
    "    summary_types = [\n",
    "        \"extractive_summary\",\n",
    "        \"detailed_summary\", \n",
    "        \"key_facts_to_qa\"\n",
    "    ]\n",
    "    \n",
    "    summary_datasets = {}\n",
    "    \n",
    "    for summary_type in summary_types:\n",
    "        dataset = load_summary_dataset(summary_type)\n",
    "        if dataset is not None:\n",
    "            summary_datasets[summary_type] = dataset\n",
    "    \n",
    "    if not summary_datasets:\n",
    "        raise ValueError(\"No datasets were successfully loaded!\")\n",
    "    \n",
    "    return summary_datasets\n",
    "\n",
    "\n",
    "# Load tokenizer and datasets\n",
    "try:\n",
    "    tokenizer = load_tokenizer(student_model)\n",
    "    summary_datasets = load_all_summary_datasets()\n",
    "     # After loading each dataset\n",
    "\n",
    "    for summary_type, dataset in summary_datasets.items():\n",
    "\n",
    "        print(f\" Columns: {list(dataset.columns)}\")\n",
    "\n",
    "        print(f\" Sample record keys: {list(dataset.head(1).to_dicts()[0].keys())}\")\n",
    "    print(f\"\\n‚úÖ Successfully loaded {len(summary_datasets)} summary datasets\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during initialization: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2121918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Validating cut sizes against available data...\n",
      "\n",
      "üìä Checking extractive_summary:\n",
      "  Cut 3: ‚úÖ Feasible (avg summaries per raw doc: 7.0)\n",
      "  Cut 5: ‚úÖ Feasible (avg summaries per raw doc: 7.0)\n",
      "  Cut 7: ‚úÖ Feasible (avg summaries per raw doc: 7.0)\n",
      "\n",
      "üìä Checking detailed_summary:\n",
      "  Cut 3: ‚úÖ Feasible (avg summaries per raw doc: 7.0)\n",
      "  Cut 5: ‚úÖ Feasible (avg summaries per raw doc: 7.0)\n",
      "  Cut 7: ‚úÖ Feasible (avg summaries per raw doc: 7.0)\n",
      "\n",
      "üìä Checking key_facts_summary:\n",
      "  Cut 3: ‚úÖ Feasible (avg summaries per raw doc: 6.0)\n",
      "  Cut 5: ‚úÖ Feasible (avg summaries per raw doc: 6.0)\n",
      "  Cut 7: ‚ùå Too large (avg summaries per raw doc: 6.0)\n",
      "\n",
      "‚ö†Ô∏è  Removing infeasible cuts: [7]\n",
      "\n",
      "‚úÖ Final feasible cuts: [3, 5]\n",
      "\n",
      "Processing 2 feasible cut sizes...\n",
      "\n",
      "üìä Processing cut size: 3\n",
      "  Processing extractive_summary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhishek/rotation_project/sdg_hub/examples/knowledge_tuning/enhanced_summary_knowledge_tuning/knowledge_mixing_utils.py:196: MapWithoutReturnDtypeWarning: 'return_dtype' of function python_udf must be set\n",
      "\n",
      "A later expression might fail because the output type is not known. Set return_dtype=pl.self_dtype() if the type is unchanged, or set the proper output data type.\n",
      "  knowledge_ds = generated_dataset.with_columns(base_columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Processed 45 samples (3.0 summaries per raw doc)\n",
      "  Processing detailed_summary...\n",
      "    ‚úÖ Processed 45 samples (3.0 summaries per raw doc)\n",
      "  Processing key_facts_summary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhishek/rotation_project/sdg_hub/examples/knowledge_tuning/enhanced_summary_knowledge_tuning/knowledge_mixing_utils.py:196: MapWithoutReturnDtypeWarning: 'return_dtype' of function python_udf must be set\n",
      "\n",
      "A later expression might fail because the output type is not known. Set return_dtype=pl.self_dtype() if the type is unchanged, or set the proper output data type.\n",
      "  knowledge_ds = generated_dataset.with_columns(base_columns)\n",
      "/Users/abhishek/rotation_project/sdg_hub/examples/knowledge_tuning/enhanced_summary_knowledge_tuning/knowledge_mixing_utils.py:196: MapWithoutReturnDtypeWarning: 'return_dtype' of function python_udf must be set\n",
      "\n",
      "A later expression might fail because the output type is not known. Set return_dtype=pl.self_dtype() if the type is unchanged, or set the proper output data type.\n",
      "  knowledge_ds = generated_dataset.with_columns(base_columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Processed 45 samples (3.0 summaries per raw doc)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 34.30ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üíæ Saved to: output_data/training_mix/combined_cut_3x.jsonl\n",
      "  üìà Total samples: 135\n",
      "  üî¢ Total tokens: 393,042\n",
      "  üìã Summary statistics:\n",
      "    extractive_summary: 45 samples, 134,210 tokens\n",
      "    detailed_summary: 45 samples, 107,682 tokens\n",
      "    key_facts_summary: 45 samples, 151,150 tokens\n",
      "\n",
      "üìä Processing cut size: 5\n",
      "  Processing extractive_summary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/abhishek/rotation_project/sdg_hub/examples/knowledge_tuning/enhanced_summary_knowledge_tuning/knowledge_mixing_utils.py:196: MapWithoutReturnDtypeWarning: 'return_dtype' of function python_udf must be set\n",
      "\n",
      "A later expression might fail because the output type is not known. Set return_dtype=pl.self_dtype() if the type is unchanged, or set the proper output data type.\n",
      "  knowledge_ds = generated_dataset.with_columns(base_columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Processed 75 samples (5.0 summaries per raw doc)\n",
      "  Processing detailed_summary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhishek/rotation_project/sdg_hub/examples/knowledge_tuning/enhanced_summary_knowledge_tuning/knowledge_mixing_utils.py:196: MapWithoutReturnDtypeWarning: 'return_dtype' of function python_udf must be set\n",
      "\n",
      "A later expression might fail because the output type is not known. Set return_dtype=pl.self_dtype() if the type is unchanged, or set the proper output data type.\n",
      "  knowledge_ds = generated_dataset.with_columns(base_columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Processed 75 samples (5.0 summaries per raw doc)\n",
      "  Processing key_facts_summary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhishek/rotation_project/sdg_hub/examples/knowledge_tuning/enhanced_summary_knowledge_tuning/knowledge_mixing_utils.py:196: MapWithoutReturnDtypeWarning: 'return_dtype' of function python_udf must be set\n",
      "\n",
      "A later expression might fail because the output type is not known. Set return_dtype=pl.self_dtype() if the type is unchanged, or set the proper output data type.\n",
      "  knowledge_ds = generated_dataset.with_columns(base_columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Processed 75 samples (5.0 summaries per raw doc)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 20.11ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üíæ Saved to: output_data/training_mix/combined_cut_5x.jsonl\n",
      "  üìà Total samples: 225\n",
      "  üî¢ Total tokens: 653,808\n",
      "  üìã Summary statistics:\n",
      "    extractive_summary: 75 samples, 226,191 tokens\n",
      "    detailed_summary: 75 samples, 176,539 tokens\n",
      "    key_facts_summary: 75 samples, 251,078 tokens\n",
      "\n",
      "==================================================\n",
      "üìä FINAL SUMMARY\n",
      "==================================================\n",
      "|   Cut Size |   Total Tokens |   Total Samples |\n",
      "|------------|----------------|-----------------|\n",
      "|          3 |         393042 |             135 |\n",
      "|          5 |         653808 |             225 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def validate_cuts_for_datasets(summary_datasets, cuts):\n",
    "    \"\"\"Validate which cut sizes are feasible for each dataset.\"\"\"\n",
    "    feasible_cuts = set(cuts)\n",
    "\n",
    "    print(\"üîç Validating cut sizes against available data...\")\n",
    "    for summary_type, df in summary_datasets.items():\n",
    "        if summary_type == \"key_facts_to_qa\":\n",
    "            print(f\"\\nüìä Skipping {summary_type}:\")\n",
    "            continue\n",
    "        print(f\"\\nüìä Checking {summary_type}:\")\n",
    "\n",
    "        for cut in cuts:\n",
    "            avg_summaries = get_avg_summaries_per_raw_doc(df)\n",
    "            is_feasible = avg_summaries >= cut\n",
    "            status = \"‚úÖ Feasible\" if is_feasible else \"‚ùå Too large\"\n",
    "            print(f\"  Cut {cut}: {status} (avg summaries per raw doc: {avg_summaries:.1f})\")\n",
    "\n",
    "            if not is_feasible:\n",
    "                feasible_cuts.discard(cut)\n",
    "\n",
    "    final_cuts = sorted(list(feasible_cuts))\n",
    "    if len(final_cuts) < len(cuts):\n",
    "        removed_cuts = set(cuts) - feasible_cuts\n",
    "        print(f\"\\n‚ö†Ô∏è  Removing infeasible cuts: {sorted(list(removed_cuts))}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Final feasible cuts: {final_cuts}\")\n",
    "    return final_cuts\n",
    "\n",
    "\n",
    "def process_single_summary_type(summary_type, df, cut, tokenizer, qa_per_doc):\n",
    "    \"\"\"Process a single summary type dataset.\"\"\"\n",
    "    try:\n",
    "        print(f\"  Processing {summary_type}...\")\n",
    "        if summary_type == \"key_facts_to_qa\":\n",
    "            # Skip the sampling step for keys facts QA dataset as we discard the generated summary and only keep the qa pairs\n",
    "            # Generate knowledge Q&A dataset\n",
    "            generated_dataset = generate_knowledge_qa_dataset(\n",
    "                df,\n",
    "                keep_columns=[\"question\", \"document_outline\", 'raw_document', 'document'],\n",
    "                pre_training=True,\n",
    "                keep_document_in_context=False\n",
    "            )\n",
    "        else:\n",
    "            # Sample documents and Q&A pairs (validation already done)\n",
    "            df_cut = sample_doc_qa(df, n_docs_per_raw=cut, qa_per_doc=qa_per_doc)\n",
    "\n",
    "            # Generate knowledge Q&A dataset\n",
    "            generated_dataset = generate_knowledge_qa_dataset(\n",
    "                df_cut,\n",
    "                keep_columns=[\"question\", \"document_outline\", 'raw_document', 'document'],\n",
    "                pre_training=True,\n",
    "                keep_document_in_context=True\n",
    "            )\n",
    "\n",
    "        # Count tokens\n",
    "        generated_dataset = count_len_in_tokens(generated_dataset, tokenizer)\n",
    "\n",
    "        # Convert back to HuggingFace dataset\n",
    "        generated_dataset = Dataset.from_polars(generated_dataset)\n",
    "\n",
    "        # Calculate statistics\n",
    "        unique_docs = len(set(generated_dataset['document']))\n",
    "        unique_raw_docs = len(set(generated_dataset['raw_document']))\n",
    "        generated_cut_size = unique_docs / unique_raw_docs if unique_raw_docs > 0 else 0\n",
    "\n",
    "        stats = {\n",
    "            'samples': len(generated_dataset),\n",
    "            'unique_docs': unique_docs,\n",
    "            'unique_raw_docs': unique_raw_docs,\n",
    "            'avg_docs_per_raw': generated_cut_size,\n",
    "            'total_tokens': sum(generated_dataset['token_length'])\n",
    "        }\n",
    "\n",
    "        print(f\"    ‚úÖ Processed {len(generated_dataset)} samples ({generated_cut_size:.1f} summaries per raw doc)\")\n",
    "        return generated_dataset, stats\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ùå Error processing {summary_type}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def combine_and_save_datasets(all_datasets, cut_stats, cut, output_dir):\n",
    "    \"\"\"Combine datasets and save to file.\"\"\"\n",
    "    if not all_datasets:\n",
    "        print(f\"  ‚ùå No datasets processed for cut size {cut}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Combine all summary types for this cut\n",
    "        combined_dataset = concatenate_datasets(all_datasets)\n",
    "        total_tokens = sum(combined_dataset['token_length'])\n",
    "\n",
    "        # Save combined dataset\n",
    "        output_path = os.path.join(output_dir, f\"combined_cut_{cut}x.jsonl\")\n",
    "        combined_dataset.to_json(output_path, orient=\"records\", lines=True)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"  üíæ Saved to: {output_path}\")\n",
    "        print(f\"  üìà Total samples: {len(combined_dataset)}\")\n",
    "        print(f\"  üî¢ Total tokens: {total_tokens:,}\")\n",
    "\n",
    "        # Print detailed statistics\n",
    "        print(f\"  üìã Summary statistics:\")\n",
    "        for summary_type, stats in cut_stats.items():\n",
    "            print(f\"    {summary_type}: {stats['samples']} samples, {stats['total_tokens']:,} tokens\")\n",
    "\n",
    "        return (cut, total_tokens, len(combined_dataset))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error combining datasets for cut {cut}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_single_cut(cut, summary_datasets, tokenizer, output_dir, qa_per_doc):\n",
    "    \"\"\"Process all summary types for a single cut size.\"\"\"\n",
    "    print(f\"\\nüìä Processing cut size: {cut}\")\n",
    "    all_datasets = []\n",
    "    cut_stats = {}\n",
    "\n",
    "    for summary_type, df in summary_datasets.items():\n",
    "        dataset, stats = process_single_summary_type(summary_type, df, cut, tokenizer, qa_per_doc)\n",
    "\n",
    "        if dataset is not None and stats is not None:\n",
    "            all_datasets.append(dataset)\n",
    "            cut_stats[summary_type] = stats\n",
    "\n",
    "    return combine_and_save_datasets(all_datasets, cut_stats, cut, output_dir)\n",
    "\n",
    "\n",
    "def process_and_mix_datasets(cuts, summary_datasets, tokenizer, output_dir, qa_per_doc):\n",
    "    \"\"\"Process and mix datasets with different cut sizes.\"\"\"\n",
    "    # First validate which cuts are feasible\n",
    "    feasible_cuts = validate_cuts_for_datasets(summary_datasets, cuts)\n",
    "\n",
    "    if not feasible_cuts:\n",
    "        print(\"\\n‚ùå No feasible cuts found! Check your data or reduce cut sizes.\")\n",
    "        return []\n",
    "\n",
    "    token_count = []\n",
    "\n",
    "    print(f\"\\nProcessing {len(feasible_cuts)} feasible cut sizes...\")\n",
    "    for cut in feasible_cuts:\n",
    "        result = process_single_cut(cut, summary_datasets, tokenizer, output_dir, qa_per_doc)\n",
    "        if result is not None:\n",
    "            token_count.append(result)\n",
    "\n",
    "    return token_count\n",
    "\n",
    "\n",
    "def print_final_summary(token_count):\n",
    "    \"\"\"Print final summary table.\"\"\"\n",
    "    if token_count:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"üìä FINAL SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        print(tabulate(\n",
    "            token_count,\n",
    "            headers=[\"Cut Size\", \"Total Tokens\", \"Total Samples\"],\n",
    "            tablefmt=\"github\",\n",
    "            numalign=\"right\"\n",
    "        ))\n",
    "    else:\n",
    "        print(\"\\n‚ùå No datasets were successfully processed!\")\n",
    "\n",
    "\n",
    "# Process datasets\n",
    "token_count = process_and_mix_datasets(cuts, summary_datasets, tokenizer, output_dir, qa_per_doc)\n",
    "\n",
    "# Print final summary\n",
    "print_final_summary(token_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
