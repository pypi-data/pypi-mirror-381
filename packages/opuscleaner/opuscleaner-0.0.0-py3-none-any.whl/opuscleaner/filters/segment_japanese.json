{
    "description": "Segments Japanese text using the fugashi tokenizer. Note the specifics of Japanese tokenization, where verbs are always separate to stem and  conjugation part, as well topic or subject particles are split from the nouns. This means that the Japanese sentences would likely be quite a bit longer than the English ones.",
    "type": "monolingual",
    "command": "./segment_japanese.py",
    "parameters": {}
}
