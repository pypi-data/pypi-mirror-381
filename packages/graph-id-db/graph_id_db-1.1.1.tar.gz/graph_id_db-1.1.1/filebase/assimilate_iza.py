#!/usr/bin/env python
import argparse
import hashlib
import json
import re
import glob
from collections import defaultdict
from pathlib import Path
from typing import Dict, Tuple

from pymatgen.core import Structure


def compute_file_hash(file_path: str) -> str:
    """Compute SHA-256 hash of a file."""
    sha256_hash = hashlib.sha256()
    with open(file_path, "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()


def extract_source_info(file_path: str) -> Tuple[str, str, str]:
    """
    Extract source information from file path.
    Returns (source, proprietary_id, url) tuple.
    """
    # Extract source from directory structure
    # path_parts = Path(file_path).parts
    source = "unknown"
    proprietary_id = ""
    url = ""

    # Try to determine source from path
    if "iza" in str(file_path).lower():
        source = "iza"
        # Extract IZA code if present
        match = re.search(r"([A-Z]{3,4})\.cif", file_path)
        if match:
            proprietary_id = match.group(1)
            url = f"http://www.iza-structure.org/databases/{proprietary_id}"
    elif (
        "mp" in str(file_path).lower() or "materials_project" in str(file_path).lower()
    ):
        source = "mp"
        # Extract MP ID if present
        match = re.search(r"(mp-\d+)", file_path)
        if match:
            proprietary_id = match.group(1)
            url = f"https://materialsproject.org/materials/{proprietary_id}"

    return source, proprietary_id, url


def extract_source_info_from_json(file_path: str) -> Tuple[str, str, str, str]:
    """
    Extract source information from JSON file path.
    Returns (source, proprietary_id, url) tuple.
    """
    # Extract source from directory structure
    # path_parts = Path(file_path).parts
    json_file = open(file_path, "r")
    dict_from_json = json.load(json_file)

    # source = ""
    # proprietary_id = ""
    # graph_id = ""
    # url = ""
    # if source == "International Zeolite Association":
    #     source = dict_from_json["data_source"]
    #     proprietary_id = dict_from_json["data_source_id"]
    #     graph_id = dict_from_json["md_id"]
    #     url = dict_from_json["data_source_url"]
    # else:
    #     pass
    source = dict_from_json["data_source"]
    proprietary_id = dict_from_json["data_source_id"]
    graph_id = dict_from_json["md_id"]
    url = dict_from_json["data_source_url"]

    return source, proprietary_id, graph_id, url


def get_composition_hash(structure: Structure) -> str:
    """Get the first two characters of the composition hash."""
    comp = structure.composition
    comp_str = str(comp)
    comp_hash = hashlib.sha256(comp_str.encode()).hexdigest()
    return comp_hash[:2]


def assimilate_json_files(input_dir: str, output_dir: str) -> None:
    """
    Assimilate JSON files generated by compute.py and construct the file database.

    Args:
        input_dir: Directory containing JSON files
        output_dir: Directory to write the database
    """
    input_dir_path = Path(input_dir)
    output_dir_path = Path(output_dir)

    # Find all JSON files
    # json_files = list(input_dir_path.glob("*.json"))
    json_files = glob.glob("/home/tanimoto/graphid-db/id_jsons/iza/*.json")
    print(f"Found {len(json_files)} JSON files")
    # cif_files = list(input_dir_path.glob("cifs/*.cif"))
    cif_files = glob.glob("/home/tanimoto/graphid-db/cifs/iza/*.cif")
    print(f"Found {len(cif_files)} CIF files")
    # if len(json_files) != len(cif_files):
    #     return

    # Dictionary to store entries by composition hash
    entries_by_comp_str: Dict[str, Dict[str, list[dict]]] = defaultdict(dict)

    # Process each JSON file
    # for json_file in json_files:python
    # for json_file, cif_file in zip(json_files, cif_files):
    for json_file in json_files:
        base_name = json_file.replace("/home/tanimoto/graphid-db/id_jsons/iza/", "").replace(".json", "")
        cif_file = f"/home/tanimoto/graphid-db/cifs/iza/{base_name}.cif"
        # Compute file hash
        file_hash = compute_file_hash(str(cif_file))
        # file_hash_prefix = file_hash[:2]

        # Extract source information
        source, proprietary_id, graph_id, url = extract_source_info_from_json(
            str(json_file)
        )
        structure = Structure.from_file(cif_file)

        # Load graph IDs from JSON
        try:
            with open(json_file, "r") as f:
                graph_ids = json.load(f)
        except Exception as e:
            print(f"Error loading JSON from {json_file}: {e}")
            continue

        # Create entry for each graph ID
        for graph_id_type, graph_id_value in graph_ids.items():
            if graph_id_type not in ["md_id", "topo_md_id", "dc_id"]:
                continue

            if not graph_id_value:  # Skip empty graph IDs
                continue

            # Create entry
            entry = {
                "graph_id_type": graph_id_type,
                "proprietary_id": proprietary_id,
                "datasource": source,
                "url": url,
                "filehash": file_hash,
            }

            # reduced_comp_str = str(structure.composition.reduced_composition).replace(" ", "")
            comp_str = str(structure.composition).replace(" ", "")

            # Add to entries dictionary
            if graph_id_value in entries_by_comp_str[comp_str].keys():
                entries_by_comp_str[comp_str][graph_id_value].append(entry)
            else:
                entries_by_comp_str[comp_str][graph_id_value] = [entry]

    # Create output directory structure and write entries.json files
    # Create directory: output_dir_path/reduced_composition_string/composition_string.json
    dir_path = output_dir_path / "SiO2"
    dir_path.mkdir(parents=True, exist_ok=True)
    for comp_str, _entries in entries_by_comp_str.items():
        entries_assimilate_by_comp_str: Dict[str, list[Dict[str, str]]] = defaultdict(
            list
        )
        for graph_id_value, entries in _entries.items():
            entries_assimilate_by_comp_str[graph_id_value] = entries

        # Write entries.json
        entries_file = dir_path / f"{comp_str}.json"
        with open(entries_file, "w") as f:
            json.dump(entries_assimilate_by_comp_str, f, indent=2)

        print(f"Wrote {len(_entries)} entries to {entries_file}")


def main():
    parser = argparse.ArgumentParser(
        description="Assimilate JSON files and construct file database"
    )
    parser.add_argument(
        "--input-dir", type=str, required=True, help="Directory containing input files"
    )
    parser.add_argument(
        "--output-dir", type=str, required=True, help="Directory to write the database"
    )
    args = parser.parse_args()

    assimilate_json_files(args.input_dir, args.output_dir)


if __name__ == "__main__":
    main()
