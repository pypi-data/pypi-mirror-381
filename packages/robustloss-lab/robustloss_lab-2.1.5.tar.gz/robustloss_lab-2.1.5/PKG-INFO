Metadata-Version: 2.4
Name: robustloss-lab
Version: 2.1.5
Summary: Robust classification losses (CCE/SCCE, Focal, GCE) + simple experiment runner
Author-email: RosePasta22 <danieljoo04@gmail.com>
License: MIT
Project-URL: Homepage, https://github.com/RosePasta22/ML-DL-Seminar
Keywords: robust,loss,noisy labels,outliers,ml,pytorch
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy>=1.23
Requires-Dist: pandas>=2.0
Requires-Dist: scikit-learn>=1.2
Requires-Dist: matplotlib>=3.7
Provides-Extra: dev
Requires-Dist: pytest; extra == "dev"
Requires-Dist: ruff; extra == "dev"
Requires-Dist: build; extra == "dev"
Requires-Dist: twine; extra == "dev"
Dynamic: license-file

ï»¿# robustloss-lab

Robust classification loss toolkit (CCE/SCCE, Focal, GCE) with simple experiment runners for noisy labels and outliers.  
ë…¸ì´ì¦ˆ/ì•„ì›ƒë¼ì´ì–´ í™˜ê²½ì—ì„œ ê°•ê±´í•œ ë¶„ë¥˜ ì‹¤í—˜ì„ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ì‹¤í—˜ ëŸ¬ë„ˆë¥¼ ì œê³µí•©ë‹ˆë‹¤.

---

## âœ¨ Features
- Losses: CE, GCE, Focal, **CCE**, **SCCE**
- Experiment runners: `run_experiment`, `run_clean_vs_noise`, `run_clean_vs_outlier`
- Noise / Outlier injection utilities and metadata logging
- Minimal, sklearn-like workflow with PyTorch backend

---

## ğŸ“¦ Install

> **Note**: PyTorchëŠ” CUDA/CPU í™˜ê²½ì— ë§ì¶° ë³„ë„ ì„¤ì¹˜í•˜ì„¸ìš”.  
> ì˜ˆ:  
> ```bash
> pip install torch --index-url https://download.pytorch.org/whl/cu121
> ```

```bash
pip install robustloss-lab
```

---

## ğŸš€ Quick start

```python
from robustloss import (
    DatasetSchema, TaskType,
    make_loss, run_experiment, plot_history,
    NoiseConfig, OutlierConfig, pct_drop
)

# 1) Define dataset schema
schema = DatasetSchema(
    name="uci_wine",
    target_name="class",
    task_type=TaskType.MULTICLASS
)

# 2) Choose a loss (e.g., SCCE)
loss_fn = make_loss("scce", eps=1e-3)

# 3) Run a quick experiment
model, hist, report = run_experiment(
    df, schema,
    loss_fn=loss_fn, loss_name="SCCE",
    epochs=50, batch_size=64, lr=1e-3, weight_decay=1e-4,
    optimizer_name="adam", seed=42
)

print(report)  # dict(test_acc=..., test_f1=..., noise_meta=..., outlier_meta=...)
plot_history([hist], ["SCCE"])
```

---

## ğŸ“‚ Modules

**Core (required submodules)**  
- `schemas.py` â€” ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜  
- `preprocess.py`, `datamod.py` â€” ì „ì²˜ë¦¬ / split  
- `loss_functions.py` â€” CE, GCE, Focal, CCE, SCCE  
- `models.py` â€” Logistic/Softmax linear classifiers  
- `train_many.py` â€” í•™ìŠµ ë£¨í”„, early stopping, ì‹œê°í™”  

**Optional submodules**  
- `noise_types.py`, `apply_noise.py` â€” ë¼ë²¨/í”¼ì²˜ ë…¸ì´ì¦ˆ  
- `outliers.py`, `apply_outliers.py` â€” ì•„ì›ƒë¼ì´ì–´ ìƒì„±/ì£¼ì…  

> ì„ íƒ ëª¨ë“ˆì€ í™˜ê²½ì— ë”°ë¼ ë¯¸í¬í•¨ì¼ ìˆ˜ ìˆìœ¼ë©°, íŒ¨í‚¤ì§€ import ì‹œ `None`ìœ¼ë¡œ ë°”ì¸ë”©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ§© API Sketch

### `DatasetSchema`
```python
@dataclass(frozen=True, slots=True)
class DatasetSchema:
    name: str
    target_name: str
    task_type: Optional[TaskType] = None
    numeric_features: Optional[Sequence[str]] = None
    categorical_features: Optional[Sequence[str]] = None
    drop_features: Sequence[str] = field(default_factory=tuple)
```

### `NoiseConfig`
```python
@dataclass(slots=True)
class NoiseConfig:
    kind: Literal["none", "label", "feature", "both"] = "none"
    label_mode: Optional[LabelMode] = None
    label_rate: float = 0.0
    seed_label: Optional[int] = None
    feature_mode: Optional[FeatureMode] = None
    seed_feature: Optional[int] = None
    feature_frac: float = 0.0
    feature_scale: float = 0.0
    spike_frac: float = 0.0
    spike_value: float = 10.0
```

### `OutlierConfig`
```python
@dataclass(frozen=True, slots=True)
class OutlierConfig:
    spike_value: float = 10.0
    rate: float = 0.1
    zmin: float = 3.0
    zmax: float = 5.0
    mmin: int = 1
    mmax: Optional[int] = None
    two_side: bool = True
    seed_outlier: Optional[int] = 42
    target: Iterable[str] = ("train",)
```

### Experiment Runners
```python
model, hist, report = run_experiment(df, schema, loss_fn, ...)

[h_c, h_n], labels, df_res = run_clean_vs_noise(df, schema, loss_fn=loss_fn, ...)

[h_c, h_o], labels, df_res = run_clean_vs_outlier(df, schema, loss_fn=loss_fn, ...)
```

### Metadata
- `noise_meta`: ë¼ë²¨/í”¼ì²˜ ë…¸ì´ì¦ˆ ì ìš© ì •ë³´ (ì „ì´í–‰ë ¬, ì¸ë±ìŠ¤, ì‹œë“œ ë“±)  
- `outlier_meta`: ì•„ì›ƒë¼ì´ì–´ ì£¼ì… ìš”ì•½ (ë¹„ìœ¨, |z|-í†µê³„, më²”ìœ„/ì‹œë“œ ë“±)

---

## ğŸ“ Patch Notes
- 1.0.0: First release  
- 2.0.x: ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª¨ë“ˆí™”, ë…¸ì´ì¦ˆ ê¸°ëŠ¥ ì¶”ê°€  
- 2.1.0: Outliers ëª¨ë“ˆ ì¶”ê°€  
- 2.1.4: SCCE clamp ì¬ì •ì˜(q_t)  
- **2.1.5: PyPI íŒ¨í‚¤ì§• ì •ë¦¬, README ê°œì„ , ë°°í¬ëª… `robustloss-lab`, import `robustloss`**

---

## ğŸ“ Links
- [Latest Release](https://github.com/RosePasta22/ML-DL-Seminar/releases/tag/v2.1.5)  
- [Homepage / Source](https://github.com/RosePasta22/ML-DL-Seminar)
