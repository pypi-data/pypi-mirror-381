import litellm
import logging
import time
import json
from pydantic import BaseModel, Field, ValidationError
from typing import List, Dict, Any

from ..base.evaluator import Evaluator
from ..base.base_optimizer import BaseOptimizer
from ..base.base_generator import BaseGenerator

from ..datamappers import BasicDataMapper
from ..types import IterationHistory, OptimizationResult

logger = logging.getLogger(__name__)


class PromptVariations(BaseModel):
    """Output model for prompt variations generated by the teacher model."""

    variations: List[str] = Field(
        description="A list of prompt variations.",
    )


class RandomSearchOptimizer(BaseOptimizer):
    """
    A simple optimization strategy that tries a number of random prompt variations
    generated by a powerful "teacher" model.
    """

    def __init__(
        self,
        generator: BaseGenerator,
        teacher_model: str = "gpt-5",
        num_variations: int = 5,
        teacher_model_kwargs: Dict[str, Any] = {},
        eval_template: str = "summary_quality",
        eval_model_name: str = "turing_flash",
    ):
        """
        Initializes the Random Search Optimizer.

        Args:
            generator: The generator to be optimized.
            teacher_model: A powerful LLM to generate prompt variations.
            num_variations: The number of random variations to try.
            teacher_model_kwargs: A dictionary of kwargs to pass to the teacher model.
            eval_template: The evaluation template to use from the ai-evaluation library.
            eval_model_name: The model to use for evaluation.
        """
        self.generator = generator
        self.teacher_model = teacher_model
        self.num_variations = num_variations

        if teacher_model_kwargs is None and "gpt" in teacher_model:
            self.teacher_model_kwargs = {"temperature": 1.0, "max_tokens": 16000}
        else:
            self.teacher_model_kwargs = teacher_model_kwargs or {}

        super().__init__()

    def optimize(
        self,
        evaluator: Evaluator,
        data_mapper: BasicDataMapper,
        dataset: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> OptimizationResult:
        logger.info("--- Starting Random Search Optimization ---")
        optimization_start_time = time.time()

        initial_prompt = self.generator.get_prompt_template()
        best_prompt = initial_prompt
        best_score = -1.0
        history = []

        variations = self._generate_variations(initial_prompt)

        for i, variation in enumerate(variations):
            iteration_start_time = time.time()
            logger.info(f"--- Testing Variation {i + 1}/{len(variations)} ---")
            logger.info(f"Prompt: {variation}")
            self.generator.set_prompt_template(variation)

            # Time generation
            generation_start_time = time.time()
            generated_outputs = [
                self.generator.generate(example) for example in dataset
            ]
            generation_end_time = time.time()
            logger.info(
                f"Generation for {len(dataset)} examples took {generation_end_time - generation_start_time:.2f} seconds."
            )

            eval_inputs = [
                data_mapper.map(gen_output, example)
                for gen_output, example in zip(generated_outputs, dataset)
            ]

            # Time evaluation
            evaluation_start_time = time.time()
            evaluation_results = evaluator.evaluate(eval_inputs)
            evaluation_end_time = time.time()
            logger.info(
                f"Evaluation for {len(dataset)} examples took {evaluation_end_time - evaluation_start_time:.2f} seconds."
            )

            if not evaluation_results:
                logger.warning(
                    "No evaluation results were returned for this variation."
                )
                continue

            # Calculate average score for decision-making
            avg_score = sum(res.score for res in evaluation_results) / len(
                evaluation_results
            )
            logger.info(f"Average Score for Variation {i + 1}: {avg_score:.4f}")

            history.append(
                IterationHistory(
                    prompt=variation,
                    average_score=avg_score,
                    individual_results=evaluation_results,
                )
            )

            if avg_score > best_score:
                best_score = avg_score
                best_prompt = variation
                logger.info("--- New Best Prompt Found! ---")

            iteration_end_time = time.time()
            logger.info(
                f"--- Variation {i + 1} finished in {iteration_end_time - iteration_start_time:.2f} seconds ---"
            )

        self.generator.set_prompt_template(best_prompt)

        optimization_end_time = time.time()
        logger.info(
            f"--- Random Search Optimization finished in {optimization_end_time - optimization_start_time:.2f} seconds ---"
        )

        return OptimizationResult(
            best_generator=self.generator, history=history, final_score=best_score
        )

    def _generate_variations(self, initial_prompt: str) -> List[str]:
        # this entire thing should ideally be sent to the generator itself.
        logger.info(
            f"Generating {self.num_variations} prompt variations with teacher model: {self.teacher_model}..."
        )

        instruction = f"""
        You are an expert in prompt engineering. Your task is to generate {self.num_variations} variations of the following prompt.
        The variations should be diverse and explore different styles.
        Return ONLY a JSON object with a key "variations" containing a list of strings. For example:
        {{"variations": ["prompt 1", "prompt 2", ...]}}

        Initial Prompt:
        ---
        {initial_prompt}
        ---
        """
        messages = [{"role": "user", "content": instruction}]
        response_content = ""

        try:
            teacher_model_kwargs = self.teacher_model_kwargs.copy()
            teacher_model_kwargs["response_format"] = {"type": "json_object"}

            response = litellm.completion(
                model=self.teacher_model,
                messages=messages,
                **teacher_model_kwargs,
            )
            response_content = response.choices[0].message.content
            # logger.info(f"Teacher model response:\n{response_content}")

            # Clean the response content to ensure it is valid JSON
            # Sometimes models return JSON wrapped in ```json ... ```
            if response_content.strip().startswith("```json"):
                response_content = response_content.strip()[7:-3].strip()

            prompt_variations = PromptVariations.model_validate_json(response_content)
            return prompt_variations.variations

        except (ValidationError, json.JSONDecodeError) as e:
            logger.error(f"Failed to parse teacher model response: {e}")
        except Exception as e:
            logger.error(f"An error occurred with the teacher model: {e}")

        logger.error(f"Problematic response content was: {response_content}")
        return [initial_prompt + " Be creative."]
