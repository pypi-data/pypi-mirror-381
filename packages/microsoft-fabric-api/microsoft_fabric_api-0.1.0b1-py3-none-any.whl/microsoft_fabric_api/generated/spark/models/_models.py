# coding=utf-8
# pylint: disable=too-many-lines
# --------------------------------------------------------------------------
# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.10.3, generator: @autorest/python@6.15.0)
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------

import datetime
from typing import Any, List, Optional, TYPE_CHECKING, Union

from .. import _serialization

if TYPE_CHECKING:
    # pylint: disable=unused-import,ungrouped-imports
    from .. import models as _models


class AutomaticLogProperties(_serialization.Model):
    """Automatic Log Properties.

    All required parameters must be populated in order to send to server.

    :ivar enabled: The status of the automatic log. False - Disabled, true - Enabled. Required.
    :vartype enabled: bool
    """

    _validation = {
        "enabled": {"required": True},
    }

    _attribute_map = {
        "enabled": {"key": "enabled", "type": "bool"},
    }

    def __init__(self, *, enabled: bool, **kwargs: Any) -> None:
        """
        :keyword enabled: The status of the automatic log. False - Disabled, true - Enabled. Required.
        :paramtype enabled: bool
        """
        super().__init__(**kwargs)
        self.enabled = enabled


class AutoScaleProperties(_serialization.Model):
    """Autoscale properties.

    All required parameters must be populated in order to send to server.

    :ivar enabled: The status of the auto scale. False - Disabled, true - Enabled. Required.
    :vartype enabled: bool
    :ivar min_node_count: The minimum node count. Required.
    :vartype min_node_count: int
    :ivar max_node_count: The maximum node count. Required.
    :vartype max_node_count: int
    """

    _validation = {
        "enabled": {"required": True},
        "min_node_count": {"required": True, "minimum": 1},
        "max_node_count": {"required": True, "minimum": 1},
    }

    _attribute_map = {
        "enabled": {"key": "enabled", "type": "bool"},
        "min_node_count": {"key": "minNodeCount", "type": "int"},
        "max_node_count": {"key": "maxNodeCount", "type": "int"},
    }

    def __init__(self, *, enabled: bool, min_node_count: int, max_node_count: int, **kwargs: Any) -> None:
        """
        :keyword enabled: The status of the auto scale. False - Disabled, true - Enabled. Required.
        :paramtype enabled: bool
        :keyword min_node_count: The minimum node count. Required.
        :paramtype min_node_count: int
        :keyword max_node_count: The maximum node count. Required.
        :paramtype max_node_count: int
        """
        super().__init__(**kwargs)
        self.enabled = enabled
        self.min_node_count = min_node_count
        self.max_node_count = max_node_count


class CreateCustomPoolRequest(_serialization.Model):
    """Create custom pool request payload.

    All required parameters must be populated in order to send to server.

    :ivar name: Custom pool name.:code:`<br>`The name must be between 1 and 64 characters long and
     must contain only letters, numbers, dashes, underscores and spaces.:code:`<br>`Custom pool
     names must be unique within the workspace.:code:`<br>`"Starter Pool" is a reserved custom pool
     name. Required.
    :vartype name: str
    :ivar node_family: Node family. Required. "MemoryOptimized"
    :vartype node_family: str or ~microsoft.fabric.api.spark.models.NodeFamily
    :ivar node_size: Node size. Required. Known values are: "Small", "Medium", "Large", "XLarge",
     and "XXLarge".
    :vartype node_size: str or ~microsoft.fabric.api.spark.models.NodeSize
    :ivar auto_scale: Autoscale. Required.
    :vartype auto_scale: ~microsoft.fabric.api.spark.models.AutoScaleProperties
    :ivar dynamic_executor_allocation: Dynamic executor allocation. Required.
    :vartype dynamic_executor_allocation:
     ~microsoft.fabric.api.spark.models.DynamicExecutorAllocationProperties
    """

    _validation = {
        "name": {"required": True},
        "node_family": {"required": True},
        "node_size": {"required": True},
        "auto_scale": {"required": True},
        "dynamic_executor_allocation": {"required": True},
    }

    _attribute_map = {
        "name": {"key": "name", "type": "str"},
        "node_family": {"key": "nodeFamily", "type": "str"},
        "node_size": {"key": "nodeSize", "type": "str"},
        "auto_scale": {"key": "autoScale", "type": "AutoScaleProperties"},
        "dynamic_executor_allocation": {
            "key": "dynamicExecutorAllocation",
            "type": "DynamicExecutorAllocationProperties",
        },
    }

    def __init__(
        self,
        *,
        name: str,
        node_family: Union[str, "_models.NodeFamily"],
        node_size: Union[str, "_models.NodeSize"],
        auto_scale: "_models.AutoScaleProperties",
        dynamic_executor_allocation: "_models.DynamicExecutorAllocationProperties",
        **kwargs: Any
    ) -> None:
        """
        :keyword name: Custom pool name.:code:`<br>`The name must be between 1 and 64 characters long
         and must contain only letters, numbers, dashes, underscores and spaces.:code:`<br>`Custom pool
         names must be unique within the workspace.:code:`<br>`"Starter Pool" is a reserved custom pool
         name. Required.
        :paramtype name: str
        :keyword node_family: Node family. Required. "MemoryOptimized"
        :paramtype node_family: str or ~microsoft.fabric.api.spark.models.NodeFamily
        :keyword node_size: Node size. Required. Known values are: "Small", "Medium", "Large",
         "XLarge", and "XXLarge".
        :paramtype node_size: str or ~microsoft.fabric.api.spark.models.NodeSize
        :keyword auto_scale: Autoscale. Required.
        :paramtype auto_scale: ~microsoft.fabric.api.spark.models.AutoScaleProperties
        :keyword dynamic_executor_allocation: Dynamic executor allocation. Required.
        :paramtype dynamic_executor_allocation:
         ~microsoft.fabric.api.spark.models.DynamicExecutorAllocationProperties
        """
        super().__init__(**kwargs)
        self.name = name
        self.node_family = node_family
        self.node_size = node_size
        self.auto_scale = auto_scale
        self.dynamic_executor_allocation = dynamic_executor_allocation


class CustomPool(_serialization.Model):
    """Custom pool.

    :ivar id: Custom pool ID.
    :vartype id: str
    :ivar name: Custom pool name.
    :vartype name: str
    :ivar type: Custom pool type. Known values are: "Workspace" and "Capacity".
    :vartype type: str or ~microsoft.fabric.api.spark.models.CustomPoolType
    :ivar node_family: Node family. "MemoryOptimized"
    :vartype node_family: str or ~microsoft.fabric.api.spark.models.NodeFamily
    :ivar node_size: Node size. Known values are: "Small", "Medium", "Large", "XLarge", and
     "XXLarge".
    :vartype node_size: str or ~microsoft.fabric.api.spark.models.NodeSize
    :ivar auto_scale: Autoscale.
    :vartype auto_scale: ~microsoft.fabric.api.spark.models.AutoScaleProperties
    :ivar dynamic_executor_allocation: Dynamic executor allocation.
    :vartype dynamic_executor_allocation:
     ~microsoft.fabric.api.spark.models.DynamicExecutorAllocationProperties
    """

    _attribute_map = {
        "id": {"key": "id", "type": "str"},
        "name": {"key": "name", "type": "str"},
        "type": {"key": "type", "type": "str"},
        "node_family": {"key": "nodeFamily", "type": "str"},
        "node_size": {"key": "nodeSize", "type": "str"},
        "auto_scale": {"key": "autoScale", "type": "AutoScaleProperties"},
        "dynamic_executor_allocation": {
            "key": "dynamicExecutorAllocation",
            "type": "DynamicExecutorAllocationProperties",
        },
    }

    def __init__(
        self,
        *,
        id: Optional[str] = None,  # pylint: disable=redefined-builtin
        name: Optional[str] = None,
        type: Optional[Union[str, "_models.CustomPoolType"]] = None,
        node_family: Optional[Union[str, "_models.NodeFamily"]] = None,
        node_size: Optional[Union[str, "_models.NodeSize"]] = None,
        auto_scale: Optional["_models.AutoScaleProperties"] = None,
        dynamic_executor_allocation: Optional["_models.DynamicExecutorAllocationProperties"] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword id: Custom pool ID.
        :paramtype id: str
        :keyword name: Custom pool name.
        :paramtype name: str
        :keyword type: Custom pool type. Known values are: "Workspace" and "Capacity".
        :paramtype type: str or ~microsoft.fabric.api.spark.models.CustomPoolType
        :keyword node_family: Node family. "MemoryOptimized"
        :paramtype node_family: str or ~microsoft.fabric.api.spark.models.NodeFamily
        :keyword node_size: Node size. Known values are: "Small", "Medium", "Large", "XLarge", and
         "XXLarge".
        :paramtype node_size: str or ~microsoft.fabric.api.spark.models.NodeSize
        :keyword auto_scale: Autoscale.
        :paramtype auto_scale: ~microsoft.fabric.api.spark.models.AutoScaleProperties
        :keyword dynamic_executor_allocation: Dynamic executor allocation.
        :paramtype dynamic_executor_allocation:
         ~microsoft.fabric.api.spark.models.DynamicExecutorAllocationProperties
        """
        super().__init__(**kwargs)
        self.id = id
        self.name = name
        self.type = type
        self.node_family = node_family
        self.node_size = node_size
        self.auto_scale = auto_scale
        self.dynamic_executor_allocation = dynamic_executor_allocation


class PaginatedResponse(_serialization.Model):
    """PaginatedResponse.

    :ivar continuation_token: The token for the next result set batch. If there are no more
     records, it's removed from the response.
    :vartype continuation_token: str
    :ivar continuation_uri: The URI of the next result set batch. If there are no more records,
     it's removed from the response.
    :vartype continuation_uri: str
    """

    _attribute_map = {
        "continuation_token": {"key": "continuationToken", "type": "str"},
        "continuation_uri": {"key": "continuationUri", "type": "str"},
    }

    def __init__(
        self, *, continuation_token: Optional[str] = None, continuation_uri: Optional[str] = None, **kwargs: Any
    ) -> None:
        """
        :keyword continuation_token: The token for the next result set batch. If there are no more
         records, it's removed from the response.
        :paramtype continuation_token: str
        :keyword continuation_uri: The URI of the next result set batch. If there are no more records,
         it's removed from the response.
        :paramtype continuation_uri: str
        """
        super().__init__(**kwargs)
        self.continuation_token = continuation_token
        self.continuation_uri = continuation_uri


class CustomPools(PaginatedResponse):
    """CustomPools.

    All required parameters must be populated in order to send to server.

    :ivar continuation_token: The token for the next result set batch. If there are no more
     records, it's removed from the response.
    :vartype continuation_token: str
    :ivar continuation_uri: The URI of the next result set batch. If there are no more records,
     it's removed from the response.
    :vartype continuation_uri: str
    :ivar value: A list of custom pools. Required.
    :vartype value: list[~microsoft.fabric.api.spark.models.CustomPool]
    """

    _validation = {
        "value": {"required": True},
    }

    _attribute_map = {
        "continuation_token": {"key": "continuationToken", "type": "str"},
        "continuation_uri": {"key": "continuationUri", "type": "str"},
        "value": {"key": "value", "type": "[CustomPool]"},
    }

    def __init__(
        self,
        *,
        value: List["_models.CustomPool"],
        continuation_token: Optional[str] = None,
        continuation_uri: Optional[str] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword continuation_token: The token for the next result set batch. If there are no more
         records, it's removed from the response.
        :paramtype continuation_token: str
        :keyword continuation_uri: The URI of the next result set batch. If there are no more records,
         it's removed from the response.
        :paramtype continuation_uri: str
        :keyword value: A list of custom pools. Required.
        :paramtype value: list[~microsoft.fabric.api.spark.models.CustomPool]
        """
        super().__init__(continuation_token=continuation_token, continuation_uri=continuation_uri, **kwargs)
        self.value = value


class Duration(_serialization.Model):
    """A duration.

    All required parameters must be populated in order to send to server.

    :ivar value: The number of timeUnits in the duration. Required.
    :vartype value: float
    :ivar time_unit: The unit of time for the duration. Additional duration types may be added over
     time. Required. Known values are: "Seconds", "Minutes", "Hours", and "Days".
    :vartype time_unit: str or ~microsoft.fabric.api.spark.models.TimeUnit
    """

    _validation = {
        "value": {"required": True},
        "time_unit": {"required": True},
    }

    _attribute_map = {
        "value": {"key": "value", "type": "float"},
        "time_unit": {"key": "timeUnit", "type": "str"},
    }

    def __init__(self, *, value: float, time_unit: Union[str, "_models.TimeUnit"], **kwargs: Any) -> None:
        """
        :keyword value: The number of timeUnits in the duration. Required.
        :paramtype value: float
        :keyword time_unit: The unit of time for the duration. Additional duration types may be added
         over time. Required. Known values are: "Seconds", "Minutes", "Hours", and "Days".
        :paramtype time_unit: str or ~microsoft.fabric.api.spark.models.TimeUnit
        """
        super().__init__(**kwargs)
        self.value = value
        self.time_unit = time_unit


class DynamicExecutorAllocationProperties(_serialization.Model):
    """Dynamic executor allocation proerties.

    All required parameters must be populated in order to send to server.

    :ivar enabled: The status of the dynamic executor allocation. False - Disabled, true - Enabled.
     Required.
    :vartype enabled: bool
    :ivar min_executors: The minimum executors. Required.
    :vartype min_executors: int
    :ivar max_executors: The maximum executors. Required.
    :vartype max_executors: int
    """

    _validation = {
        "enabled": {"required": True},
        "min_executors": {"required": True, "minimum": 1},
        "max_executors": {"required": True, "minimum": 1},
    }

    _attribute_map = {
        "enabled": {"key": "enabled", "type": "bool"},
        "min_executors": {"key": "minExecutors", "type": "int"},
        "max_executors": {"key": "maxExecutors", "type": "int"},
    }

    def __init__(self, *, enabled: bool, min_executors: int, max_executors: int, **kwargs: Any) -> None:
        """
        :keyword enabled: The status of the dynamic executor allocation. False - Disabled, true -
         Enabled. Required.
        :paramtype enabled: bool
        :keyword min_executors: The minimum executors. Required.
        :paramtype min_executors: int
        :keyword max_executors: The maximum executors. Required.
        :paramtype max_executors: int
        """
        super().__init__(**kwargs)
        self.enabled = enabled
        self.min_executors = min_executors
        self.max_executors = max_executors


class EnvironmentProperties(_serialization.Model):
    """Properties of an environment.

    :ivar name: The name of the default environment. Empty string indicated there is no workspace
     default environment.
    :vartype name: str
    :ivar runtime_version: `Runtime </fabric/data-engineering/runtime>`_ version. For example: 1.3.
    :vartype runtime_version: str
    """

    _attribute_map = {
        "name": {"key": "name", "type": "str"},
        "runtime_version": {"key": "runtimeVersion", "type": "str"},
    }

    def __init__(self, *, name: Optional[str] = None, runtime_version: Optional[str] = None, **kwargs: Any) -> None:
        """
        :keyword name: The name of the default environment. Empty string indicated there is no
         workspace default environment.
        :paramtype name: str
        :keyword runtime_version: `Runtime </fabric/data-engineering/runtime>`_ version. For example:
         1.3.
        :paramtype runtime_version: str
        """
        super().__init__(**kwargs)
        self.name = name
        self.runtime_version = runtime_version


class ErrorRelatedResource(_serialization.Model):
    """The error related resource details object.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to server.

    :ivar resource_id: The resource ID that's involved in the error. Required.
    :vartype resource_id: str
    :ivar resource_type: The type of the resource that's involved in the error. Required.
    :vartype resource_type: str
    """

    _validation = {
        "resource_id": {"required": True, "readonly": True},
        "resource_type": {"required": True, "readonly": True},
    }

    _attribute_map = {
        "resource_id": {"key": "resourceId", "type": "str"},
        "resource_type": {"key": "resourceType", "type": "str"},
    }

    def __init__(self, **kwargs: Any) -> None:
        """ """
        super().__init__(**kwargs)
        self.resource_id = None
        self.resource_type = None


class ErrorResponseDetails(_serialization.Model):
    """The error response details.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to server.

    :ivar error_code: A specific identifier that provides information about an error condition,
     allowing for standardized communication between our service and its users. Required.
    :vartype error_code: str
    :ivar message: A human readable representation of the error. Required.
    :vartype message: str
    :ivar related_resource: The error related resource details.
    :vartype related_resource: ~microsoft.fabric.api.spark.models.ErrorRelatedResource
    """

    _validation = {
        "error_code": {"required": True, "readonly": True},
        "message": {"required": True, "readonly": True},
        "related_resource": {"readonly": True},
    }

    _attribute_map = {
        "error_code": {"key": "errorCode", "type": "str"},
        "message": {"key": "message", "type": "str"},
        "related_resource": {"key": "relatedResource", "type": "ErrorRelatedResource"},
    }

    def __init__(self, **kwargs: Any) -> None:
        """ """
        super().__init__(**kwargs)
        self.error_code = None
        self.message = None
        self.related_resource = None


class ErrorResponse(ErrorResponseDetails):
    """The error response.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to server.

    :ivar error_code: A specific identifier that provides information about an error condition,
     allowing for standardized communication between our service and its users. Required.
    :vartype error_code: str
    :ivar message: A human readable representation of the error. Required.
    :vartype message: str
    :ivar related_resource: The error related resource details.
    :vartype related_resource: ~microsoft.fabric.api.spark.models.ErrorRelatedResource
    :ivar request_id: ID of the request associated with the error.
    :vartype request_id: str
    :ivar more_details: List of additional error details.
    :vartype more_details: list[~microsoft.fabric.api.spark.models.ErrorResponseDetails]
    """

    _validation = {
        "error_code": {"required": True, "readonly": True},
        "message": {"required": True, "readonly": True},
        "related_resource": {"readonly": True},
        "request_id": {"readonly": True},
        "more_details": {"readonly": True},
    }

    _attribute_map = {
        "error_code": {"key": "errorCode", "type": "str"},
        "message": {"key": "message", "type": "str"},
        "related_resource": {"key": "relatedResource", "type": "ErrorRelatedResource"},
        "request_id": {"key": "requestId", "type": "str"},
        "more_details": {"key": "moreDetails", "type": "[ErrorResponseDetails]"},
    }

    def __init__(self, **kwargs: Any) -> None:
        """ """
        super().__init__(**kwargs)
        self.request_id = None
        self.more_details = None


class HighConcurrencyProperties(_serialization.Model):
    """High Concurrency Properties.

    :ivar notebook_interactive_run_enabled: The status of the high concurrency for notebook
     interactive run. False - Disabled, true - Enabled.
    :vartype notebook_interactive_run_enabled: bool
    :ivar notebook_pipeline_run_enabled: The status of the high concurrency for notebook pipeline
     run. False - Disabled, true - Enabled.
    :vartype notebook_pipeline_run_enabled: bool
    """

    _attribute_map = {
        "notebook_interactive_run_enabled": {"key": "notebookInteractiveRunEnabled", "type": "bool"},
        "notebook_pipeline_run_enabled": {"key": "notebookPipelineRunEnabled", "type": "bool"},
    }

    def __init__(
        self,
        *,
        notebook_interactive_run_enabled: Optional[bool] = None,
        notebook_pipeline_run_enabled: Optional[bool] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword notebook_interactive_run_enabled: The status of the high concurrency for notebook
         interactive run. False - Disabled, true - Enabled.
        :paramtype notebook_interactive_run_enabled: bool
        :keyword notebook_pipeline_run_enabled: The status of the high concurrency for notebook
         pipeline run. False - Disabled, true - Enabled.
        :paramtype notebook_pipeline_run_enabled: bool
        """
        super().__init__(**kwargs)
        self.notebook_interactive_run_enabled = notebook_interactive_run_enabled
        self.notebook_pipeline_run_enabled = notebook_pipeline_run_enabled


class InstancePool(_serialization.Model):
    """InstancePool.

    :ivar name: Instance pool name.
    :vartype name: str
    :ivar type: Instance pool type. Known values are: "Workspace" and "Capacity".
    :vartype type: str or ~microsoft.fabric.api.spark.models.CustomPoolType
    :ivar id: Instance pool ID.
    :vartype id: str
    """

    _attribute_map = {
        "name": {"key": "name", "type": "str"},
        "type": {"key": "type", "type": "str"},
        "id": {"key": "id", "type": "str"},
    }

    def __init__(
        self,
        *,
        name: Optional[str] = None,
        type: Optional[Union[str, "_models.CustomPoolType"]] = None,
        id: Optional[str] = None,  # pylint: disable=redefined-builtin
        **kwargs: Any
    ) -> None:
        """
        :keyword name: Instance pool name.
        :paramtype name: str
        :keyword type: Instance pool type. Known values are: "Workspace" and "Capacity".
        :paramtype type: str or ~microsoft.fabric.api.spark.models.CustomPoolType
        :keyword id: Instance pool ID.
        :paramtype id: str
        """
        super().__init__(**kwargs)
        self.name = name
        self.type = type
        self.id = id


class ItemReference(_serialization.Model):
    """An item reference object.

    You probably want to use the sub-classes and not this class directly. Known sub-classes are:
    ItemReferenceById

    All required parameters must be populated in order to send to server.

    :ivar reference_type: The item reference type. Required. "ById"
    :vartype reference_type: str or ~microsoft.fabric.api.spark.models.ItemReferenceType
    """

    _validation = {
        "reference_type": {"required": True},
    }

    _attribute_map = {
        "reference_type": {"key": "referenceType", "type": "str"},
    }

    _subtype_map = {"reference_type": {"ById": "ItemReferenceById"}}

    def __init__(self, **kwargs: Any) -> None:
        """ """
        super().__init__(**kwargs)
        self.reference_type: Optional[str] = None


class ItemReferenceById(ItemReference):
    """An item reference by ID object.

    All required parameters must be populated in order to send to server.

    :ivar reference_type: The item reference type. Required. "ById"
    :vartype reference_type: str or ~microsoft.fabric.api.spark.models.ItemReferenceType
    :ivar item_id: The ID of the item. Required.
    :vartype item_id: str
    :ivar workspace_id: The workspace ID of the item. Required.
    :vartype workspace_id: str
    """

    _validation = {
        "reference_type": {"required": True},
        "item_id": {"required": True},
        "workspace_id": {"required": True},
    }

    _attribute_map = {
        "reference_type": {"key": "referenceType", "type": "str"},
        "item_id": {"key": "itemId", "type": "str"},
        "workspace_id": {"key": "workspaceId", "type": "str"},
    }

    def __init__(self, *, item_id: str, workspace_id: str, **kwargs: Any) -> None:
        """
        :keyword item_id: The ID of the item. Required.
        :paramtype item_id: str
        :keyword workspace_id: The workspace ID of the item. Required.
        :paramtype workspace_id: str
        """
        super().__init__(**kwargs)
        self.reference_type: str = "ById"
        self.item_id = item_id
        self.workspace_id = workspace_id


class LivySession(_serialization.Model):  # pylint: disable=too-many-instance-attributes
    """The livy session response.

    :ivar spark_application_id: A Spark application ID is a unique identifier assigned to each
     Apache Spark application. It also appears in the Spark UI.
    :vartype spark_application_id: str
    :ivar state: Current state of the job. Known values are: "InProgress", "Cancelled",
     "NotStarted", "Succeeded", "Failed", and "Unknown".
    :vartype state: str or ~microsoft.fabric.api.spark.models.State
    :ivar livy_id: ID of the Livy session or Livy batch.
    :vartype livy_id: str
    :ivar origin: Origin of the job. Known values are: "SubmittedJob" and "PendingJob".
    :vartype origin: str or ~microsoft.fabric.api.spark.models.Origin
    :ivar attempt_number: Current attempt number.
    :vartype attempt_number: int
    :ivar max_number_of_attempts: Maximum number of attempts.
    :vartype max_number_of_attempts: int
    :ivar livy_name: Name of the Livy session or Livy batch.
    :vartype livy_name: str
    :ivar submitter: ID of the submitter.
    :vartype submitter: ~microsoft.fabric.api.spark.models.Principal
    :ivar item: ID of the item.
    :vartype item: ~microsoft.fabric.api.spark.models.ItemReferenceById
    :ivar item_type: The item type. Known values are: "Lakehouse", "SparkJobDefinition", and
     "Notebook".
    :vartype item_type: str or ~microsoft.fabric.api.spark.models.ItemType
    :ivar item_name: Name of the item.
    :vartype item_name: str
    :ivar job_type: Current state of the job. Known values are: "Unknown", "SparkSession",
     "SparkBatch", and "JupyterSession".
    :vartype job_type: str or ~microsoft.fabric.api.spark.models.JobType
    :ivar submitted_date_time: Timestamp when the job was submitted in UTC, using the
     YYYY-MM-DDTHH:mm:ssZ format.
    :vartype submitted_date_time: ~datetime.datetime
    :ivar start_date_time: Timestamp when the job started in UTC, using the YYYY-MM-DDTHH:mm:ssZ
     format.
    :vartype start_date_time: ~datetime.datetime
    :ivar end_date_time: Timestamp when the job ended in UTC, using the YYYY-MM-DDTHH:mm:ssZ
     format.
    :vartype end_date_time: ~datetime.datetime
    :ivar queued_duration: Duration for which the job was queued.
    :vartype queued_duration: ~microsoft.fabric.api.spark.models.Duration
    :ivar running_duration: Time it took the job to run.
    :vartype running_duration: ~microsoft.fabric.api.spark.models.Duration
    :ivar total_duration: Total duration of the job.
    :vartype total_duration: ~microsoft.fabric.api.spark.models.Duration
    :ivar job_instance_id: ID of the job instance.
    :vartype job_instance_id: str
    :ivar creator_item: ID of the item creator. When ``isHighConcurrency`` is set to ``true`` this
     value might be different than ``itemId``.
    :vartype creator_item: ~microsoft.fabric.api.spark.models.ItemReferenceById
    :ivar is_high_concurrency: Flag indicating high concurrency.
    :vartype is_high_concurrency: bool
    :ivar cancellation_reason: Reason for the job cancellation.
    :vartype cancellation_reason: str
    :ivar capacity_id: ID of the capacity.
    :vartype capacity_id: str
    :ivar operation_name: Name of the operation. Possible values include: *Notebook run*\\ ,
     *Notebook HC run* and *Notebook pipeline run*.
    :vartype operation_name: str
    :ivar consumer_id: ID of the consumer.
    :vartype consumer_id: ~microsoft.fabric.api.spark.models.Principal
    :ivar runtime_version: The fabric runtime version.
    :vartype runtime_version: str
    :ivar livy_session_item_resource_uri: The URI used to retrieve all Livy sessions for a given
     item.
    :vartype livy_session_item_resource_uri: str
    """

    _attribute_map = {
        "spark_application_id": {"key": "sparkApplicationId", "type": "str"},
        "state": {"key": "state", "type": "str"},
        "livy_id": {"key": "livyId", "type": "str"},
        "origin": {"key": "origin", "type": "str"},
        "attempt_number": {"key": "attemptNumber", "type": "int"},
        "max_number_of_attempts": {"key": "maxNumberOfAttempts", "type": "int"},
        "livy_name": {"key": "livyName", "type": "str"},
        "submitter": {"key": "submitter", "type": "Principal"},
        "item": {"key": "item", "type": "ItemReferenceById"},
        "item_type": {"key": "itemType", "type": "str"},
        "item_name": {"key": "itemName", "type": "str"},
        "job_type": {"key": "jobType", "type": "str"},
        "submitted_date_time": {"key": "submittedDateTime", "type": "iso-8601"},
        "start_date_time": {"key": "startDateTime", "type": "iso-8601"},
        "end_date_time": {"key": "endDateTime", "type": "iso-8601"},
        "queued_duration": {"key": "queuedDuration", "type": "Duration"},
        "running_duration": {"key": "runningDuration", "type": "Duration"},
        "total_duration": {"key": "totalDuration", "type": "Duration"},
        "job_instance_id": {"key": "jobInstanceId", "type": "str"},
        "creator_item": {"key": "creatorItem", "type": "ItemReferenceById"},
        "is_high_concurrency": {"key": "isHighConcurrency", "type": "bool"},
        "cancellation_reason": {"key": "cancellationReason", "type": "str"},
        "capacity_id": {"key": "capacityId", "type": "str"},
        "operation_name": {"key": "operationName", "type": "str"},
        "consumer_id": {"key": "consumerId", "type": "Principal"},
        "runtime_version": {"key": "runtimeVersion", "type": "str"},
        "livy_session_item_resource_uri": {"key": "livySessionItemResourceUri", "type": "str"},
    }

    def __init__(  # pylint: disable=too-many-locals
        self,
        *,
        spark_application_id: Optional[str] = None,
        state: Optional[Union[str, "_models.State"]] = None,
        livy_id: Optional[str] = None,
        origin: Optional[Union[str, "_models.Origin"]] = None,
        attempt_number: Optional[int] = None,
        max_number_of_attempts: Optional[int] = None,
        livy_name: Optional[str] = None,
        submitter: Optional["_models.Principal"] = None,
        item: Optional["_models.ItemReferenceById"] = None,
        item_type: Optional[Union[str, "_models.ItemType"]] = None,
        item_name: Optional[str] = None,
        job_type: Optional[Union[str, "_models.JobType"]] = None,
        submitted_date_time: Optional[datetime.datetime] = None,
        start_date_time: Optional[datetime.datetime] = None,
        end_date_time: Optional[datetime.datetime] = None,
        queued_duration: Optional["_models.Duration"] = None,
        running_duration: Optional["_models.Duration"] = None,
        total_duration: Optional["_models.Duration"] = None,
        job_instance_id: Optional[str] = None,
        creator_item: Optional["_models.ItemReferenceById"] = None,
        is_high_concurrency: Optional[bool] = None,
        cancellation_reason: Optional[str] = None,
        capacity_id: Optional[str] = None,
        operation_name: Optional[str] = None,
        consumer_id: Optional["_models.Principal"] = None,
        runtime_version: Optional[str] = None,
        livy_session_item_resource_uri: Optional[str] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword spark_application_id: A Spark application ID is a unique identifier assigned to each
         Apache Spark application. It also appears in the Spark UI.
        :paramtype spark_application_id: str
        :keyword state: Current state of the job. Known values are: "InProgress", "Cancelled",
         "NotStarted", "Succeeded", "Failed", and "Unknown".
        :paramtype state: str or ~microsoft.fabric.api.spark.models.State
        :keyword livy_id: ID of the Livy session or Livy batch.
        :paramtype livy_id: str
        :keyword origin: Origin of the job. Known values are: "SubmittedJob" and "PendingJob".
        :paramtype origin: str or ~microsoft.fabric.api.spark.models.Origin
        :keyword attempt_number: Current attempt number.
        :paramtype attempt_number: int
        :keyword max_number_of_attempts: Maximum number of attempts.
        :paramtype max_number_of_attempts: int
        :keyword livy_name: Name of the Livy session or Livy batch.
        :paramtype livy_name: str
        :keyword submitter: ID of the submitter.
        :paramtype submitter: ~microsoft.fabric.api.spark.models.Principal
        :keyword item: ID of the item.
        :paramtype item: ~microsoft.fabric.api.spark.models.ItemReferenceById
        :keyword item_type: The item type. Known values are: "Lakehouse", "SparkJobDefinition", and
         "Notebook".
        :paramtype item_type: str or ~microsoft.fabric.api.spark.models.ItemType
        :keyword item_name: Name of the item.
        :paramtype item_name: str
        :keyword job_type: Current state of the job. Known values are: "Unknown", "SparkSession",
         "SparkBatch", and "JupyterSession".
        :paramtype job_type: str or ~microsoft.fabric.api.spark.models.JobType
        :keyword submitted_date_time: Timestamp when the job was submitted in UTC, using the
         YYYY-MM-DDTHH:mm:ssZ format.
        :paramtype submitted_date_time: ~datetime.datetime
        :keyword start_date_time: Timestamp when the job started in UTC, using the YYYY-MM-DDTHH:mm:ssZ
         format.
        :paramtype start_date_time: ~datetime.datetime
        :keyword end_date_time: Timestamp when the job ended in UTC, using the YYYY-MM-DDTHH:mm:ssZ
         format.
        :paramtype end_date_time: ~datetime.datetime
        :keyword queued_duration: Duration for which the job was queued.
        :paramtype queued_duration: ~microsoft.fabric.api.spark.models.Duration
        :keyword running_duration: Time it took the job to run.
        :paramtype running_duration: ~microsoft.fabric.api.spark.models.Duration
        :keyword total_duration: Total duration of the job.
        :paramtype total_duration: ~microsoft.fabric.api.spark.models.Duration
        :keyword job_instance_id: ID of the job instance.
        :paramtype job_instance_id: str
        :keyword creator_item: ID of the item creator. When ``isHighConcurrency`` is set to ``true``
         this value might be different than ``itemId``.
        :paramtype creator_item: ~microsoft.fabric.api.spark.models.ItemReferenceById
        :keyword is_high_concurrency: Flag indicating high concurrency.
        :paramtype is_high_concurrency: bool
        :keyword cancellation_reason: Reason for the job cancellation.
        :paramtype cancellation_reason: str
        :keyword capacity_id: ID of the capacity.
        :paramtype capacity_id: str
        :keyword operation_name: Name of the operation. Possible values include: *Notebook run*\\ ,
         *Notebook HC run* and *Notebook pipeline run*.
        :paramtype operation_name: str
        :keyword consumer_id: ID of the consumer.
        :paramtype consumer_id: ~microsoft.fabric.api.spark.models.Principal
        :keyword runtime_version: The fabric runtime version.
        :paramtype runtime_version: str
        :keyword livy_session_item_resource_uri: The URI used to retrieve all Livy sessions for a given
         item.
        :paramtype livy_session_item_resource_uri: str
        """
        super().__init__(**kwargs)
        self.spark_application_id = spark_application_id
        self.state = state
        self.livy_id = livy_id
        self.origin = origin
        self.attempt_number = attempt_number
        self.max_number_of_attempts = max_number_of_attempts
        self.livy_name = livy_name
        self.submitter = submitter
        self.item = item
        self.item_type = item_type
        self.item_name = item_name
        self.job_type = job_type
        self.submitted_date_time = submitted_date_time
        self.start_date_time = start_date_time
        self.end_date_time = end_date_time
        self.queued_duration = queued_duration
        self.running_duration = running_duration
        self.total_duration = total_duration
        self.job_instance_id = job_instance_id
        self.creator_item = creator_item
        self.is_high_concurrency = is_high_concurrency
        self.cancellation_reason = cancellation_reason
        self.capacity_id = capacity_id
        self.operation_name = operation_name
        self.consumer_id = consumer_id
        self.runtime_version = runtime_version
        self.livy_session_item_resource_uri = livy_session_item_resource_uri


class LivySessions(PaginatedResponse):
    """A paginated list of livy sessions.

    All required parameters must be populated in order to send to server.

    :ivar continuation_token: The token for the next result set batch. If there are no more
     records, it's removed from the response.
    :vartype continuation_token: str
    :ivar continuation_uri: The URI of the next result set batch. If there are no more records,
     it's removed from the response.
    :vartype continuation_uri: str
    :ivar value: A list of livy sessions. Required.
    :vartype value: list[~microsoft.fabric.api.spark.models.LivySession]
    """

    _validation = {
        "value": {"required": True},
    }

    _attribute_map = {
        "continuation_token": {"key": "continuationToken", "type": "str"},
        "continuation_uri": {"key": "continuationUri", "type": "str"},
        "value": {"key": "value", "type": "[LivySession]"},
    }

    def __init__(
        self,
        *,
        value: List["_models.LivySession"],
        continuation_token: Optional[str] = None,
        continuation_uri: Optional[str] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword continuation_token: The token for the next result set batch. If there are no more
         records, it's removed from the response.
        :paramtype continuation_token: str
        :keyword continuation_uri: The URI of the next result set batch. If there are no more records,
         it's removed from the response.
        :paramtype continuation_uri: str
        :keyword value: A list of livy sessions. Required.
        :paramtype value: list[~microsoft.fabric.api.spark.models.LivySession]
        """
        super().__init__(continuation_token=continuation_token, continuation_uri=continuation_uri, **kwargs)
        self.value = value


class PoolProperties(_serialization.Model):
    """Properties of a pool.

    :ivar customize_compute_enabled: Customize compute configurations for items. False - Disabled,
     true - Enabled.
    :vartype customize_compute_enabled: bool
    :ivar default_pool: Default pool for workspace. It should be a valid custom pool name. "Starter
     Pool" means use starter pool.
    :vartype default_pool: ~microsoft.fabric.api.spark.models.InstancePool
    :ivar starter_pool: Customize starter pool. For more information about configuring starter
     pool, see `configuring starter pool </fabric/data-engineering/configure-starter-pools>`_.
    :vartype starter_pool: ~microsoft.fabric.api.spark.models.StarterPoolProperties
    """

    _attribute_map = {
        "customize_compute_enabled": {"key": "customizeComputeEnabled", "type": "bool"},
        "default_pool": {"key": "defaultPool", "type": "InstancePool"},
        "starter_pool": {"key": "starterPool", "type": "StarterPoolProperties"},
    }

    def __init__(
        self,
        *,
        customize_compute_enabled: Optional[bool] = None,
        default_pool: Optional["_models.InstancePool"] = None,
        starter_pool: Optional["_models.StarterPoolProperties"] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword customize_compute_enabled: Customize compute configurations for items. False -
         Disabled, true - Enabled.
        :paramtype customize_compute_enabled: bool
        :keyword default_pool: Default pool for workspace. It should be a valid custom pool name.
         "Starter Pool" means use starter pool.
        :paramtype default_pool: ~microsoft.fabric.api.spark.models.InstancePool
        :keyword starter_pool: Customize starter pool. For more information about configuring starter
         pool, see `configuring starter pool </fabric/data-engineering/configure-starter-pools>`_.
        :paramtype starter_pool: ~microsoft.fabric.api.spark.models.StarterPoolProperties
        """
        super().__init__(**kwargs)
        self.customize_compute_enabled = customize_compute_enabled
        self.default_pool = default_pool
        self.starter_pool = starter_pool


class Principal(_serialization.Model):
    """Represents an identity or a Microsoft Entra group.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to server.

    :ivar id: The principal's ID. Required.
    :vartype id: str
    :ivar display_name: The principal's display name.
    :vartype display_name: str
    :ivar type: The type of the principal. Additional principal types may be added over time.
     Required. Known values are: "User", "ServicePrincipal", "Group", "ServicePrincipalProfile", and
     "EntireTenant".
    :vartype type: str or ~microsoft.fabric.api.spark.models.PrincipalType
    :ivar user_details: User principal specific details. Applicable when the principal type is
     ``User``.
    :vartype user_details: ~microsoft.fabric.api.spark.models.PrincipalUserDetails
    :ivar service_principal_details: Service principal specific details. Applicable when the
     principal type is ``ServicePrincipal``.
    :vartype service_principal_details:
     ~microsoft.fabric.api.spark.models.PrincipalServicePrincipalDetails
    :ivar group_details: Group specific details. Applicable when the principal type is ``Group``.
    :vartype group_details: ~microsoft.fabric.api.spark.models.PrincipalGroupDetails
    :ivar service_principal_profile_details: Service principal profile details. Applicable when the
     principal type is ``ServicePrincipalProfile``.
    :vartype service_principal_profile_details:
     ~microsoft.fabric.api.spark.models.PrincipalServicePrincipalProfileDetails
    """

    _validation = {
        "id": {"required": True},
        "display_name": {"readonly": True},
        "type": {"required": True},
        "user_details": {"readonly": True},
        "service_principal_details": {"readonly": True},
    }

    _attribute_map = {
        "id": {"key": "id", "type": "str"},
        "display_name": {"key": "displayName", "type": "str"},
        "type": {"key": "type", "type": "str"},
        "user_details": {"key": "userDetails", "type": "PrincipalUserDetails"},
        "service_principal_details": {"key": "servicePrincipalDetails", "type": "PrincipalServicePrincipalDetails"},
        "group_details": {"key": "groupDetails", "type": "PrincipalGroupDetails"},
        "service_principal_profile_details": {
            "key": "servicePrincipalProfileDetails",
            "type": "PrincipalServicePrincipalProfileDetails",
        },
    }

    def __init__(
        self,
        *,
        id: str,  # pylint: disable=redefined-builtin
        type: Union[str, "_models.PrincipalType"],
        group_details: Optional["_models.PrincipalGroupDetails"] = None,
        service_principal_profile_details: Optional["_models.PrincipalServicePrincipalProfileDetails"] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword id: The principal's ID. Required.
        :paramtype id: str
        :keyword type: The type of the principal. Additional principal types may be added over time.
         Required. Known values are: "User", "ServicePrincipal", "Group", "ServicePrincipalProfile", and
         "EntireTenant".
        :paramtype type: str or ~microsoft.fabric.api.spark.models.PrincipalType
        :keyword group_details: Group specific details. Applicable when the principal type is
         ``Group``.
        :paramtype group_details: ~microsoft.fabric.api.spark.models.PrincipalGroupDetails
        :keyword service_principal_profile_details: Service principal profile details. Applicable when
         the principal type is ``ServicePrincipalProfile``.
        :paramtype service_principal_profile_details:
         ~microsoft.fabric.api.spark.models.PrincipalServicePrincipalProfileDetails
        """
        super().__init__(**kwargs)
        self.id = id
        self.display_name = None
        self.type = type
        self.user_details = None
        self.service_principal_details = None
        self.group_details = group_details
        self.service_principal_profile_details = service_principal_profile_details


class PrincipalGroupDetails(_serialization.Model):
    """Group specific details. Applicable when the principal type is ``Group``.

    :ivar group_type: The type of the group. Additional group types may be added over time. Known
     values are: "Unknown", "SecurityGroup", and "DistributionList".
    :vartype group_type: str or ~microsoft.fabric.api.spark.models.GroupType
    """

    _attribute_map = {
        "group_type": {"key": "groupType", "type": "str"},
    }

    def __init__(self, *, group_type: Optional[Union[str, "_models.GroupType"]] = None, **kwargs: Any) -> None:
        """
        :keyword group_type: The type of the group. Additional group types may be added over time.
         Known values are: "Unknown", "SecurityGroup", and "DistributionList".
        :paramtype group_type: str or ~microsoft.fabric.api.spark.models.GroupType
        """
        super().__init__(**kwargs)
        self.group_type = group_type


class PrincipalServicePrincipalDetails(_serialization.Model):
    """Service principal specific details. Applicable when the principal type is ``ServicePrincipal``.

    Variables are only populated by the server, and will be ignored when sending a request.

    :ivar aad_app_id: The service principal's Microsoft Entra AppId.
    :vartype aad_app_id: str
    """

    _validation = {
        "aad_app_id": {"readonly": True},
    }

    _attribute_map = {
        "aad_app_id": {"key": "aadAppId", "type": "str"},
    }

    def __init__(self, **kwargs: Any) -> None:
        """ """
        super().__init__(**kwargs)
        self.aad_app_id = None


class PrincipalServicePrincipalProfileDetails(_serialization.Model):
    """Service principal profile details. Applicable when the principal type is
    ``ServicePrincipalProfile``.

    :ivar parent_principal: The service principal profile's parent principal.
    :vartype parent_principal: ~microsoft.fabric.api.spark.models.Principal
    """

    _attribute_map = {
        "parent_principal": {"key": "parentPrincipal", "type": "Principal"},
    }

    def __init__(self, *, parent_principal: Optional["_models.Principal"] = None, **kwargs: Any) -> None:
        """
        :keyword parent_principal: The service principal profile's parent principal.
        :paramtype parent_principal: ~microsoft.fabric.api.spark.models.Principal
        """
        super().__init__(**kwargs)
        self.parent_principal = parent_principal


class PrincipalUserDetails(_serialization.Model):
    """User principal specific details. Applicable when the principal type is ``User``.

    Variables are only populated by the server, and will be ignored when sending a request.

    :ivar user_principal_name: The user principal name.
    :vartype user_principal_name: str
    """

    _validation = {
        "user_principal_name": {"readonly": True},
    }

    _attribute_map = {
        "user_principal_name": {"key": "userPrincipalName", "type": "str"},
    }

    def __init__(self, **kwargs: Any) -> None:
        """ """
        super().__init__(**kwargs)
        self.user_principal_name = None


class SparkJobsProperties(_serialization.Model):
    """Properties of a Spark job.

    :ivar conservative_job_admission_enabled: Reserve maximum cores for active Spark jobs. When
     this setting is enabled, your Fabric capacity reserves the maximum number of cores needed for
     active Spark jobs, ensuring job reliability by making sure that cores are available if a job
     scales up. When this setting is disabled, jobs are started based on the minimum number of cores
     needed, letting more jobs run at the same time. For more information see `job admission and
     management </fabric/data-engineering/job-admission-management>`_. False - Disabled, true -
     Enabled.
    :vartype conservative_job_admission_enabled: bool
    :ivar session_timeout_in_minutes: Time to terminate inactive Spark sessions. The maximum is 14
     days.
    :vartype session_timeout_in_minutes: int
    """

    _validation = {
        "session_timeout_in_minutes": {"maximum": 20160, "minimum": 1},
    }

    _attribute_map = {
        "conservative_job_admission_enabled": {"key": "conservativeJobAdmissionEnabled", "type": "bool"},
        "session_timeout_in_minutes": {"key": "sessionTimeoutInMinutes", "type": "int"},
    }

    def __init__(
        self,
        *,
        conservative_job_admission_enabled: Optional[bool] = None,
        session_timeout_in_minutes: Optional[int] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword conservative_job_admission_enabled: Reserve maximum cores for active Spark jobs. When
         this setting is enabled, your Fabric capacity reserves the maximum number of cores needed for
         active Spark jobs, ensuring job reliability by making sure that cores are available if a job
         scales up. When this setting is disabled, jobs are started based on the minimum number of cores
         needed, letting more jobs run at the same time. For more information see `job admission and
         management </fabric/data-engineering/job-admission-management>`_. False - Disabled, true -
         Enabled.
        :paramtype conservative_job_admission_enabled: bool
        :keyword session_timeout_in_minutes: Time to terminate inactive Spark sessions. The maximum is
         14 days.
        :paramtype session_timeout_in_minutes: int
        """
        super().__init__(**kwargs)
        self.conservative_job_admission_enabled = conservative_job_admission_enabled
        self.session_timeout_in_minutes = session_timeout_in_minutes


class StarterPoolProperties(_serialization.Model):
    """Custom starter pool.

    :ivar max_node_count: The maximum node count.
    :vartype max_node_count: int
    :ivar max_executors: The maximum executors count.
    :vartype max_executors: int
    """

    _validation = {
        "max_node_count": {"minimum": 1},
        "max_executors": {"minimum": 1},
    }

    _attribute_map = {
        "max_node_count": {"key": "maxNodeCount", "type": "int"},
        "max_executors": {"key": "maxExecutors", "type": "int"},
    }

    def __init__(
        self, *, max_node_count: Optional[int] = None, max_executors: Optional[int] = None, **kwargs: Any
    ) -> None:
        """
        :keyword max_node_count: The maximum node count.
        :paramtype max_node_count: int
        :keyword max_executors: The maximum executors count.
        :paramtype max_executors: int
        """
        super().__init__(**kwargs)
        self.max_node_count = max_node_count
        self.max_executors = max_executors


class UpdateCustomPoolRequest(_serialization.Model):
    """Update custom pool request payload.

    :ivar name: Custom pool name.:code:`<br>`The name must be between 1 and 64 characters long and
     must contain only letters, numbers, dashes, underscores and spaces.:code:`<br>`Custom pool
     names must be unique within the workspace.:code:`<br>`"Starter Pool" is a reserved custom pool
     name.
    :vartype name: str
    :ivar node_family: Node family. "MemoryOptimized"
    :vartype node_family: str or ~microsoft.fabric.api.spark.models.NodeFamily
    :ivar node_size: Node size. Known values are: "Small", "Medium", "Large", "XLarge", and
     "XXLarge".
    :vartype node_size: str or ~microsoft.fabric.api.spark.models.NodeSize
    :ivar auto_scale: Autoscale.
    :vartype auto_scale: ~microsoft.fabric.api.spark.models.AutoScaleProperties
    :ivar dynamic_executor_allocation: Dynamic executor allocation.
    :vartype dynamic_executor_allocation:
     ~microsoft.fabric.api.spark.models.DynamicExecutorAllocationProperties
    """

    _attribute_map = {
        "name": {"key": "name", "type": "str"},
        "node_family": {"key": "nodeFamily", "type": "str"},
        "node_size": {"key": "nodeSize", "type": "str"},
        "auto_scale": {"key": "autoScale", "type": "AutoScaleProperties"},
        "dynamic_executor_allocation": {
            "key": "dynamicExecutorAllocation",
            "type": "DynamicExecutorAllocationProperties",
        },
    }

    def __init__(
        self,
        *,
        name: Optional[str] = None,
        node_family: Optional[Union[str, "_models.NodeFamily"]] = None,
        node_size: Optional[Union[str, "_models.NodeSize"]] = None,
        auto_scale: Optional["_models.AutoScaleProperties"] = None,
        dynamic_executor_allocation: Optional["_models.DynamicExecutorAllocationProperties"] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword name: Custom pool name.:code:`<br>`The name must be between 1 and 64 characters long
         and must contain only letters, numbers, dashes, underscores and spaces.:code:`<br>`Custom pool
         names must be unique within the workspace.:code:`<br>`"Starter Pool" is a reserved custom pool
         name.
        :paramtype name: str
        :keyword node_family: Node family. "MemoryOptimized"
        :paramtype node_family: str or ~microsoft.fabric.api.spark.models.NodeFamily
        :keyword node_size: Node size. Known values are: "Small", "Medium", "Large", "XLarge", and
         "XXLarge".
        :paramtype node_size: str or ~microsoft.fabric.api.spark.models.NodeSize
        :keyword auto_scale: Autoscale.
        :paramtype auto_scale: ~microsoft.fabric.api.spark.models.AutoScaleProperties
        :keyword dynamic_executor_allocation: Dynamic executor allocation.
        :paramtype dynamic_executor_allocation:
         ~microsoft.fabric.api.spark.models.DynamicExecutorAllocationProperties
        """
        super().__init__(**kwargs)
        self.name = name
        self.node_family = node_family
        self.node_size = node_size
        self.auto_scale = auto_scale
        self.dynamic_executor_allocation = dynamic_executor_allocation


class UpdateWorkspaceSparkSettingsRequest(_serialization.Model):
    """Update workspace Spark settings request payload.

    :ivar automatic_log: Automatic log settings.
    :vartype automatic_log: ~microsoft.fabric.api.spark.models.AutomaticLogProperties
    :ivar high_concurrency: High concurrency settings.
    :vartype high_concurrency: ~microsoft.fabric.api.spark.models.HighConcurrencyProperties
    :ivar pool: Pool settings.
    :vartype pool: ~microsoft.fabric.api.spark.models.PoolProperties
    :ivar environment: Environment settings.
    :vartype environment: ~microsoft.fabric.api.spark.models.EnvironmentProperties
    :ivar job: Job management settings.
    :vartype job: ~microsoft.fabric.api.spark.models.SparkJobsProperties
    """

    _attribute_map = {
        "automatic_log": {"key": "automaticLog", "type": "AutomaticLogProperties"},
        "high_concurrency": {"key": "highConcurrency", "type": "HighConcurrencyProperties"},
        "pool": {"key": "pool", "type": "PoolProperties"},
        "environment": {"key": "environment", "type": "EnvironmentProperties"},
        "job": {"key": "job", "type": "SparkJobsProperties"},
    }

    def __init__(
        self,
        *,
        automatic_log: Optional["_models.AutomaticLogProperties"] = None,
        high_concurrency: Optional["_models.HighConcurrencyProperties"] = None,
        pool: Optional["_models.PoolProperties"] = None,
        environment: Optional["_models.EnvironmentProperties"] = None,
        job: Optional["_models.SparkJobsProperties"] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword automatic_log: Automatic log settings.
        :paramtype automatic_log: ~microsoft.fabric.api.spark.models.AutomaticLogProperties
        :keyword high_concurrency: High concurrency settings.
        :paramtype high_concurrency: ~microsoft.fabric.api.spark.models.HighConcurrencyProperties
        :keyword pool: Pool settings.
        :paramtype pool: ~microsoft.fabric.api.spark.models.PoolProperties
        :keyword environment: Environment settings.
        :paramtype environment: ~microsoft.fabric.api.spark.models.EnvironmentProperties
        :keyword job: Job management settings.
        :paramtype job: ~microsoft.fabric.api.spark.models.SparkJobsProperties
        """
        super().__init__(**kwargs)
        self.automatic_log = automatic_log
        self.high_concurrency = high_concurrency
        self.pool = pool
        self.environment = environment
        self.job = job


class WorkspaceSparkSettings(_serialization.Model):
    """Workspace Spark settings.

    :ivar automatic_log: Automatic log settings.
    :vartype automatic_log: ~microsoft.fabric.api.spark.models.AutomaticLogProperties
    :ivar high_concurrency: High concurrency settings.
    :vartype high_concurrency: ~microsoft.fabric.api.spark.models.HighConcurrencyProperties
    :ivar pool: Pool settings.
    :vartype pool: ~microsoft.fabric.api.spark.models.PoolProperties
    :ivar environment: Environment settings.
    :vartype environment: ~microsoft.fabric.api.spark.models.EnvironmentProperties
    :ivar job: Job management settings.
    :vartype job: ~microsoft.fabric.api.spark.models.SparkJobsProperties
    """

    _attribute_map = {
        "automatic_log": {"key": "automaticLog", "type": "AutomaticLogProperties"},
        "high_concurrency": {"key": "highConcurrency", "type": "HighConcurrencyProperties"},
        "pool": {"key": "pool", "type": "PoolProperties"},
        "environment": {"key": "environment", "type": "EnvironmentProperties"},
        "job": {"key": "job", "type": "SparkJobsProperties"},
    }

    def __init__(
        self,
        *,
        automatic_log: Optional["_models.AutomaticLogProperties"] = None,
        high_concurrency: Optional["_models.HighConcurrencyProperties"] = None,
        pool: Optional["_models.PoolProperties"] = None,
        environment: Optional["_models.EnvironmentProperties"] = None,
        job: Optional["_models.SparkJobsProperties"] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword automatic_log: Automatic log settings.
        :paramtype automatic_log: ~microsoft.fabric.api.spark.models.AutomaticLogProperties
        :keyword high_concurrency: High concurrency settings.
        :paramtype high_concurrency: ~microsoft.fabric.api.spark.models.HighConcurrencyProperties
        :keyword pool: Pool settings.
        :paramtype pool: ~microsoft.fabric.api.spark.models.PoolProperties
        :keyword environment: Environment settings.
        :paramtype environment: ~microsoft.fabric.api.spark.models.EnvironmentProperties
        :keyword job: Job management settings.
        :paramtype job: ~microsoft.fabric.api.spark.models.SparkJobsProperties
        """
        super().__init__(**kwargs)
        self.automatic_log = automatic_log
        self.high_concurrency = high_concurrency
        self.pool = pool
        self.environment = environment
        self.job = job
