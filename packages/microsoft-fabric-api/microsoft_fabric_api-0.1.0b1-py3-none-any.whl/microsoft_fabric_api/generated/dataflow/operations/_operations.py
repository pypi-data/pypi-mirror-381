# pylint: disable=too-many-lines,too-many-statements
# coding=utf-8
# --------------------------------------------------------------------------
# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.10.3, generator: @autorest/python@6.15.0)
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
from io import IOBase
import sys
from typing import Any, Callable, Dict, IO, Iterable, Iterator, Optional, Type, TypeVar, Union, cast, overload

from azure.core.exceptions import (
    ClientAuthenticationError,
    HttpResponseError,
    ResourceExistsError,
    ResourceNotFoundError,
    ResourceNotModifiedError,
    map_error,
)
from azure.core.paging import ItemPaged
from azure.core.pipeline import PipelineResponse
from azure.core.polling import LROPoller, NoPolling, PollingMethod
from azure.core.polling.base_polling import LROBasePolling
from azure.core.rest import HttpRequest, HttpResponse
from azure.core.tracing.decorator import distributed_trace
from azure.core.utils import case_insensitive_dict

from .. import models as _models
from .._serialization import Serializer

if sys.version_info >= (3, 9):
    from collections.abc import MutableMapping
else:
    from typing import MutableMapping  # type: ignore  # pylint: disable=ungrouped-imports
T = TypeVar("T")
ClsType = Optional[Callable[[PipelineResponse[HttpRequest, HttpResponse], T, Dict[str, Any]], Any]]

_SERIALIZER = Serializer()
_SERIALIZER.client_side_validation = False


def build_items_list_dataflows_request(
    workspace_id: str, *, continuation_token_parameter: Optional[str] = None, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataflows"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct parameters
    if continuation_token_parameter is not None:
        _params["continuationToken"] = _SERIALIZER.query(
            "continuation_token_parameter", continuation_token_parameter, "str"
        )

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="GET", url=_url, params=_params, headers=_headers, **kwargs)


def build_items_create_dataflow_request(workspace_id: str, **kwargs: Any) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataflows"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="POST", url=_url, headers=_headers, **kwargs)


def build_items_get_dataflow_request(workspace_id: str, dataflow_id: str, **kwargs: Any) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataflows/{dataflowId}"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "dataflowId": _SERIALIZER.url("dataflow_id", dataflow_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="GET", url=_url, headers=_headers, **kwargs)


def build_items_update_dataflow_request(workspace_id: str, dataflow_id: str, **kwargs: Any) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataflows/{dataflowId}"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "dataflowId": _SERIALIZER.url("dataflow_id", dataflow_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="PATCH", url=_url, headers=_headers, **kwargs)


def build_items_delete_dataflow_request(workspace_id: str, dataflow_id: str, **kwargs: Any) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataflows/{dataflowId}"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "dataflowId": _SERIALIZER.url("dataflow_id", dataflow_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="DELETE", url=_url, headers=_headers, **kwargs)


def build_items_get_dataflow_definition_request(  # pylint: disable=name-too-long
    workspace_id: str, dataflow_id: str, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataflows/{dataflowId}/getDefinition"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "dataflowId": _SERIALIZER.url("dataflow_id", dataflow_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="POST", url=_url, headers=_headers, **kwargs)


def build_items_update_dataflow_definition_request(  # pylint: disable=name-too-long
    workspace_id: str, dataflow_id: str, *, update_metadata: Optional[bool] = None, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataflows/{dataflowId}/updateDefinition"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "dataflowId": _SERIALIZER.url("dataflow_id", dataflow_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct parameters
    if update_metadata is not None:
        _params["updateMetadata"] = _SERIALIZER.query("update_metadata", update_metadata, "bool")

    # Construct headers
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="POST", url=_url, params=_params, headers=_headers, **kwargs)


def build_items_discover_dataflow_parameters_request(  # pylint: disable=name-too-long
    workspace_id: str, dataflow_id: str, *, continuation_token_parameter: Optional[str] = None, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataflows/{dataflowId}/parameters"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "dataflowId": _SERIALIZER.url("dataflow_id", dataflow_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct parameters
    if continuation_token_parameter is not None:
        _params["continuationToken"] = _SERIALIZER.query(
            "continuation_token_parameter", continuation_token_parameter, "str"
        )

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="GET", url=_url, params=_params, headers=_headers, **kwargs)


def build_background_jobs_schedule_execute_request(  # pylint: disable=name-too-long
    workspace_id: str, dataflow_id: str, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataflows/{dataflowId}/jobs/Execute/schedules"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "dataflowId": _SERIALIZER.url("dataflow_id", dataflow_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="POST", url=_url, headers=_headers, **kwargs)


def build_background_jobs_schedule_apply_changes_request(  # pylint: disable=name-too-long
    workspace_id: str, dataflow_id: str, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataflows/{dataflowId}/jobs/ApplyChanges/schedules"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "dataflowId": _SERIALIZER.url("dataflow_id", dataflow_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="POST", url=_url, headers=_headers, **kwargs)


def build_background_jobs_run_on_demand_execute_request(  # pylint: disable=name-too-long
    workspace_id: str, dataflow_id: str, *, job_type: str, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataflows/{dataflowId}/jobs/instances"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "dataflowId": _SERIALIZER.url("dataflow_id", dataflow_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct parameters
    _params["jobType"] = _SERIALIZER.query("job_type", job_type, "str")

    # Construct headers
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="POST", url=_url, params=_params, headers=_headers, **kwargs)


def build_background_jobs_run_on_demand_apply_changes_request(  # pylint: disable=name-too-long
    workspace_id: str, dataflow_id: str, *, job_type: str, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataflows/{dataflowId}/jobs/instances"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "dataflowId": _SERIALIZER.url("dataflow_id", dataflow_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct parameters
    _params["jobType"] = _SERIALIZER.query("job_type", job_type, "str")

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="POST", url=_url, params=_params, headers=_headers, **kwargs)


class ItemsOperations:
    """
    .. warning::
        **DO NOT** instantiate this class directly.

        Instead, you should access the following operations through
        :class:`~microsoft.fabric.api.dataflow.FabricDataflowClient`'s
        :attr:`items` attribute.
    """

    models = _models

    def __init__(self, *args, **kwargs):
        input_args = list(args)
        self._client = input_args.pop(0) if input_args else kwargs.pop("client")
        self._config = input_args.pop(0) if input_args else kwargs.pop("config")
        self._serialize = input_args.pop(0) if input_args else kwargs.pop("serializer")
        self._deserialize = input_args.pop(0) if input_args else kwargs.pop("deserializer")

    @distributed_trace
    def list_dataflows(
        self, workspace_id: str, *, continuation_token_parameter: Optional[str] = None, **kwargs: Any
    ) -> Iterable["_models.Dataflow"]:
        """Returns a list of Dataflows from the specified workspace.

        This API supports `pagination </rest/api/fabric/articles/pagination>`_.

        Permissions
        -----------

         The caller must have a *viewer* workspace role.

        Required Delegated Scopes
        -------------------------

        Workspace.Read.All or Workspace.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :keyword continuation_token_parameter: A token for retrieving the next page of results. Default
         value is None.
        :paramtype continuation_token_parameter: str
        :return: An iterator like instance of Dataflow
        :rtype: ~azure.core.paging.ItemPaged[~microsoft.fabric.api.dataflow.models.Dataflow]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models._models.Dataflows] = kwargs.pop("cls", None)  # pylint: disable=protected-access

        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        def prepare_request(next_link=None):
            if not next_link:

                _request = build_items_list_dataflows_request(
                    workspace_id=workspace_id,
                    continuation_token_parameter=continuation_token_parameter,
                    headers=_headers,
                    params=_params,
                )
                _request.url = self._client.format_url(_request.url)

            else:
                _request = HttpRequest("GET", next_link)
                _request.url = self._client.format_url(_request.url)

            return _request

        def extract_data(pipeline_response):
            deserialized = self._deserialize(
                _models._models.Dataflows, pipeline_response  # pylint: disable=protected-access
            )
            list_of_elem = deserialized.value
            if cls:
                list_of_elem = cls(list_of_elem)  # type: ignore
            return deserialized.continuation_uri or None, iter(list_of_elem)

        def get_next(next_link=None):
            _request = prepare_request(next_link)

            _stream = False
            pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
                _request, stream=_stream, **kwargs
            )
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
                raise HttpResponseError(response=response, model=error)

            return pipeline_response

        return ItemPaged(get_next, extract_data)

    def _create_dataflow_initial(
        self, workspace_id: str, create_dataflow_request: Union[_models.CreateDataflowRequest, IO[bytes]], **kwargs: Any
    ) -> Iterator[bytes]:
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[Iterator[bytes]] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _json = None
        _content = None
        if isinstance(create_dataflow_request, (IOBase, bytes)):
            _content = create_dataflow_request
        else:
            _json = self._serialize.body(create_dataflow_request, "CreateDataflowRequest")

        _request = build_items_create_dataflow_request(
            workspace_id=workspace_id,
            content_type=content_type,
            json=_json,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = True
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [201, 202]:
            response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        if response.status_code == 201:
            deserialized = response.iter_bytes()

        if response.status_code == 202:
            response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))
            response_headers["x-ms-operation-id"] = self._deserialize("str", response.headers.get("x-ms-operation-id"))
            response_headers["Retry-After"] = self._deserialize("int", response.headers.get("Retry-After"))

            deserialized = response.iter_bytes()

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @overload
    def begin_create_dataflow(
        self,
        workspace_id: str,
        create_dataflow_request: _models.CreateDataflowRequest,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> LROPoller[_models.Dataflow]:
        """Creates a Dataflow in the specified workspace.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

         To create Dataflow with a public definition, refer to `Dataflow
        </rest/api/fabric/articles/item-management/definitions/dataflow-definition>`_ article.

        Permissions
        -----------

         The caller must have a *contributor* workspace role.

        Required Delegated Scopes
        -------------------------

         Dataflow.ReadWrite.All or Item.ReadWrite.All

        Limitations
        -----------


        * To create a Dataflow the workspace must be on a supported Fabric capacity. For more
        information see: `Microsoft Fabric license types
        </fabric/enterprise/licenses#microsoft-fabric-license-types>`_.

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param create_dataflow_request: Create item request payload. Required.
        :type create_dataflow_request: ~microsoft.fabric.api.dataflow.models.CreateDataflowRequest
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: An instance of LROPoller that returns Dataflow
        :rtype: ~azure.core.polling.LROPoller[~microsoft.fabric.api.dataflow.models.Dataflow]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    def begin_create_dataflow(
        self,
        workspace_id: str,
        create_dataflow_request: IO[bytes],
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> LROPoller[_models.Dataflow]:
        """Creates a Dataflow in the specified workspace.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

         To create Dataflow with a public definition, refer to `Dataflow
        </rest/api/fabric/articles/item-management/definitions/dataflow-definition>`_ article.

        Permissions
        -----------

         The caller must have a *contributor* workspace role.

        Required Delegated Scopes
        -------------------------

         Dataflow.ReadWrite.All or Item.ReadWrite.All

        Limitations
        -----------


        * To create a Dataflow the workspace must be on a supported Fabric capacity. For more
        information see: `Microsoft Fabric license types
        </fabric/enterprise/licenses#microsoft-fabric-license-types>`_.

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param create_dataflow_request: Create item request payload. Required.
        :type create_dataflow_request: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: An instance of LROPoller that returns Dataflow
        :rtype: ~azure.core.polling.LROPoller[~microsoft.fabric.api.dataflow.models.Dataflow]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace
    def begin_create_dataflow(
        self, workspace_id: str, create_dataflow_request: Union[_models.CreateDataflowRequest, IO[bytes]], **kwargs: Any
    ) -> LROPoller[_models.Dataflow]:
        """Creates a Dataflow in the specified workspace.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

         To create Dataflow with a public definition, refer to `Dataflow
        </rest/api/fabric/articles/item-management/definitions/dataflow-definition>`_ article.

        Permissions
        -----------

         The caller must have a *contributor* workspace role.

        Required Delegated Scopes
        -------------------------

         Dataflow.ReadWrite.All or Item.ReadWrite.All

        Limitations
        -----------


        * To create a Dataflow the workspace must be on a supported Fabric capacity. For more
        information see: `Microsoft Fabric license types
        </fabric/enterprise/licenses#microsoft-fabric-license-types>`_.

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param create_dataflow_request: Create item request payload. Is either a CreateDataflowRequest
         type or a IO[bytes] type. Required.
        :type create_dataflow_request: ~microsoft.fabric.api.dataflow.models.CreateDataflowRequest or
         IO[bytes]
        :return: An instance of LROPoller that returns Dataflow
        :rtype: ~azure.core.polling.LROPoller[~microsoft.fabric.api.dataflow.models.Dataflow]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.Dataflow] = kwargs.pop("cls", None)
        polling: Union[bool, PollingMethod] = kwargs.pop("polling", True)
        lro_delay = kwargs.pop("polling_interval", self._config.polling_interval)
        cont_token: Optional[str] = kwargs.pop("continuation_token", None)
        if cont_token is None:
            raw_result = self._create_dataflow_initial(
                workspace_id=workspace_id,
                create_dataflow_request=create_dataflow_request,
                content_type=content_type,
                cls=lambda x, y, z: x,
                headers=_headers,
                params=_params,
                **kwargs
            )
            raw_result.http_response.read()  # type: ignore
        kwargs.pop("error_map", None)

        def get_long_running_output(pipeline_response):
            deserialized = self._deserialize("Dataflow", pipeline_response.http_response)
            if cls:
                return cls(pipeline_response, deserialized, {})  # type: ignore
            return deserialized

        if polling is True:
            polling_method: PollingMethod = cast(PollingMethod, LROBasePolling(lro_delay, **kwargs))
        elif polling is False:
            polling_method = cast(PollingMethod, NoPolling())
        else:
            polling_method = polling
        if cont_token:
            return LROPoller[_models.Dataflow].from_continuation_token(
                polling_method=polling_method,
                continuation_token=cont_token,
                client=self._client,
                deserialization_callback=get_long_running_output,
            )
        return LROPoller[_models.Dataflow](
            self._client, raw_result, get_long_running_output, polling_method  # type: ignore
        )

    @distributed_trace
    def get_dataflow(self, workspace_id: str, dataflow_id: str, **kwargs: Any) -> _models.Dataflow:
        """Returns properties of the specified Dataflow.

        Permissions
        -----------

         The caller must have *read* permissions for the dataflow.

        Required Delegated Scopes
        -------------------------

         Dataflow.Read.All or Dataflow.ReadWrite.All or Item.Read.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The Dataflow ID. Required.
        :type dataflow_id: str
        :return: Dataflow
        :rtype: ~microsoft.fabric.api.dataflow.models.Dataflow
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.Dataflow] = kwargs.pop("cls", None)

        _request = build_items_get_dataflow_request(
            workspace_id=workspace_id,
            dataflow_id=dataflow_id,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        deserialized = self._deserialize("Dataflow", pipeline_response.http_response)

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    def update_dataflow(
        self,
        workspace_id: str,
        dataflow_id: str,
        update_dataflow_request: _models.UpdateDataflowRequest,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.Dataflow:
        """Updates the properties of the specified Dataflow.

        Permissions
        -----------

         The caller must have *read and write* permissions for the dataflow.

        Required Delegated Scopes
        -------------------------

         Dataflow.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The Dataflow ID. Required.
        :type dataflow_id: str
        :param update_dataflow_request: Update Dataflow request payload. Required.
        :type update_dataflow_request: ~microsoft.fabric.api.dataflow.models.UpdateDataflowRequest
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Dataflow
        :rtype: ~microsoft.fabric.api.dataflow.models.Dataflow
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    def update_dataflow(
        self,
        workspace_id: str,
        dataflow_id: str,
        update_dataflow_request: IO[bytes],
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.Dataflow:
        """Updates the properties of the specified Dataflow.

        Permissions
        -----------

         The caller must have *read and write* permissions for the dataflow.

        Required Delegated Scopes
        -------------------------

         Dataflow.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The Dataflow ID. Required.
        :type dataflow_id: str
        :param update_dataflow_request: Update Dataflow request payload. Required.
        :type update_dataflow_request: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Dataflow
        :rtype: ~microsoft.fabric.api.dataflow.models.Dataflow
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace
    def update_dataflow(
        self,
        workspace_id: str,
        dataflow_id: str,
        update_dataflow_request: Union[_models.UpdateDataflowRequest, IO[bytes]],
        **kwargs: Any
    ) -> _models.Dataflow:
        """Updates the properties of the specified Dataflow.

        Permissions
        -----------

         The caller must have *read and write* permissions for the dataflow.

        Required Delegated Scopes
        -------------------------

         Dataflow.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The Dataflow ID. Required.
        :type dataflow_id: str
        :param update_dataflow_request: Update Dataflow request payload. Is either a
         UpdateDataflowRequest type or a IO[bytes] type. Required.
        :type update_dataflow_request: ~microsoft.fabric.api.dataflow.models.UpdateDataflowRequest or
         IO[bytes]
        :return: Dataflow
        :rtype: ~microsoft.fabric.api.dataflow.models.Dataflow
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.Dataflow] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _json = None
        _content = None
        if isinstance(update_dataflow_request, (IOBase, bytes)):
            _content = update_dataflow_request
        else:
            _json = self._serialize.body(update_dataflow_request, "UpdateDataflowRequest")

        _request = build_items_update_dataflow_request(
            workspace_id=workspace_id,
            dataflow_id=dataflow_id,
            content_type=content_type,
            json=_json,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        deserialized = self._deserialize("Dataflow", pipeline_response.http_response)

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace
    def delete_dataflow(  # pylint: disable=inconsistent-return-statements
        self, workspace_id: str, dataflow_id: str, **kwargs: Any
    ) -> None:
        """Deletes the specified Dataflow.

        Permissions
        -----------

         The caller must have *write* permissions for the dataflow.

        Required Delegated Scopes
        -------------------------

         Dataflow.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The Dataflow ID. Required.
        :type dataflow_id: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[None] = kwargs.pop("cls", None)

        _request = build_items_delete_dataflow_request(
            workspace_id=workspace_id,
            dataflow_id=dataflow_id,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        if cls:
            return cls(pipeline_response, None, {})  # type: ignore

    def _get_dataflow_definition_initial(self, workspace_id: str, dataflow_id: str, **kwargs: Any) -> Iterator[bytes]:
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[Iterator[bytes]] = kwargs.pop("cls", None)

        _request = build_items_get_dataflow_definition_request(
            workspace_id=workspace_id,
            dataflow_id=dataflow_id,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = True
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200, 202]:
            response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        if response.status_code == 200:
            deserialized = response.iter_bytes()

        if response.status_code == 202:
            response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))
            response_headers["x-ms-operation-id"] = self._deserialize("str", response.headers.get("x-ms-operation-id"))
            response_headers["Retry-After"] = self._deserialize("int", response.headers.get("Retry-After"))

            deserialized = response.iter_bytes()

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace
    def begin_get_dataflow_definition(
        self, workspace_id: str, dataflow_id: str, **kwargs: Any
    ) -> LROPoller[_models.DataflowDefinitionResponse]:
        """Returns the specified Dataflow public definition.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        When you get a Dataflow's public definition, the sensitivity label is not a part of the
        definition.

        Permissions
        -----------

         The caller must have *read and write* permissions for the dataflow.

        Required Delegated Scopes
        -------------------------

         Dataflow.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The Dataflow ID. Required.
        :type dataflow_id: str
        :return: An instance of LROPoller that returns DataflowDefinitionResponse
        :rtype:
         ~azure.core.polling.LROPoller[~microsoft.fabric.api.dataflow.models.DataflowDefinitionResponse]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.DataflowDefinitionResponse] = kwargs.pop("cls", None)
        polling: Union[bool, PollingMethod] = kwargs.pop("polling", True)
        lro_delay = kwargs.pop("polling_interval", self._config.polling_interval)
        cont_token: Optional[str] = kwargs.pop("continuation_token", None)
        if cont_token is None:
            raw_result = self._get_dataflow_definition_initial(
                workspace_id=workspace_id,
                dataflow_id=dataflow_id,
                cls=lambda x, y, z: x,
                headers=_headers,
                params=_params,
                **kwargs
            )
            raw_result.http_response.read()  # type: ignore
        kwargs.pop("error_map", None)

        def get_long_running_output(pipeline_response):
            deserialized = self._deserialize("DataflowDefinitionResponse", pipeline_response.http_response)
            if cls:
                return cls(pipeline_response, deserialized, {})  # type: ignore
            return deserialized

        if polling is True:
            polling_method: PollingMethod = cast(PollingMethod, LROBasePolling(lro_delay, **kwargs))
        elif polling is False:
            polling_method = cast(PollingMethod, NoPolling())
        else:
            polling_method = polling
        if cont_token:
            return LROPoller[_models.DataflowDefinitionResponse].from_continuation_token(
                polling_method=polling_method,
                continuation_token=cont_token,
                client=self._client,
                deserialization_callback=get_long_running_output,
            )
        return LROPoller[_models.DataflowDefinitionResponse](
            self._client, raw_result, get_long_running_output, polling_method  # type: ignore
        )

    def _update_dataflow_definition_initial(
        self,
        workspace_id: str,
        dataflow_id: str,
        update_dataflow_definition_request: Union[_models.UpdateDataflowDefinitionRequest, IO[bytes]],
        *,
        update_metadata: Optional[bool] = None,
        **kwargs: Any
    ) -> Iterator[bytes]:
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[Iterator[bytes]] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _json = None
        _content = None
        if isinstance(update_dataflow_definition_request, (IOBase, bytes)):
            _content = update_dataflow_definition_request
        else:
            _json = self._serialize.body(update_dataflow_definition_request, "UpdateDataflowDefinitionRequest")

        _request = build_items_update_dataflow_definition_request(
            workspace_id=workspace_id,
            dataflow_id=dataflow_id,
            update_metadata=update_metadata,
            content_type=content_type,
            json=_json,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = True
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200, 202]:
            response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        if response.status_code == 200:
            deserialized = response.iter_bytes()

        if response.status_code == 202:
            response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))
            response_headers["x-ms-operation-id"] = self._deserialize("str", response.headers.get("x-ms-operation-id"))
            response_headers["Retry-After"] = self._deserialize("int", response.headers.get("Retry-After"))

            deserialized = response.iter_bytes()

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @overload
    def begin_update_dataflow_definition(
        self,
        workspace_id: str,
        dataflow_id: str,
        update_dataflow_definition_request: _models.UpdateDataflowDefinitionRequest,
        *,
        update_metadata: Optional[bool] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> LROPoller[None]:
        """Overrides the definition for the specified Dataflow.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Updating the Dataflow's definition, does not affect its sensitivity label.

        Permissions
        -----------

         The caller must have *read and write* permissions for the dataflow.

        Required Delegated Scopes
        -------------------------

         Dataflow.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The Dataflow ID. Required.
        :type dataflow_id: str
        :param update_dataflow_definition_request: Update Dataflow definition request payload.
         Required.
        :type update_dataflow_definition_request:
         ~microsoft.fabric.api.dataflow.models.UpdateDataflowDefinitionRequest
        :keyword update_metadata: When set to true and the .platform file is provided as part of the
         definition, the item's metadata is updated using the metadata in the .platform file. Default
         value is None.
        :paramtype update_metadata: bool
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: An instance of LROPoller that returns None
        :rtype: ~azure.core.polling.LROPoller[None]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    def begin_update_dataflow_definition(
        self,
        workspace_id: str,
        dataflow_id: str,
        update_dataflow_definition_request: IO[bytes],
        *,
        update_metadata: Optional[bool] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> LROPoller[None]:
        """Overrides the definition for the specified Dataflow.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Updating the Dataflow's definition, does not affect its sensitivity label.

        Permissions
        -----------

         The caller must have *read and write* permissions for the dataflow.

        Required Delegated Scopes
        -------------------------

         Dataflow.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The Dataflow ID. Required.
        :type dataflow_id: str
        :param update_dataflow_definition_request: Update Dataflow definition request payload.
         Required.
        :type update_dataflow_definition_request: IO[bytes]
        :keyword update_metadata: When set to true and the .platform file is provided as part of the
         definition, the item's metadata is updated using the metadata in the .platform file. Default
         value is None.
        :paramtype update_metadata: bool
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: An instance of LROPoller that returns None
        :rtype: ~azure.core.polling.LROPoller[None]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace
    def begin_update_dataflow_definition(
        self,
        workspace_id: str,
        dataflow_id: str,
        update_dataflow_definition_request: Union[_models.UpdateDataflowDefinitionRequest, IO[bytes]],
        *,
        update_metadata: Optional[bool] = None,
        **kwargs: Any
    ) -> LROPoller[None]:
        """Overrides the definition for the specified Dataflow.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Updating the Dataflow's definition, does not affect its sensitivity label.

        Permissions
        -----------

         The caller must have *read and write* permissions for the dataflow.

        Required Delegated Scopes
        -------------------------

         Dataflow.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The Dataflow ID. Required.
        :type dataflow_id: str
        :param update_dataflow_definition_request: Update Dataflow definition request payload. Is
         either a UpdateDataflowDefinitionRequest type or a IO[bytes] type. Required.
        :type update_dataflow_definition_request:
         ~microsoft.fabric.api.dataflow.models.UpdateDataflowDefinitionRequest or IO[bytes]
        :keyword update_metadata: When set to true and the .platform file is provided as part of the
         definition, the item's metadata is updated using the metadata in the .platform file. Default
         value is None.
        :paramtype update_metadata: bool
        :return: An instance of LROPoller that returns None
        :rtype: ~azure.core.polling.LROPoller[None]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[None] = kwargs.pop("cls", None)
        polling: Union[bool, PollingMethod] = kwargs.pop("polling", True)
        lro_delay = kwargs.pop("polling_interval", self._config.polling_interval)
        cont_token: Optional[str] = kwargs.pop("continuation_token", None)
        if cont_token is None:
            raw_result = self._update_dataflow_definition_initial(
                workspace_id=workspace_id,
                dataflow_id=dataflow_id,
                update_dataflow_definition_request=update_dataflow_definition_request,
                update_metadata=update_metadata,
                content_type=content_type,
                cls=lambda x, y, z: x,
                headers=_headers,
                params=_params,
                **kwargs
            )
            raw_result.http_response.read()  # type: ignore
        kwargs.pop("error_map", None)

        def get_long_running_output(pipeline_response):  # pylint: disable=inconsistent-return-statements
            if cls:
                return cls(pipeline_response, None, {})  # type: ignore

        if polling is True:
            polling_method: PollingMethod = cast(PollingMethod, LROBasePolling(lro_delay, **kwargs))
        elif polling is False:
            polling_method = cast(PollingMethod, NoPolling())
        else:
            polling_method = polling
        if cont_token:
            return LROPoller[None].from_continuation_token(
                polling_method=polling_method,
                continuation_token=cont_token,
                client=self._client,
                deserialization_callback=get_long_running_output,
            )
        return LROPoller[None](self._client, raw_result, get_long_running_output, polling_method)  # type: ignore

    @distributed_trace
    def discover_dataflow_parameters(
        self, workspace_id: str, dataflow_id: str, *, continuation_token_parameter: Optional[str] = None, **kwargs: Any
    ) -> Iterable["_models.DataflowParameter"]:
        """Retrieves all parameters defined in the specified Dataflow.

        Permissions
        -----------

        The caller must have *read* permissions for the dataflow.

        Required Delegated Scopes
        -------------------------

        Dataflow.Read.All or Dataflow.ReadWrite.All or Item.Read.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The Dataflow ID. Required.
        :type dataflow_id: str
        :keyword continuation_token_parameter: A token for retrieving the next page of results. Default
         value is None.
        :paramtype continuation_token_parameter: str
        :return: An iterator like instance of DataflowParameter
        :rtype: ~azure.core.paging.ItemPaged[~microsoft.fabric.api.dataflow.models.DataflowParameter]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models._models.DataflowParameters] = kwargs.pop("cls", None)  # pylint: disable=protected-access

        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        def prepare_request(next_link=None):
            if not next_link:

                _request = build_items_discover_dataflow_parameters_request(
                    workspace_id=workspace_id,
                    dataflow_id=dataflow_id,
                    continuation_token_parameter=continuation_token_parameter,
                    headers=_headers,
                    params=_params,
                )
                _request.url = self._client.format_url(_request.url)

            else:
                _request = HttpRequest("GET", next_link)
                _request.url = self._client.format_url(_request.url)

            return _request

        def extract_data(pipeline_response):
            deserialized = self._deserialize(
                _models._models.DataflowParameters, pipeline_response  # pylint: disable=protected-access
            )
            list_of_elem = deserialized.value
            if cls:
                list_of_elem = cls(list_of_elem)  # type: ignore
            return deserialized.continuation_uri or None, iter(list_of_elem)

        def get_next(next_link=None):
            _request = prepare_request(next_link)

            _stream = False
            pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
                _request, stream=_stream, **kwargs
            )
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
                raise HttpResponseError(response=response, model=error)

            return pipeline_response

        return ItemPaged(get_next, extract_data)


class BackgroundJobsOperations:
    """
    .. warning::
        **DO NOT** instantiate this class directly.

        Instead, you should access the following operations through
        :class:`~microsoft.fabric.api.dataflow.FabricDataflowClient`'s
        :attr:`background_jobs` attribute.
    """

    models = _models

    def __init__(self, *args, **kwargs):
        input_args = list(args)
        self._client = input_args.pop(0) if input_args else kwargs.pop("client")
        self._config = input_args.pop(0) if input_args else kwargs.pop("config")
        self._serialize = input_args.pop(0) if input_args else kwargs.pop("serializer")
        self._deserialize = input_args.pop(0) if input_args else kwargs.pop("deserializer")

    @overload
    def schedule_execute(
        self,
        workspace_id: str,
        dataflow_id: str,
        create_schedule_request: _models.CreateDataflowExecuteScheduleRequest,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.DataflowSchedule:
        """Create a new execute schedule for a dataflow. A dataflow can create maximum 20 schedulers.

        ..

           [!NOTE]
           This API is part of a Preview release and is provided for evaluation and development
        purposes only. It may change based on feedback and is not recommended for production use.


        Required Delegated Scopes:
        --------------------------

        Dataflow.Execute.All and Dataflow.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - No


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The item ID. Required.
        :type dataflow_id: str
        :param create_schedule_request: A dataflow execute schedule create request. Required.
        :type create_schedule_request:
         ~microsoft.fabric.api.dataflow.models.CreateDataflowExecuteScheduleRequest
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: DataflowSchedule
        :rtype: ~microsoft.fabric.api.dataflow.models.DataflowSchedule
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    def schedule_execute(
        self,
        workspace_id: str,
        dataflow_id: str,
        create_schedule_request: IO[bytes],
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.DataflowSchedule:
        """Create a new execute schedule for a dataflow. A dataflow can create maximum 20 schedulers.

        ..

           [!NOTE]
           This API is part of a Preview release and is provided for evaluation and development
        purposes only. It may change based on feedback and is not recommended for production use.


        Required Delegated Scopes:
        --------------------------

        Dataflow.Execute.All and Dataflow.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - No


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The item ID. Required.
        :type dataflow_id: str
        :param create_schedule_request: A dataflow execute schedule create request. Required.
        :type create_schedule_request: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: DataflowSchedule
        :rtype: ~microsoft.fabric.api.dataflow.models.DataflowSchedule
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace
    def schedule_execute(
        self,
        workspace_id: str,
        dataflow_id: str,
        create_schedule_request: Union[_models.CreateDataflowExecuteScheduleRequest, IO[bytes]],
        **kwargs: Any
    ) -> _models.DataflowSchedule:
        """Create a new execute schedule for a dataflow. A dataflow can create maximum 20 schedulers.

        ..

           [!NOTE]
           This API is part of a Preview release and is provided for evaluation and development
        purposes only. It may change based on feedback and is not recommended for production use.


        Required Delegated Scopes:
        --------------------------

        Dataflow.Execute.All and Dataflow.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - No


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The item ID. Required.
        :type dataflow_id: str
        :param create_schedule_request: A dataflow execute schedule create request. Is either a
         CreateDataflowExecuteScheduleRequest type or a IO[bytes] type. Required.
        :type create_schedule_request:
         ~microsoft.fabric.api.dataflow.models.CreateDataflowExecuteScheduleRequest or IO[bytes]
        :return: DataflowSchedule
        :rtype: ~microsoft.fabric.api.dataflow.models.DataflowSchedule
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.DataflowSchedule] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _json = None
        _content = None
        if isinstance(create_schedule_request, (IOBase, bytes)):
            _content = create_schedule_request
        else:
            _json = self._serialize.body(create_schedule_request, "CreateDataflowExecuteScheduleRequest")

        _request = build_background_jobs_schedule_execute_request(
            workspace_id=workspace_id,
            dataflow_id=dataflow_id,
            content_type=content_type,
            json=_json,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [201]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))

        deserialized = self._deserialize("DataflowSchedule", pipeline_response.http_response)

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @overload
    def schedule_apply_changes(
        self,
        workspace_id: str,
        dataflow_id: str,
        create_schedule_request: _models.CreateDataflowApplyChangesScheduleRequest,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.DataflowSchedule:
        """Create a new apply changes schedule for a dataflow. A dataflow can create maximum 20
        schedulers.

        ..

           [!NOTE]
           This API is part of a Preview release and is provided for evaluation and development
        purposes only. It may change based on feedback and is not recommended for production use.


        Required Delegated Scopes:
        --------------------------

        Dataflow.Execute.All and Dataflow.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - No


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The item ID. Required.
        :type dataflow_id: str
        :param create_schedule_request: A apply changes dataflow schedule create request. Required.
        :type create_schedule_request:
         ~microsoft.fabric.api.dataflow.models.CreateDataflowApplyChangesScheduleRequest
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: DataflowSchedule
        :rtype: ~microsoft.fabric.api.dataflow.models.DataflowSchedule
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    def schedule_apply_changes(
        self,
        workspace_id: str,
        dataflow_id: str,
        create_schedule_request: IO[bytes],
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.DataflowSchedule:
        """Create a new apply changes schedule for a dataflow. A dataflow can create maximum 20
        schedulers.

        ..

           [!NOTE]
           This API is part of a Preview release and is provided for evaluation and development
        purposes only. It may change based on feedback and is not recommended for production use.


        Required Delegated Scopes:
        --------------------------

        Dataflow.Execute.All and Dataflow.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - No


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The item ID. Required.
        :type dataflow_id: str
        :param create_schedule_request: A apply changes dataflow schedule create request. Required.
        :type create_schedule_request: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: DataflowSchedule
        :rtype: ~microsoft.fabric.api.dataflow.models.DataflowSchedule
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace
    def schedule_apply_changes(
        self,
        workspace_id: str,
        dataflow_id: str,
        create_schedule_request: Union[_models.CreateDataflowApplyChangesScheduleRequest, IO[bytes]],
        **kwargs: Any
    ) -> _models.DataflowSchedule:
        """Create a new apply changes schedule for a dataflow. A dataflow can create maximum 20
        schedulers.

        ..

           [!NOTE]
           This API is part of a Preview release and is provided for evaluation and development
        purposes only. It may change based on feedback and is not recommended for production use.


        Required Delegated Scopes:
        --------------------------

        Dataflow.Execute.All and Dataflow.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - No


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The item ID. Required.
        :type dataflow_id: str
        :param create_schedule_request: A apply changes dataflow schedule create request. Is either a
         CreateDataflowApplyChangesScheduleRequest type or a IO[bytes] type. Required.
        :type create_schedule_request:
         ~microsoft.fabric.api.dataflow.models.CreateDataflowApplyChangesScheduleRequest or IO[bytes]
        :return: DataflowSchedule
        :rtype: ~microsoft.fabric.api.dataflow.models.DataflowSchedule
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.DataflowSchedule] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _json = None
        _content = None
        if isinstance(create_schedule_request, (IOBase, bytes)):
            _content = create_schedule_request
        else:
            _json = self._serialize.body(create_schedule_request, "CreateDataflowApplyChangesScheduleRequest")

        _request = build_background_jobs_schedule_apply_changes_request(
            workspace_id=workspace_id,
            dataflow_id=dataflow_id,
            content_type=content_type,
            json=_json,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [201]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))

        deserialized = self._deserialize("DataflowSchedule", pipeline_response.http_response)

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @overload
    def run_on_demand_execute(  # pylint: disable=inconsistent-return-statements
        self,
        workspace_id: str,
        dataflow_id: str,
        run_on_demand_item_job_request: Optional[_models.RunOnDemandDataflowExecuteJobRequest] = None,
        *,
        job_type: str,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> None:
        """Run on-demand execute job instance.

        ..

           [!NOTE]
           This API is part of a Preview release and is provided for evaluation and development
        purposes only. It may change based on feedback and is not recommended for production use.


        Required Delegated Scopes
        -------------------------

         For dataflows APIs use these scope types:


        *
          Specific scope: Dataflow.Execute.All

          for more information about scopes, see: `scopes article </rest/api/fabric/articles/scopes>`_.

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - No


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The dataflow ID. Required.
        :type dataflow_id: str
        :param run_on_demand_item_job_request: Run on-demand item job request payload. Default value is
         None.
        :type run_on_demand_item_job_request:
         ~microsoft.fabric.api.dataflow.models.RunOnDemandDataflowExecuteJobRequest
        :keyword job_type: *Execute* job type. *Refresh* can also be used as an alias for *Execute*.
         Required.
        :paramtype job_type: str
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    def run_on_demand_execute(  # pylint: disable=inconsistent-return-statements
        self,
        workspace_id: str,
        dataflow_id: str,
        run_on_demand_item_job_request: Optional[IO[bytes]] = None,
        *,
        job_type: str,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> None:
        """Run on-demand execute job instance.

        ..

           [!NOTE]
           This API is part of a Preview release and is provided for evaluation and development
        purposes only. It may change based on feedback and is not recommended for production use.


        Required Delegated Scopes
        -------------------------

         For dataflows APIs use these scope types:


        *
          Specific scope: Dataflow.Execute.All

          for more information about scopes, see: `scopes article </rest/api/fabric/articles/scopes>`_.

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - No


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The dataflow ID. Required.
        :type dataflow_id: str
        :param run_on_demand_item_job_request: Run on-demand item job request payload. Default value is
         None.
        :type run_on_demand_item_job_request: IO[bytes]
        :keyword job_type: *Execute* job type. *Refresh* can also be used as an alias for *Execute*.
         Required.
        :paramtype job_type: str
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace
    def run_on_demand_execute(  # pylint: disable=inconsistent-return-statements
        self,
        workspace_id: str,
        dataflow_id: str,
        run_on_demand_item_job_request: Optional[Union[_models.RunOnDemandDataflowExecuteJobRequest, IO[bytes]]] = None,
        *,
        job_type: str,
        **kwargs: Any
    ) -> None:
        """Run on-demand execute job instance.

        ..

           [!NOTE]
           This API is part of a Preview release and is provided for evaluation and development
        purposes only. It may change based on feedback and is not recommended for production use.


        Required Delegated Scopes
        -------------------------

         For dataflows APIs use these scope types:


        *
          Specific scope: Dataflow.Execute.All

          for more information about scopes, see: `scopes article </rest/api/fabric/articles/scopes>`_.

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - No


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The dataflow ID. Required.
        :type dataflow_id: str
        :param run_on_demand_item_job_request: Run on-demand item job request payload. Is either a
         RunOnDemandDataflowExecuteJobRequest type or a IO[bytes] type. Default value is None.
        :type run_on_demand_item_job_request:
         ~microsoft.fabric.api.dataflow.models.RunOnDemandDataflowExecuteJobRequest or IO[bytes]
        :keyword job_type: *Execute* job type. *Refresh* can also be used as an alias for *Execute*.
         Required.
        :paramtype job_type: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[None] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _json = None
        _content = None
        if isinstance(run_on_demand_item_job_request, (IOBase, bytes)):
            _content = run_on_demand_item_job_request
        else:
            if run_on_demand_item_job_request is not None:
                _json = self._serialize.body(run_on_demand_item_job_request, "RunOnDemandDataflowExecuteJobRequest")
            else:
                _json = None

        _request = build_background_jobs_run_on_demand_execute_request(
            workspace_id=workspace_id,
            dataflow_id=dataflow_id,
            job_type=job_type,
            content_type=content_type,
            json=_json,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [202]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))
        response_headers["Retry-After"] = self._deserialize("int", response.headers.get("Retry-After"))

        if cls:
            return cls(pipeline_response, None, response_headers)  # type: ignore

    @distributed_trace
    def run_on_demand_apply_changes(  # pylint: disable=inconsistent-return-statements
        self, workspace_id: str, dataflow_id: str, *, job_type: str, **kwargs: Any
    ) -> None:
        """Run on-demand apply changes job instance.

        ..

           [!NOTE]
           This API is part of a Preview release and is provided for evaluation and development
        purposes only. It may change based on feedback and is not recommended for production use.


        Required Delegated Scopes
        -------------------------

         For dataflows APIs use these scope types:


        *
          Specific scope: Dataflow.Execute.All

          for more information about scopes, see: `scopes article </rest/api/fabric/articles/scopes>`_.

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - No


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param dataflow_id: The dataflow ID. Required.
        :type dataflow_id: str
        :keyword job_type: *ApplyChanges* job type. Required.
        :paramtype job_type: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[None] = kwargs.pop("cls", None)

        _request = build_background_jobs_run_on_demand_apply_changes_request(
            workspace_id=workspace_id,
            dataflow_id=dataflow_id,
            job_type=job_type,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [202]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))
        response_headers["Retry-After"] = self._deserialize("int", response.headers.get("Retry-After"))

        if cls:
            return cls(pipeline_response, None, response_headers)  # type: ignore
