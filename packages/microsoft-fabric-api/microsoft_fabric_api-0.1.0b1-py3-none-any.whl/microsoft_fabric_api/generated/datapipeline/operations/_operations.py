# pylint: disable=too-many-lines,too-many-statements
# coding=utf-8
# --------------------------------------------------------------------------
# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.10.3, generator: @autorest/python@6.15.0)
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
from io import IOBase
import sys
from typing import Any, Callable, Dict, IO, Iterable, Iterator, Optional, Type, TypeVar, Union, cast, overload

from azure.core.exceptions import (
    ClientAuthenticationError,
    HttpResponseError,
    ResourceExistsError,
    ResourceNotFoundError,
    ResourceNotModifiedError,
    map_error,
)
from azure.core.paging import ItemPaged
from azure.core.pipeline import PipelineResponse
from azure.core.polling import LROPoller, NoPolling, PollingMethod
from azure.core.polling.base_polling import LROBasePolling
from azure.core.rest import HttpRequest, HttpResponse
from azure.core.tracing.decorator import distributed_trace
from azure.core.utils import case_insensitive_dict

from .. import models as _models
from .._serialization import Serializer

if sys.version_info >= (3, 9):
    from collections.abc import MutableMapping
else:
    from typing import MutableMapping  # type: ignore  # pylint: disable=ungrouped-imports
T = TypeVar("T")
ClsType = Optional[Callable[[PipelineResponse[HttpRequest, HttpResponse], T, Dict[str, Any]], Any]]

_SERIALIZER = Serializer()
_SERIALIZER.client_side_validation = False


def build_items_list_data_pipelines_request(
    workspace_id: str, *, continuation_token_parameter: Optional[str] = None, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataPipelines"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct parameters
    if continuation_token_parameter is not None:
        _params["continuationToken"] = _SERIALIZER.query(
            "continuation_token_parameter", continuation_token_parameter, "str"
        )

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="GET", url=_url, params=_params, headers=_headers, **kwargs)


def build_items_create_data_pipeline_request(workspace_id: str, **kwargs: Any) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataPipelines"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="POST", url=_url, headers=_headers, **kwargs)


def build_items_get_data_pipeline_request(workspace_id: str, data_pipeline_id: str, **kwargs: Any) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataPipelines/{dataPipelineId}"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "dataPipelineId": _SERIALIZER.url("data_pipeline_id", data_pipeline_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="GET", url=_url, headers=_headers, **kwargs)


def build_items_update_data_pipeline_request(workspace_id: str, data_pipeline_id: str, **kwargs: Any) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataPipelines/{dataPipelineId}"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "dataPipelineId": _SERIALIZER.url("data_pipeline_id", data_pipeline_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="PATCH", url=_url, headers=_headers, **kwargs)


def build_items_delete_data_pipeline_request(workspace_id: str, data_pipeline_id: str, **kwargs: Any) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataPipelines/{dataPipelineId}"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "dataPipelineId": _SERIALIZER.url("data_pipeline_id", data_pipeline_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="DELETE", url=_url, headers=_headers, **kwargs)


def build_items_get_data_pipeline_definition_request(  # pylint: disable=name-too-long
    workspace_id: str, data_pipeline_id: str, *, format: Optional[str] = None, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataPipelines/{dataPipelineId}/getDefinition"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "dataPipelineId": _SERIALIZER.url("data_pipeline_id", data_pipeline_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct parameters
    if format is not None:
        _params["format"] = _SERIALIZER.query("format", format, "str")

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="POST", url=_url, params=_params, headers=_headers, **kwargs)


def build_items_update_data_pipeline_definition_request(  # pylint: disable=name-too-long
    workspace_id: str, data_pipeline_id: str, *, update_metadata: Optional[bool] = None, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/dataPipelines/{dataPipelineId}/updateDefinition"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "dataPipelineId": _SERIALIZER.url("data_pipeline_id", data_pipeline_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct parameters
    if update_metadata is not None:
        _params["updateMetadata"] = _SERIALIZER.query("update_metadata", update_metadata, "bool")

    # Construct headers
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="POST", url=_url, params=_params, headers=_headers, **kwargs)


class ItemsOperations:
    """
    .. warning::
        **DO NOT** instantiate this class directly.

        Instead, you should access the following operations through
        :class:`~microsoft.fabric.api.datapipeline.FabricDataPipelineClient`'s
        :attr:`items` attribute.
    """

    models = _models

    def __init__(self, *args, **kwargs):
        input_args = list(args)
        self._client = input_args.pop(0) if input_args else kwargs.pop("client")
        self._config = input_args.pop(0) if input_args else kwargs.pop("config")
        self._serialize = input_args.pop(0) if input_args else kwargs.pop("serializer")
        self._deserialize = input_args.pop(0) if input_args else kwargs.pop("deserializer")

    @distributed_trace
    def list_data_pipelines(
        self, workspace_id: str, *, continuation_token_parameter: Optional[str] = None, **kwargs: Any
    ) -> Iterable["_models.DataPipeline"]:
        """Returns a list of data pipelines from the specified workspace.

        This API supports `pagination </rest/api/fabric/articles/pagination>`_.

        Permissions
        -----------

         The caller must have a *viewer* workspace role.

        Required Delegated Scopes
        -------------------------

        Workspace.Read.All or Workspace.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :keyword continuation_token_parameter: A token for retrieving the next page of results. Default
         value is None.
        :paramtype continuation_token_parameter: str
        :return: An iterator like instance of DataPipeline
        :rtype: ~azure.core.paging.ItemPaged[~microsoft.fabric.api.datapipeline.models.DataPipeline]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models._models.DataPipelines] = kwargs.pop("cls", None)  # pylint: disable=protected-access

        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        def prepare_request(next_link=None):
            if not next_link:

                _request = build_items_list_data_pipelines_request(
                    workspace_id=workspace_id,
                    continuation_token_parameter=continuation_token_parameter,
                    headers=_headers,
                    params=_params,
                )
                _request.url = self._client.format_url(_request.url)

            else:
                _request = HttpRequest("GET", next_link)
                _request.url = self._client.format_url(_request.url)

            return _request

        def extract_data(pipeline_response):
            deserialized = self._deserialize(
                _models._models.DataPipelines, pipeline_response  # pylint: disable=protected-access
            )
            list_of_elem = deserialized.value
            if cls:
                list_of_elem = cls(list_of_elem)  # type: ignore
            return deserialized.continuation_uri or None, iter(list_of_elem)

        def get_next(next_link=None):
            _request = prepare_request(next_link)

            _stream = False
            pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
                _request, stream=_stream, **kwargs
            )
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
                raise HttpResponseError(response=response, model=error)

            return pipeline_response

        return ItemPaged(get_next, extract_data)

    def _create_data_pipeline_initial(
        self,
        workspace_id: str,
        create_data_pipeline_request: Union[_models.CreateDataPipelineRequest, IO[bytes]],
        **kwargs: Any
    ) -> Iterator[bytes]:
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[Iterator[bytes]] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _json = None
        _content = None
        if isinstance(create_data_pipeline_request, (IOBase, bytes)):
            _content = create_data_pipeline_request
        else:
            _json = self._serialize.body(create_data_pipeline_request, "CreateDataPipelineRequest")

        _request = build_items_create_data_pipeline_request(
            workspace_id=workspace_id,
            content_type=content_type,
            json=_json,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = True
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [201, 202]:
            response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        if response.status_code == 201:
            deserialized = response.iter_bytes()

        if response.status_code == 202:
            response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))
            response_headers["x-ms-operation-id"] = self._deserialize("str", response.headers.get("x-ms-operation-id"))
            response_headers["Retry-After"] = self._deserialize("int", response.headers.get("Retry-After"))

            deserialized = response.iter_bytes()

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @overload
    def begin_create_data_pipeline(
        self,
        workspace_id: str,
        create_data_pipeline_request: _models.CreateDataPipelineRequest,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> LROPoller[_models.DataPipeline]:
        """Creates a data pipeline in the specified workspace.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Permissions
        -----------

         The caller must have a *contributor* workspace role.

        Required Delegated Scopes
        -------------------------

         DataPipeline.ReadWrite.All or Item.ReadWrite.All

        Limitations
        -----------


        * To create a data pipeline, the workspace must be on a supported Fabric capacity.

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param create_data_pipeline_request: Create item request payload. Required.
        :type create_data_pipeline_request:
         ~microsoft.fabric.api.datapipeline.models.CreateDataPipelineRequest
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: An instance of LROPoller that returns DataPipeline
        :rtype: ~azure.core.polling.LROPoller[~microsoft.fabric.api.datapipeline.models.DataPipeline]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    def begin_create_data_pipeline(
        self,
        workspace_id: str,
        create_data_pipeline_request: IO[bytes],
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> LROPoller[_models.DataPipeline]:
        """Creates a data pipeline in the specified workspace.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Permissions
        -----------

         The caller must have a *contributor* workspace role.

        Required Delegated Scopes
        -------------------------

         DataPipeline.ReadWrite.All or Item.ReadWrite.All

        Limitations
        -----------


        * To create a data pipeline, the workspace must be on a supported Fabric capacity.

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param create_data_pipeline_request: Create item request payload. Required.
        :type create_data_pipeline_request: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: An instance of LROPoller that returns DataPipeline
        :rtype: ~azure.core.polling.LROPoller[~microsoft.fabric.api.datapipeline.models.DataPipeline]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace
    def begin_create_data_pipeline(
        self,
        workspace_id: str,
        create_data_pipeline_request: Union[_models.CreateDataPipelineRequest, IO[bytes]],
        **kwargs: Any
    ) -> LROPoller[_models.DataPipeline]:
        """Creates a data pipeline in the specified workspace.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Permissions
        -----------

         The caller must have a *contributor* workspace role.

        Required Delegated Scopes
        -------------------------

         DataPipeline.ReadWrite.All or Item.ReadWrite.All

        Limitations
        -----------


        * To create a data pipeline, the workspace must be on a supported Fabric capacity.

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param create_data_pipeline_request: Create item request payload. Is either a
         CreateDataPipelineRequest type or a IO[bytes] type. Required.
        :type create_data_pipeline_request:
         ~microsoft.fabric.api.datapipeline.models.CreateDataPipelineRequest or IO[bytes]
        :return: An instance of LROPoller that returns DataPipeline
        :rtype: ~azure.core.polling.LROPoller[~microsoft.fabric.api.datapipeline.models.DataPipeline]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.DataPipeline] = kwargs.pop("cls", None)
        polling: Union[bool, PollingMethod] = kwargs.pop("polling", True)
        lro_delay = kwargs.pop("polling_interval", self._config.polling_interval)
        cont_token: Optional[str] = kwargs.pop("continuation_token", None)
        if cont_token is None:
            raw_result = self._create_data_pipeline_initial(
                workspace_id=workspace_id,
                create_data_pipeline_request=create_data_pipeline_request,
                content_type=content_type,
                cls=lambda x, y, z: x,
                headers=_headers,
                params=_params,
                **kwargs
            )
            raw_result.http_response.read()  # type: ignore
        kwargs.pop("error_map", None)

        def get_long_running_output(pipeline_response):
            deserialized = self._deserialize("DataPipeline", pipeline_response.http_response)
            if cls:
                return cls(pipeline_response, deserialized, {})  # type: ignore
            return deserialized

        if polling is True:
            polling_method: PollingMethod = cast(PollingMethod, LROBasePolling(lro_delay, **kwargs))
        elif polling is False:
            polling_method = cast(PollingMethod, NoPolling())
        else:
            polling_method = polling
        if cont_token:
            return LROPoller[_models.DataPipeline].from_continuation_token(
                polling_method=polling_method,
                continuation_token=cont_token,
                client=self._client,
                deserialization_callback=get_long_running_output,
            )
        return LROPoller[_models.DataPipeline](
            self._client, raw_result, get_long_running_output, polling_method  # type: ignore
        )

    @distributed_trace
    def get_data_pipeline(self, workspace_id: str, data_pipeline_id: str, **kwargs: Any) -> _models.DataPipeline:
        """Returns properties of the specified data pipeline.

        Permissions
        -----------

         The caller must have *read* permissions for the data pipeline.

        Required Delegated Scopes
        -------------------------

         DataPipeline.Read.All or DataPipeline.ReadWrite.All or Item.Read.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param data_pipeline_id: The data pipeline ID. Required.
        :type data_pipeline_id: str
        :return: DataPipeline
        :rtype: ~microsoft.fabric.api.datapipeline.models.DataPipeline
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.DataPipeline] = kwargs.pop("cls", None)

        _request = build_items_get_data_pipeline_request(
            workspace_id=workspace_id,
            data_pipeline_id=data_pipeline_id,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        deserialized = self._deserialize("DataPipeline", pipeline_response.http_response)

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    def update_data_pipeline(
        self,
        workspace_id: str,
        data_pipeline_id: str,
        update_data_pipeline_request: _models.UpdateDataPipelineRequest,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.DataPipeline:
        """Updates the properties of the specified data pipeline.

        Permissions
        -----------

         The caller must have *read and write* permissions for the data pipeline.

        Required Delegated Scopes
        -------------------------

         DataPipeline.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param data_pipeline_id: The data pipeline ID. Required.
        :type data_pipeline_id: str
        :param update_data_pipeline_request: Update data pipeline request payload. Required.
        :type update_data_pipeline_request:
         ~microsoft.fabric.api.datapipeline.models.UpdateDataPipelineRequest
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: DataPipeline
        :rtype: ~microsoft.fabric.api.datapipeline.models.DataPipeline
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    def update_data_pipeline(
        self,
        workspace_id: str,
        data_pipeline_id: str,
        update_data_pipeline_request: IO[bytes],
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.DataPipeline:
        """Updates the properties of the specified data pipeline.

        Permissions
        -----------

         The caller must have *read and write* permissions for the data pipeline.

        Required Delegated Scopes
        -------------------------

         DataPipeline.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param data_pipeline_id: The data pipeline ID. Required.
        :type data_pipeline_id: str
        :param update_data_pipeline_request: Update data pipeline request payload. Required.
        :type update_data_pipeline_request: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: DataPipeline
        :rtype: ~microsoft.fabric.api.datapipeline.models.DataPipeline
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace
    def update_data_pipeline(
        self,
        workspace_id: str,
        data_pipeline_id: str,
        update_data_pipeline_request: Union[_models.UpdateDataPipelineRequest, IO[bytes]],
        **kwargs: Any
    ) -> _models.DataPipeline:
        """Updates the properties of the specified data pipeline.

        Permissions
        -----------

         The caller must have *read and write* permissions for the data pipeline.

        Required Delegated Scopes
        -------------------------

         DataPipeline.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param data_pipeline_id: The data pipeline ID. Required.
        :type data_pipeline_id: str
        :param update_data_pipeline_request: Update data pipeline request payload. Is either a
         UpdateDataPipelineRequest type or a IO[bytes] type. Required.
        :type update_data_pipeline_request:
         ~microsoft.fabric.api.datapipeline.models.UpdateDataPipelineRequest or IO[bytes]
        :return: DataPipeline
        :rtype: ~microsoft.fabric.api.datapipeline.models.DataPipeline
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.DataPipeline] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _json = None
        _content = None
        if isinstance(update_data_pipeline_request, (IOBase, bytes)):
            _content = update_data_pipeline_request
        else:
            _json = self._serialize.body(update_data_pipeline_request, "UpdateDataPipelineRequest")

        _request = build_items_update_data_pipeline_request(
            workspace_id=workspace_id,
            data_pipeline_id=data_pipeline_id,
            content_type=content_type,
            json=_json,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        deserialized = self._deserialize("DataPipeline", pipeline_response.http_response)

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace
    def delete_data_pipeline(  # pylint: disable=inconsistent-return-statements
        self, workspace_id: str, data_pipeline_id: str, **kwargs: Any
    ) -> None:
        """Deletes the specified data pipeline.

        Permissions
        -----------

         The caller must have *write* permissions for the data pipeline.

        Required Delegated Scopes
        -------------------------

         DataPipeline.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param data_pipeline_id: The data pipeline ID. Required.
        :type data_pipeline_id: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[None] = kwargs.pop("cls", None)

        _request = build_items_delete_data_pipeline_request(
            workspace_id=workspace_id,
            data_pipeline_id=data_pipeline_id,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        if cls:
            return cls(pipeline_response, None, {})  # type: ignore

    def _get_data_pipeline_definition_initial(
        self, workspace_id: str, data_pipeline_id: str, *, format: Optional[str] = None, **kwargs: Any
    ) -> Iterator[bytes]:
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[Iterator[bytes]] = kwargs.pop("cls", None)

        _request = build_items_get_data_pipeline_definition_request(
            workspace_id=workspace_id,
            data_pipeline_id=data_pipeline_id,
            format=format,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = True
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200, 202]:
            response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        if response.status_code == 200:
            deserialized = response.iter_bytes()

        if response.status_code == 202:
            response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))
            response_headers["x-ms-operation-id"] = self._deserialize("str", response.headers.get("x-ms-operation-id"))
            response_headers["Retry-After"] = self._deserialize("int", response.headers.get("Retry-After"))

            deserialized = response.iter_bytes()

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace
    def begin_get_data_pipeline_definition(
        self, workspace_id: str, data_pipeline_id: str, *, format: Optional[str] = None, **kwargs: Any
    ) -> LROPoller[_models.DataPipelineDefinitionResponse]:
        """Returns the specified data pipeline public definition.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        When you get a DataPipeline's public definition, the sensitivity label is not a part of the
        definition.

        Permissions
        -----------

         The caller must have *read and write* permissions for the data pipeline.

        Required Delegated Scopes
        -------------------------

         DataPipeline.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param data_pipeline_id: The data pipeline ID. Required.
        :type data_pipeline_id: str
        :keyword format: The format of the data pipeline public definition. Default value is None.
        :paramtype format: str
        :return: An instance of LROPoller that returns DataPipelineDefinitionResponse
        :rtype:
         ~azure.core.polling.LROPoller[~microsoft.fabric.api.datapipeline.models.DataPipelineDefinitionResponse]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.DataPipelineDefinitionResponse] = kwargs.pop("cls", None)
        polling: Union[bool, PollingMethod] = kwargs.pop("polling", True)
        lro_delay = kwargs.pop("polling_interval", self._config.polling_interval)
        cont_token: Optional[str] = kwargs.pop("continuation_token", None)
        if cont_token is None:
            raw_result = self._get_data_pipeline_definition_initial(
                workspace_id=workspace_id,
                data_pipeline_id=data_pipeline_id,
                format=format,
                cls=lambda x, y, z: x,
                headers=_headers,
                params=_params,
                **kwargs
            )
            raw_result.http_response.read()  # type: ignore
        kwargs.pop("error_map", None)

        def get_long_running_output(pipeline_response):
            deserialized = self._deserialize("DataPipelineDefinitionResponse", pipeline_response.http_response)
            if cls:
                return cls(pipeline_response, deserialized, {})  # type: ignore
            return deserialized

        if polling is True:
            polling_method: PollingMethod = cast(PollingMethod, LROBasePolling(lro_delay, **kwargs))
        elif polling is False:
            polling_method = cast(PollingMethod, NoPolling())
        else:
            polling_method = polling
        if cont_token:
            return LROPoller[_models.DataPipelineDefinitionResponse].from_continuation_token(
                polling_method=polling_method,
                continuation_token=cont_token,
                client=self._client,
                deserialization_callback=get_long_running_output,
            )
        return LROPoller[_models.DataPipelineDefinitionResponse](
            self._client, raw_result, get_long_running_output, polling_method  # type: ignore
        )

    def _update_data_pipeline_definition_initial(
        self,
        workspace_id: str,
        data_pipeline_id: str,
        update_pipeline_definition_request: Union[_models.UpdateDataPipelineDefinitionRequest, IO[bytes]],
        *,
        update_metadata: Optional[bool] = None,
        **kwargs: Any
    ) -> Iterator[bytes]:
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[Iterator[bytes]] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _json = None
        _content = None
        if isinstance(update_pipeline_definition_request, (IOBase, bytes)):
            _content = update_pipeline_definition_request
        else:
            _json = self._serialize.body(update_pipeline_definition_request, "UpdateDataPipelineDefinitionRequest")

        _request = build_items_update_data_pipeline_definition_request(
            workspace_id=workspace_id,
            data_pipeline_id=data_pipeline_id,
            update_metadata=update_metadata,
            content_type=content_type,
            json=_json,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = True
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200, 202]:
            response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        if response.status_code == 200:
            deserialized = response.iter_bytes()

        if response.status_code == 202:
            response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))
            response_headers["x-ms-operation-id"] = self._deserialize("str", response.headers.get("x-ms-operation-id"))
            response_headers["Retry-After"] = self._deserialize("int", response.headers.get("Retry-After"))

            deserialized = response.iter_bytes()

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @overload
    def begin_update_data_pipeline_definition(
        self,
        workspace_id: str,
        data_pipeline_id: str,
        update_pipeline_definition_request: _models.UpdateDataPipelineDefinitionRequest,
        *,
        update_metadata: Optional[bool] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> LROPoller[None]:
        """Overrides the definition for the specified data pipeline.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Updating the DataPipeline's definition, does not affect its sensitivity label.

        Permissions
        -----------

         The caller must have *read and write* permissions for the data pipeline.

        Required Delegated Scopes
        -------------------------

         DataPipeline.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param data_pipeline_id: The data pipeline ID. Required.
        :type data_pipeline_id: str
        :param update_pipeline_definition_request: Update data pipeline definition request payload.
         Required.
        :type update_pipeline_definition_request:
         ~microsoft.fabric.api.datapipeline.models.UpdateDataPipelineDefinitionRequest
        :keyword update_metadata: When set to true and the .platform file is provided as part of the
         definition, the item's metadata is updated using the metadata in the .platform file. Default
         value is None.
        :paramtype update_metadata: bool
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: An instance of LROPoller that returns None
        :rtype: ~azure.core.polling.LROPoller[None]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    def begin_update_data_pipeline_definition(
        self,
        workspace_id: str,
        data_pipeline_id: str,
        update_pipeline_definition_request: IO[bytes],
        *,
        update_metadata: Optional[bool] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> LROPoller[None]:
        """Overrides the definition for the specified data pipeline.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Updating the DataPipeline's definition, does not affect its sensitivity label.

        Permissions
        -----------

         The caller must have *read and write* permissions for the data pipeline.

        Required Delegated Scopes
        -------------------------

         DataPipeline.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param data_pipeline_id: The data pipeline ID. Required.
        :type data_pipeline_id: str
        :param update_pipeline_definition_request: Update data pipeline definition request payload.
         Required.
        :type update_pipeline_definition_request: IO[bytes]
        :keyword update_metadata: When set to true and the .platform file is provided as part of the
         definition, the item's metadata is updated using the metadata in the .platform file. Default
         value is None.
        :paramtype update_metadata: bool
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: An instance of LROPoller that returns None
        :rtype: ~azure.core.polling.LROPoller[None]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace
    def begin_update_data_pipeline_definition(
        self,
        workspace_id: str,
        data_pipeline_id: str,
        update_pipeline_definition_request: Union[_models.UpdateDataPipelineDefinitionRequest, IO[bytes]],
        *,
        update_metadata: Optional[bool] = None,
        **kwargs: Any
    ) -> LROPoller[None]:
        """Overrides the definition for the specified data pipeline.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Updating the DataPipeline's definition, does not affect its sensitivity label.

        Permissions
        -----------

         The caller must have *read and write* permissions for the data pipeline.

        Required Delegated Scopes
        -------------------------

         DataPipeline.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param data_pipeline_id: The data pipeline ID. Required.
        :type data_pipeline_id: str
        :param update_pipeline_definition_request: Update data pipeline definition request payload. Is
         either a UpdateDataPipelineDefinitionRequest type or a IO[bytes] type. Required.
        :type update_pipeline_definition_request:
         ~microsoft.fabric.api.datapipeline.models.UpdateDataPipelineDefinitionRequest or IO[bytes]
        :keyword update_metadata: When set to true and the .platform file is provided as part of the
         definition, the item's metadata is updated using the metadata in the .platform file. Default
         value is None.
        :paramtype update_metadata: bool
        :return: An instance of LROPoller that returns None
        :rtype: ~azure.core.polling.LROPoller[None]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[None] = kwargs.pop("cls", None)
        polling: Union[bool, PollingMethod] = kwargs.pop("polling", True)
        lro_delay = kwargs.pop("polling_interval", self._config.polling_interval)
        cont_token: Optional[str] = kwargs.pop("continuation_token", None)
        if cont_token is None:
            raw_result = self._update_data_pipeline_definition_initial(
                workspace_id=workspace_id,
                data_pipeline_id=data_pipeline_id,
                update_pipeline_definition_request=update_pipeline_definition_request,
                update_metadata=update_metadata,
                content_type=content_type,
                cls=lambda x, y, z: x,
                headers=_headers,
                params=_params,
                **kwargs
            )
            raw_result.http_response.read()  # type: ignore
        kwargs.pop("error_map", None)

        def get_long_running_output(pipeline_response):  # pylint: disable=inconsistent-return-statements
            if cls:
                return cls(pipeline_response, None, {})  # type: ignore

        if polling is True:
            polling_method: PollingMethod = cast(PollingMethod, LROBasePolling(lro_delay, **kwargs))
        elif polling is False:
            polling_method = cast(PollingMethod, NoPolling())
        else:
            polling_method = polling
        if cont_token:
            return LROPoller[None].from_continuation_token(
                polling_method=polling_method,
                continuation_token=cont_token,
                client=self._client,
                deserialization_callback=get_long_running_output,
            )
        return LROPoller[None](self._client, raw_result, get_long_running_output, polling_method)  # type: ignore
