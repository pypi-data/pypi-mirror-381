# pylint: disable=too-many-lines,too-many-statements
# coding=utf-8
# --------------------------------------------------------------------------
# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.10.3, generator: @autorest/python@6.15.0)
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
from io import IOBase
import sys
from typing import Any, Callable, Dict, IO, Iterable, Iterator, Optional, Type, TypeVar, Union, cast, overload

from azure.core.exceptions import (
    ClientAuthenticationError,
    HttpResponseError,
    ResourceExistsError,
    ResourceNotFoundError,
    ResourceNotModifiedError,
    map_error,
)
from azure.core.paging import ItemPaged
from azure.core.pipeline import PipelineResponse
from azure.core.polling import LROPoller, NoPolling, PollingMethod
from azure.core.polling.base_polling import LROBasePolling
from azure.core.rest import HttpRequest, HttpResponse
from azure.core.tracing.decorator import distributed_trace
from azure.core.utils import case_insensitive_dict

from .. import models as _models
from .._serialization import Serializer

if sys.version_info >= (3, 9):
    from collections.abc import MutableMapping
else:
    from typing import MutableMapping  # type: ignore  # pylint: disable=ungrouped-imports
T = TypeVar("T")
ClsType = Optional[Callable[[PipelineResponse[HttpRequest, HttpResponse], T, Dict[str, Any]], Any]]

_SERIALIZER = Serializer()
_SERIALIZER.client_side_validation = False


def build_items_list_spark_job_definitions_request(  # pylint: disable=name-too-long
    workspace_id: str, *, continuation_token_parameter: Optional[str] = None, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/sparkJobDefinitions"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct parameters
    if continuation_token_parameter is not None:
        _params["continuationToken"] = _SERIALIZER.query(
            "continuation_token_parameter", continuation_token_parameter, "str"
        )

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="GET", url=_url, params=_params, headers=_headers, **kwargs)


def build_items_create_spark_job_definition_request(  # pylint: disable=name-too-long
    workspace_id: str, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/sparkJobDefinitions"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="POST", url=_url, headers=_headers, **kwargs)


def build_items_get_spark_job_definition_request(  # pylint: disable=name-too-long
    workspace_id: str, spark_job_definition_id: str, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/sparkJobDefinitions/{sparkJobDefinitionId}"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "sparkJobDefinitionId": _SERIALIZER.url("spark_job_definition_id", spark_job_definition_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="GET", url=_url, headers=_headers, **kwargs)


def build_items_update_spark_job_definition_request(  # pylint: disable=name-too-long
    workspace_id: str, spark_job_definition_id: str, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/sparkJobDefinitions/{sparkJobDefinitionId}"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "sparkJobDefinitionId": _SERIALIZER.url("spark_job_definition_id", spark_job_definition_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="PATCH", url=_url, headers=_headers, **kwargs)


def build_items_delete_spark_job_definition_request(  # pylint: disable=name-too-long
    workspace_id: str, spark_job_definition_id: str, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/sparkJobDefinitions/{sparkJobDefinitionId}"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "sparkJobDefinitionId": _SERIALIZER.url("spark_job_definition_id", spark_job_definition_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="DELETE", url=_url, headers=_headers, **kwargs)


def build_items_get_spark_job_definition_definition_request(  # pylint: disable=name-too-long
    workspace_id: str, spark_job_definition_id: str, *, format: Optional[str] = None, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/sparkJobDefinitions/{sparkJobDefinitionId}/getDefinition"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "sparkJobDefinitionId": _SERIALIZER.url("spark_job_definition_id", spark_job_definition_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct parameters
    if format is not None:
        _params["format"] = _SERIALIZER.query("format", format, "str")

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="POST", url=_url, params=_params, headers=_headers, **kwargs)


def build_items_update_spark_job_definition_definition_request(  # pylint: disable=name-too-long
    workspace_id: str, spark_job_definition_id: str, *, update_metadata: Optional[bool] = None, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/sparkJobDefinitions/{sparkJobDefinitionId}/updateDefinition"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "sparkJobDefinitionId": _SERIALIZER.url("spark_job_definition_id", spark_job_definition_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct parameters
    if update_metadata is not None:
        _params["updateMetadata"] = _SERIALIZER.query("update_metadata", update_metadata, "bool")

    # Construct headers
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="POST", url=_url, params=_params, headers=_headers, **kwargs)


def build_background_jobs_run_on_demand_spark_job_definition_request(  # pylint: disable=name-too-long
    workspace_id: str, spark_job_definition_id: str, *, job_type: str, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/sparkJobDefinitions/{sparkJobDefinitionId}/jobs/instances"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "sparkJobDefinitionId": _SERIALIZER.url("spark_job_definition_id", spark_job_definition_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct parameters
    _params["jobType"] = _SERIALIZER.query("job_type", job_type, "str")

    # Construct headers
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="POST", url=_url, params=_params, headers=_headers, **kwargs)


def build_livy_sessions_list_livy_sessions_request(  # pylint: disable=name-too-long
    workspace_id: str,
    spark_job_definition_id: str,
    *,
    continuation_token_parameter: Optional[str] = None,
    **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/sparkJobDefinitions/{sparkJobDefinitionId}/livySessions"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "sparkJobDefinitionId": _SERIALIZER.url("spark_job_definition_id", spark_job_definition_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct parameters
    if continuation_token_parameter is not None:
        _params["continuationToken"] = _SERIALIZER.query(
            "continuation_token_parameter", continuation_token_parameter, "str"
        )

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="GET", url=_url, params=_params, headers=_headers, **kwargs)


def build_livy_sessions_get_livy_session_request(  # pylint: disable=name-too-long
    workspace_id: str, spark_job_definition_id: str, livy_id: str, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/sparkJobDefinitions/{sparkJobDefinitionId}/livySessions/{livyId}"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "sparkJobDefinitionId": _SERIALIZER.url("spark_job_definition_id", spark_job_definition_id, "str"),
        "livyId": _SERIALIZER.url("livy_id", livy_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="GET", url=_url, headers=_headers, **kwargs)


class ItemsOperations:
    """
    .. warning::
        **DO NOT** instantiate this class directly.

        Instead, you should access the following operations through
        :class:`~microsoft.fabric.api.sparkjobdefinition.FabricSparkJobDefinitionClient`'s
        :attr:`items` attribute.
    """

    models = _models

    def __init__(self, *args, **kwargs):
        input_args = list(args)
        self._client = input_args.pop(0) if input_args else kwargs.pop("client")
        self._config = input_args.pop(0) if input_args else kwargs.pop("config")
        self._serialize = input_args.pop(0) if input_args else kwargs.pop("serializer")
        self._deserialize = input_args.pop(0) if input_args else kwargs.pop("deserializer")

    @distributed_trace
    def list_spark_job_definitions(
        self, workspace_id: str, *, continuation_token_parameter: Optional[str] = None, **kwargs: Any
    ) -> Iterable["_models.SparkJobDefinition"]:
        """Returns a list of spark job definitions from the specified workspace.

        This API supports `pagination </rest/api/fabric/articles/pagination>`_.

        Permissions
        -----------

        The caller must have a *viewer* workspace role.

        Required Delegated Scopes
        -------------------------

        Workspace.Read.All or Workspace.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :keyword continuation_token_parameter: A token for retrieving the next page of results. Default
         value is None.
        :paramtype continuation_token_parameter: str
        :return: An iterator like instance of SparkJobDefinition
        :rtype:
         ~azure.core.paging.ItemPaged[~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinition]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models._models.SparkJobDefinitions] = kwargs.pop("cls", None)  # pylint: disable=protected-access

        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        def prepare_request(next_link=None):
            if not next_link:

                _request = build_items_list_spark_job_definitions_request(
                    workspace_id=workspace_id,
                    continuation_token_parameter=continuation_token_parameter,
                    headers=_headers,
                    params=_params,
                )
                _request.url = self._client.format_url(_request.url)

            else:
                _request = HttpRequest("GET", next_link)
                _request.url = self._client.format_url(_request.url)

            return _request

        def extract_data(pipeline_response):
            deserialized = self._deserialize(
                _models._models.SparkJobDefinitions, pipeline_response  # pylint: disable=protected-access
            )
            list_of_elem = deserialized.value
            if cls:
                list_of_elem = cls(list_of_elem)  # type: ignore
            return deserialized.continuation_uri or None, iter(list_of_elem)

        def get_next(next_link=None):
            _request = prepare_request(next_link)

            _stream = False
            pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
                _request, stream=_stream, **kwargs
            )
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
                raise HttpResponseError(response=response, model=error)

            return pipeline_response

        return ItemPaged(get_next, extract_data)

    def _create_spark_job_definition_initial(
        self,
        workspace_id: str,
        create_spark_job_definition_request: Union[_models.CreateSparkJobDefinitionRequest, IO[bytes]],
        **kwargs: Any
    ) -> Iterator[bytes]:
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[Iterator[bytes]] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _json = None
        _content = None
        if isinstance(create_spark_job_definition_request, (IOBase, bytes)):
            _content = create_spark_job_definition_request
        else:
            _json = self._serialize.body(create_spark_job_definition_request, "CreateSparkJobDefinitionRequest")

        _request = build_items_create_spark_job_definition_request(
            workspace_id=workspace_id,
            content_type=content_type,
            json=_json,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = True
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [201, 202]:
            response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        if response.status_code == 201:
            deserialized = response.iter_bytes()

        if response.status_code == 202:
            response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))
            response_headers["x-ms-operation-id"] = self._deserialize("str", response.headers.get("x-ms-operation-id"))
            response_headers["Retry-After"] = self._deserialize("int", response.headers.get("Retry-After"))

            deserialized = response.iter_bytes()

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @overload
    def begin_create_spark_job_definition(
        self,
        workspace_id: str,
        create_spark_job_definition_request: _models.CreateSparkJobDefinitionRequest,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> LROPoller[_models.SparkJobDefinition]:
        """Creates a spark job definition in the specified workspace.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

         To create spark job definition with a public definition, refer to `Spark job definition
        </rest/api/fabric/articles/item-management/definitions/spark-job-definition>`_ article.

        Permissions
        -----------

        The caller must have a *contributor* workspace role.

        Required Delegated Scopes
        -------------------------

         SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All

        Limitations
        -----------


        * To create a spark job definition the workspace must be on a supported Fabric capacity. For
        more information see: `Microsoft Fabric license types
        </fabric/enterprise/licenses#microsoft-fabric-license-types>`_.

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param create_spark_job_definition_request: Create item request payload. Required.
        :type create_spark_job_definition_request:
         ~microsoft.fabric.api.sparkjobdefinition.models.CreateSparkJobDefinitionRequest
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: An instance of LROPoller that returns SparkJobDefinition
        :rtype:
         ~azure.core.polling.LROPoller[~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinition]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    def begin_create_spark_job_definition(
        self,
        workspace_id: str,
        create_spark_job_definition_request: IO[bytes],
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> LROPoller[_models.SparkJobDefinition]:
        """Creates a spark job definition in the specified workspace.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

         To create spark job definition with a public definition, refer to `Spark job definition
        </rest/api/fabric/articles/item-management/definitions/spark-job-definition>`_ article.

        Permissions
        -----------

        The caller must have a *contributor* workspace role.

        Required Delegated Scopes
        -------------------------

         SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All

        Limitations
        -----------


        * To create a spark job definition the workspace must be on a supported Fabric capacity. For
        more information see: `Microsoft Fabric license types
        </fabric/enterprise/licenses#microsoft-fabric-license-types>`_.

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param create_spark_job_definition_request: Create item request payload. Required.
        :type create_spark_job_definition_request: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: An instance of LROPoller that returns SparkJobDefinition
        :rtype:
         ~azure.core.polling.LROPoller[~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinition]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace
    def begin_create_spark_job_definition(
        self,
        workspace_id: str,
        create_spark_job_definition_request: Union[_models.CreateSparkJobDefinitionRequest, IO[bytes]],
        **kwargs: Any
    ) -> LROPoller[_models.SparkJobDefinition]:
        """Creates a spark job definition in the specified workspace.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

         To create spark job definition with a public definition, refer to `Spark job definition
        </rest/api/fabric/articles/item-management/definitions/spark-job-definition>`_ article.

        Permissions
        -----------

        The caller must have a *contributor* workspace role.

        Required Delegated Scopes
        -------------------------

         SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All

        Limitations
        -----------


        * To create a spark job definition the workspace must be on a supported Fabric capacity. For
        more information see: `Microsoft Fabric license types
        </fabric/enterprise/licenses#microsoft-fabric-license-types>`_.

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param create_spark_job_definition_request: Create item request payload. Is either a
         CreateSparkJobDefinitionRequest type or a IO[bytes] type. Required.
        :type create_spark_job_definition_request:
         ~microsoft.fabric.api.sparkjobdefinition.models.CreateSparkJobDefinitionRequest or IO[bytes]
        :return: An instance of LROPoller that returns SparkJobDefinition
        :rtype:
         ~azure.core.polling.LROPoller[~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinition]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.SparkJobDefinition] = kwargs.pop("cls", None)
        polling: Union[bool, PollingMethod] = kwargs.pop("polling", True)
        lro_delay = kwargs.pop("polling_interval", self._config.polling_interval)
        cont_token: Optional[str] = kwargs.pop("continuation_token", None)
        if cont_token is None:
            raw_result = self._create_spark_job_definition_initial(
                workspace_id=workspace_id,
                create_spark_job_definition_request=create_spark_job_definition_request,
                content_type=content_type,
                cls=lambda x, y, z: x,
                headers=_headers,
                params=_params,
                **kwargs
            )
            raw_result.http_response.read()  # type: ignore
        kwargs.pop("error_map", None)

        def get_long_running_output(pipeline_response):
            deserialized = self._deserialize("SparkJobDefinition", pipeline_response.http_response)
            if cls:
                return cls(pipeline_response, deserialized, {})  # type: ignore
            return deserialized

        if polling is True:
            polling_method: PollingMethod = cast(PollingMethod, LROBasePolling(lro_delay, **kwargs))
        elif polling is False:
            polling_method = cast(PollingMethod, NoPolling())
        else:
            polling_method = polling
        if cont_token:
            return LROPoller[_models.SparkJobDefinition].from_continuation_token(
                polling_method=polling_method,
                continuation_token=cont_token,
                client=self._client,
                deserialization_callback=get_long_running_output,
            )
        return LROPoller[_models.SparkJobDefinition](
            self._client, raw_result, get_long_running_output, polling_method  # type: ignore
        )

    @distributed_trace
    def get_spark_job_definition(
        self, workspace_id: str, spark_job_definition_id: str, **kwargs: Any
    ) -> _models.SparkJobDefinition:
        """Returns properties of the specified spark job definition.

        Permissions
        -----------

         The caller must have *read* permissions for the spark job definition.

        Required Delegated Scopes
        -------------------------

         SparkJobDefinition.Read.All or SparkJobDefinition.ReadWrite.All or Item.Read.All or
        Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param spark_job_definition_id: The spark job definition ID. Required.
        :type spark_job_definition_id: str
        :return: SparkJobDefinition
        :rtype: ~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinition
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.SparkJobDefinition] = kwargs.pop("cls", None)

        _request = build_items_get_spark_job_definition_request(
            workspace_id=workspace_id,
            spark_job_definition_id=spark_job_definition_id,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        deserialized = self._deserialize("SparkJobDefinition", pipeline_response.http_response)

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    def update_spark_job_definition(
        self,
        workspace_id: str,
        spark_job_definition_id: str,
        update_spark_job_definition_request: _models.UpdateSparkJobDefinitionRequest,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.SparkJobDefinition:
        """Updates the properties of the specified spark job definition.

        Permissions
        -----------

        The caller must have *read and write* permissions for the spark job definition.

        Required Delegated Scopes
        -------------------------

         SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param spark_job_definition_id: The spark job definition ID. Required.
        :type spark_job_definition_id: str
        :param update_spark_job_definition_request: Update spark job definition request payload.
         Required.
        :type update_spark_job_definition_request:
         ~microsoft.fabric.api.sparkjobdefinition.models.UpdateSparkJobDefinitionRequest
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: SparkJobDefinition
        :rtype: ~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinition
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    def update_spark_job_definition(
        self,
        workspace_id: str,
        spark_job_definition_id: str,
        update_spark_job_definition_request: IO[bytes],
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.SparkJobDefinition:
        """Updates the properties of the specified spark job definition.

        Permissions
        -----------

        The caller must have *read and write* permissions for the spark job definition.

        Required Delegated Scopes
        -------------------------

         SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param spark_job_definition_id: The spark job definition ID. Required.
        :type spark_job_definition_id: str
        :param update_spark_job_definition_request: Update spark job definition request payload.
         Required.
        :type update_spark_job_definition_request: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: SparkJobDefinition
        :rtype: ~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinition
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace
    def update_spark_job_definition(
        self,
        workspace_id: str,
        spark_job_definition_id: str,
        update_spark_job_definition_request: Union[_models.UpdateSparkJobDefinitionRequest, IO[bytes]],
        **kwargs: Any
    ) -> _models.SparkJobDefinition:
        """Updates the properties of the specified spark job definition.

        Permissions
        -----------

        The caller must have *read and write* permissions for the spark job definition.

        Required Delegated Scopes
        -------------------------

         SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param spark_job_definition_id: The spark job definition ID. Required.
        :type spark_job_definition_id: str
        :param update_spark_job_definition_request: Update spark job definition request payload. Is
         either a UpdateSparkJobDefinitionRequest type or a IO[bytes] type. Required.
        :type update_spark_job_definition_request:
         ~microsoft.fabric.api.sparkjobdefinition.models.UpdateSparkJobDefinitionRequest or IO[bytes]
        :return: SparkJobDefinition
        :rtype: ~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinition
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.SparkJobDefinition] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _json = None
        _content = None
        if isinstance(update_spark_job_definition_request, (IOBase, bytes)):
            _content = update_spark_job_definition_request
        else:
            _json = self._serialize.body(update_spark_job_definition_request, "UpdateSparkJobDefinitionRequest")

        _request = build_items_update_spark_job_definition_request(
            workspace_id=workspace_id,
            spark_job_definition_id=spark_job_definition_id,
            content_type=content_type,
            json=_json,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        deserialized = self._deserialize("SparkJobDefinition", pipeline_response.http_response)

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace
    def delete_spark_job_definition(  # pylint: disable=inconsistent-return-statements
        self, workspace_id: str, spark_job_definition_id: str, **kwargs: Any
    ) -> None:
        """Deletes the specified spark job definition.

        Permissions
        -----------

        The caller must have *write* permissions for the spark job definition.

        Required Delegated Scopes
        -------------------------

         SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param spark_job_definition_id: The spark job definition ID. Required.
        :type spark_job_definition_id: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[None] = kwargs.pop("cls", None)

        _request = build_items_delete_spark_job_definition_request(
            workspace_id=workspace_id,
            spark_job_definition_id=spark_job_definition_id,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        if cls:
            return cls(pipeline_response, None, {})  # type: ignore

    def _get_spark_job_definition_definition_initial(  # pylint: disable=name-too-long
        self, workspace_id: str, spark_job_definition_id: str, *, format: Optional[str] = None, **kwargs: Any
    ) -> Iterator[bytes]:
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[Iterator[bytes]] = kwargs.pop("cls", None)

        _request = build_items_get_spark_job_definition_definition_request(
            workspace_id=workspace_id,
            spark_job_definition_id=spark_job_definition_id,
            format=format,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = True
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200, 202]:
            response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        if response.status_code == 200:
            deserialized = response.iter_bytes()

        if response.status_code == 202:
            response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))
            response_headers["x-ms-operation-id"] = self._deserialize("str", response.headers.get("x-ms-operation-id"))
            response_headers["Retry-After"] = self._deserialize("int", response.headers.get("Retry-After"))

            deserialized = response.iter_bytes()

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace
    def begin_get_spark_job_definition_definition(  # pylint: disable=name-too-long
        self, workspace_id: str, spark_job_definition_id: str, *, format: Optional[str] = None, **kwargs: Any
    ) -> LROPoller[_models.SparkJobDefinitionResponse]:
        """Returns the specified spark job definition public definition.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        When you get a spark job definition's public definition, the sensitivity label is not a part of
        the definition.

        Permissions
        -----------

        The caller must have *read and write* permissions for the spark job definition.

        Required Delegated Scopes
        -------------------------

         SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All

        Limitations
        -----------

         This API is blocked for a spark job definition with an encrypted sensitivity label.

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param spark_job_definition_id: The spark job definition ID. Required.
        :type spark_job_definition_id: str
        :keyword format: The format of the spark job definition public definition. Default value is
         None.
        :paramtype format: str
        :return: An instance of LROPoller that returns SparkJobDefinitionResponse
        :rtype:
         ~azure.core.polling.LROPoller[~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinitionResponse]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.SparkJobDefinitionResponse] = kwargs.pop("cls", None)
        polling: Union[bool, PollingMethod] = kwargs.pop("polling", True)
        lro_delay = kwargs.pop("polling_interval", self._config.polling_interval)
        cont_token: Optional[str] = kwargs.pop("continuation_token", None)
        if cont_token is None:
            raw_result = self._get_spark_job_definition_definition_initial(
                workspace_id=workspace_id,
                spark_job_definition_id=spark_job_definition_id,
                format=format,
                cls=lambda x, y, z: x,
                headers=_headers,
                params=_params,
                **kwargs
            )
            raw_result.http_response.read()  # type: ignore
        kwargs.pop("error_map", None)

        def get_long_running_output(pipeline_response):
            deserialized = self._deserialize("SparkJobDefinitionResponse", pipeline_response.http_response)
            if cls:
                return cls(pipeline_response, deserialized, {})  # type: ignore
            return deserialized

        if polling is True:
            polling_method: PollingMethod = cast(PollingMethod, LROBasePolling(lro_delay, **kwargs))
        elif polling is False:
            polling_method = cast(PollingMethod, NoPolling())
        else:
            polling_method = polling
        if cont_token:
            return LROPoller[_models.SparkJobDefinitionResponse].from_continuation_token(
                polling_method=polling_method,
                continuation_token=cont_token,
                client=self._client,
                deserialization_callback=get_long_running_output,
            )
        return LROPoller[_models.SparkJobDefinitionResponse](
            self._client, raw_result, get_long_running_output, polling_method  # type: ignore
        )

    def _update_spark_job_definition_definition_initial(  # pylint: disable=name-too-long
        self,
        workspace_id: str,
        spark_job_definition_id: str,
        update_spark_job_definition_request: Union[_models.UpdateSparkJobDefinitionDefinitionRequest, IO[bytes]],
        *,
        update_metadata: Optional[bool] = None,
        **kwargs: Any
    ) -> Iterator[bytes]:
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[Iterator[bytes]] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _json = None
        _content = None
        if isinstance(update_spark_job_definition_request, (IOBase, bytes)):
            _content = update_spark_job_definition_request
        else:
            _json = self._serialize.body(
                update_spark_job_definition_request, "UpdateSparkJobDefinitionDefinitionRequest"
            )

        _request = build_items_update_spark_job_definition_definition_request(
            workspace_id=workspace_id,
            spark_job_definition_id=spark_job_definition_id,
            update_metadata=update_metadata,
            content_type=content_type,
            json=_json,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = True
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200, 202]:
            response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        if response.status_code == 200:
            deserialized = response.iter_bytes()

        if response.status_code == 202:
            response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))
            response_headers["x-ms-operation-id"] = self._deserialize("str", response.headers.get("x-ms-operation-id"))
            response_headers["Retry-After"] = self._deserialize("int", response.headers.get("Retry-After"))

            deserialized = response.iter_bytes()

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @overload
    def begin_update_spark_job_definition_definition(  # pylint: disable=name-too-long
        self,
        workspace_id: str,
        spark_job_definition_id: str,
        update_spark_job_definition_request: _models.UpdateSparkJobDefinitionDefinitionRequest,
        *,
        update_metadata: Optional[bool] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> LROPoller[None]:
        """Overrides the definition for the specified spark job definition.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Updating the spark job definition's definition, does not affect its sensitivity label.

        Permissions
        -----------

        The caller must have *read and write* permissions for the spark job definition.

        Required Delegated Scopes
        -------------------------

         SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param spark_job_definition_id: The spark job definition ID. Required.
        :type spark_job_definition_id: str
        :param update_spark_job_definition_request: Update spark job definition definition request
         payload. Required.
        :type update_spark_job_definition_request:
         ~microsoft.fabric.api.sparkjobdefinition.models.UpdateSparkJobDefinitionDefinitionRequest
        :keyword update_metadata: When set to true and the .platform file is provided as part of the
         definition, the item's metadata is updated using the metadata in the .platform file. Default
         value is None.
        :paramtype update_metadata: bool
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: An instance of LROPoller that returns None
        :rtype: ~azure.core.polling.LROPoller[None]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    def begin_update_spark_job_definition_definition(  # pylint: disable=name-too-long
        self,
        workspace_id: str,
        spark_job_definition_id: str,
        update_spark_job_definition_request: IO[bytes],
        *,
        update_metadata: Optional[bool] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> LROPoller[None]:
        """Overrides the definition for the specified spark job definition.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Updating the spark job definition's definition, does not affect its sensitivity label.

        Permissions
        -----------

        The caller must have *read and write* permissions for the spark job definition.

        Required Delegated Scopes
        -------------------------

         SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param spark_job_definition_id: The spark job definition ID. Required.
        :type spark_job_definition_id: str
        :param update_spark_job_definition_request: Update spark job definition definition request
         payload. Required.
        :type update_spark_job_definition_request: IO[bytes]
        :keyword update_metadata: When set to true and the .platform file is provided as part of the
         definition, the item's metadata is updated using the metadata in the .platform file. Default
         value is None.
        :paramtype update_metadata: bool
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: An instance of LROPoller that returns None
        :rtype: ~azure.core.polling.LROPoller[None]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace
    def begin_update_spark_job_definition_definition(  # pylint: disable=name-too-long
        self,
        workspace_id: str,
        spark_job_definition_id: str,
        update_spark_job_definition_request: Union[_models.UpdateSparkJobDefinitionDefinitionRequest, IO[bytes]],
        *,
        update_metadata: Optional[bool] = None,
        **kwargs: Any
    ) -> LROPoller[None]:
        """Overrides the definition for the specified spark job definition.

        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Updating the spark job definition's definition, does not affect its sensitivity label.

        Permissions
        -----------

        The caller must have *read and write* permissions for the spark job definition.

        Required Delegated Scopes
        -------------------------

         SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param spark_job_definition_id: The spark job definition ID. Required.
        :type spark_job_definition_id: str
        :param update_spark_job_definition_request: Update spark job definition definition request
         payload. Is either a UpdateSparkJobDefinitionDefinitionRequest type or a IO[bytes] type.
         Required.
        :type update_spark_job_definition_request:
         ~microsoft.fabric.api.sparkjobdefinition.models.UpdateSparkJobDefinitionDefinitionRequest or
         IO[bytes]
        :keyword update_metadata: When set to true and the .platform file is provided as part of the
         definition, the item's metadata is updated using the metadata in the .platform file. Default
         value is None.
        :paramtype update_metadata: bool
        :return: An instance of LROPoller that returns None
        :rtype: ~azure.core.polling.LROPoller[None]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[None] = kwargs.pop("cls", None)
        polling: Union[bool, PollingMethod] = kwargs.pop("polling", True)
        lro_delay = kwargs.pop("polling_interval", self._config.polling_interval)
        cont_token: Optional[str] = kwargs.pop("continuation_token", None)
        if cont_token is None:
            raw_result = self._update_spark_job_definition_definition_initial(
                workspace_id=workspace_id,
                spark_job_definition_id=spark_job_definition_id,
                update_spark_job_definition_request=update_spark_job_definition_request,
                update_metadata=update_metadata,
                content_type=content_type,
                cls=lambda x, y, z: x,
                headers=_headers,
                params=_params,
                **kwargs
            )
            raw_result.http_response.read()  # type: ignore
        kwargs.pop("error_map", None)

        def get_long_running_output(pipeline_response):  # pylint: disable=inconsistent-return-statements
            if cls:
                return cls(pipeline_response, None, {})  # type: ignore

        if polling is True:
            polling_method: PollingMethod = cast(PollingMethod, LROBasePolling(lro_delay, **kwargs))
        elif polling is False:
            polling_method = cast(PollingMethod, NoPolling())
        else:
            polling_method = polling
        if cont_token:
            return LROPoller[None].from_continuation_token(
                polling_method=polling_method,
                continuation_token=cont_token,
                client=self._client,
                deserialization_callback=get_long_running_output,
            )
        return LROPoller[None](self._client, raw_result, get_long_running_output, polling_method)  # type: ignore


class BackgroundJobsOperations:
    """
    .. warning::
        **DO NOT** instantiate this class directly.

        Instead, you should access the following operations through
        :class:`~microsoft.fabric.api.sparkjobdefinition.FabricSparkJobDefinitionClient`'s
        :attr:`background_jobs` attribute.
    """

    models = _models

    def __init__(self, *args, **kwargs):
        input_args = list(args)
        self._client = input_args.pop(0) if input_args else kwargs.pop("client")
        self._config = input_args.pop(0) if input_args else kwargs.pop("config")
        self._serialize = input_args.pop(0) if input_args else kwargs.pop("serializer")
        self._deserialize = input_args.pop(0) if input_args else kwargs.pop("deserializer")

    @overload
    def run_on_demand_spark_job_definition(  # pylint: disable=inconsistent-return-statements
        self,
        workspace_id: str,
        spark_job_definition_id: str,
        run_spark_job_definition_request: Optional[_models.RunSparkJobDefinitionRequest] = None,
        *,
        job_type: str,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> None:
        """Run on-demand `Spark job definition </fabric/data-engineering/run-spark-job-definition>`_ job
        instance.

        Required Delegated Scopes
        -------------------------

        SparkJobDefinition.Execute.All or Item.Execute.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param spark_job_definition_id: The Spark job definition item ID. Required.
        :type spark_job_definition_id: str
        :param run_spark_job_definition_request: Run spark job definition request payload with
         parameters to use. Default value is None.
        :type run_spark_job_definition_request:
         ~microsoft.fabric.api.sparkjobdefinition.models.RunSparkJobDefinitionRequest
        :keyword job_type: The supported job type for Spark job definition is ``sparkjob``. Required.
        :paramtype job_type: str
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    def run_on_demand_spark_job_definition(  # pylint: disable=inconsistent-return-statements
        self,
        workspace_id: str,
        spark_job_definition_id: str,
        run_spark_job_definition_request: Optional[IO[bytes]] = None,
        *,
        job_type: str,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> None:
        """Run on-demand `Spark job definition </fabric/data-engineering/run-spark-job-definition>`_ job
        instance.

        Required Delegated Scopes
        -------------------------

        SparkJobDefinition.Execute.All or Item.Execute.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param spark_job_definition_id: The Spark job definition item ID. Required.
        :type spark_job_definition_id: str
        :param run_spark_job_definition_request: Run spark job definition request payload with
         parameters to use. Default value is None.
        :type run_spark_job_definition_request: IO[bytes]
        :keyword job_type: The supported job type for Spark job definition is ``sparkjob``. Required.
        :paramtype job_type: str
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace
    def run_on_demand_spark_job_definition(  # pylint: disable=inconsistent-return-statements
        self,
        workspace_id: str,
        spark_job_definition_id: str,
        run_spark_job_definition_request: Optional[Union[_models.RunSparkJobDefinitionRequest, IO[bytes]]] = None,
        *,
        job_type: str,
        **kwargs: Any
    ) -> None:
        """Run on-demand `Spark job definition </fabric/data-engineering/run-spark-job-definition>`_ job
        instance.

        Required Delegated Scopes
        -------------------------

        SparkJobDefinition.Execute.All or Item.Execute.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param spark_job_definition_id: The Spark job definition item ID. Required.
        :type spark_job_definition_id: str
        :param run_spark_job_definition_request: Run spark job definition request payload with
         parameters to use. Is either a RunSparkJobDefinitionRequest type or a IO[bytes] type. Default
         value is None.
        :type run_spark_job_definition_request:
         ~microsoft.fabric.api.sparkjobdefinition.models.RunSparkJobDefinitionRequest or IO[bytes]
        :keyword job_type: The supported job type for Spark job definition is ``sparkjob``. Required.
        :paramtype job_type: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[None] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _json = None
        _content = None
        if isinstance(run_spark_job_definition_request, (IOBase, bytes)):
            _content = run_spark_job_definition_request
        else:
            if run_spark_job_definition_request is not None:
                _json = self._serialize.body(run_spark_job_definition_request, "RunSparkJobDefinitionRequest")
            else:
                _json = None

        _request = build_background_jobs_run_on_demand_spark_job_definition_request(
            workspace_id=workspace_id,
            spark_job_definition_id=spark_job_definition_id,
            job_type=job_type,
            content_type=content_type,
            json=_json,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [202]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))
        response_headers["Retry-After"] = self._deserialize("int", response.headers.get("Retry-After"))

        if cls:
            return cls(pipeline_response, None, response_headers)  # type: ignore


class LivySessionsOperations:
    """
    .. warning::
        **DO NOT** instantiate this class directly.

        Instead, you should access the following operations through
        :class:`~microsoft.fabric.api.sparkjobdefinition.FabricSparkJobDefinitionClient`'s
        :attr:`livy_sessions` attribute.
    """

    models = _models

    def __init__(self, *args, **kwargs):
        input_args = list(args)
        self._client = input_args.pop(0) if input_args else kwargs.pop("client")
        self._config = input_args.pop(0) if input_args else kwargs.pop("config")
        self._serialize = input_args.pop(0) if input_args else kwargs.pop("serializer")
        self._deserialize = input_args.pop(0) if input_args else kwargs.pop("deserializer")

    @distributed_trace
    def list_livy_sessions(
        self,
        workspace_id: str,
        spark_job_definition_id: str,
        *,
        continuation_token_parameter: Optional[str] = None,
        **kwargs: Any
    ) -> Iterable["_models.LivySession"]:
        """Returns a list of livy sessions from the specified item identifier.

        This API supports `pagination </rest/api/fabric/articles/pagination>`_.

        Permissions
        -----------

         The caller must have *viewer* or higher workspace role.

        Required Delegated Scopes
        -------------------------

         SparkJobDefinition.Read.All or SparkJobDefinition.ReadWrite.All or Item.Read.All or
        Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace identifier. Required.
        :type workspace_id: str
        :param spark_job_definition_id: The sparkJobDefinition ID. Required.
        :type spark_job_definition_id: str
        :keyword continuation_token_parameter: Token to retrieve the next page of results, if
         available. Default value is None.
        :paramtype continuation_token_parameter: str
        :return: An iterator like instance of LivySession
        :rtype:
         ~azure.core.paging.ItemPaged[~microsoft.fabric.api.sparkjobdefinition.models.LivySession]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models._models.LivySessions] = kwargs.pop("cls", None)  # pylint: disable=protected-access

        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        def prepare_request(next_link=None):
            if not next_link:

                _request = build_livy_sessions_list_livy_sessions_request(
                    workspace_id=workspace_id,
                    spark_job_definition_id=spark_job_definition_id,
                    continuation_token_parameter=continuation_token_parameter,
                    headers=_headers,
                    params=_params,
                )
                _request.url = self._client.format_url(_request.url)

            else:
                _request = HttpRequest("GET", next_link)
                _request.url = self._client.format_url(_request.url)

            return _request

        def extract_data(pipeline_response):
            deserialized = self._deserialize(
                _models._models.LivySessions, pipeline_response  # pylint: disable=protected-access
            )
            list_of_elem = deserialized.value
            if cls:
                list_of_elem = cls(list_of_elem)  # type: ignore
            return deserialized.continuation_uri or None, iter(list_of_elem)

        def get_next(next_link=None):
            _request = prepare_request(next_link)

            _stream = False
            pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
                _request, stream=_stream, **kwargs
            )
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
                raise HttpResponseError(response=response, model=error)

            return pipeline_response

        return ItemPaged(get_next, extract_data)

    @distributed_trace
    def get_livy_session(
        self, workspace_id: str, spark_job_definition_id: str, livy_id: str, **kwargs: Any
    ) -> _models.LivySession:
        """Returns properties of the specified livy session.

        Permissions
        -----------

         The caller must have *viewer* or higher workspace role.

        Required Delegated Scopes
        -------------------------

         SparkJobDefinition.Read.All or SparkJobDefinition.ReadWrite.All or Item.Read.All or
        Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace identifier. Required.
        :type workspace_id: str
        :param spark_job_definition_id: The sparkJobDefinition ID. Required.
        :type spark_job_definition_id: str
        :param livy_id: The session identifier. Required.
        :type livy_id: str
        :return: LivySession
        :rtype: ~microsoft.fabric.api.sparkjobdefinition.models.LivySession
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.LivySession] = kwargs.pop("cls", None)

        _request = build_livy_sessions_get_livy_session_request(
            workspace_id=workspace_id,
            spark_job_definition_id=spark_job_definition_id,
            livy_id=livy_id,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        deserialized = self._deserialize("LivySession", pipeline_response.http_response)

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore
