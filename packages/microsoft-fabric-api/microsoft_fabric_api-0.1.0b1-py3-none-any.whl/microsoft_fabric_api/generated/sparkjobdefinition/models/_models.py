# coding=utf-8
# pylint: disable=too-many-lines
# --------------------------------------------------------------------------
# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.10.3, generator: @autorest/python@6.15.0)
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------

import datetime
from typing import Any, List, Optional, TYPE_CHECKING, Union

from .. import _serialization

if TYPE_CHECKING:
    # pylint: disable=unused-import,ungrouped-imports
    from .. import models as _models


class CreateSparkJobDefinitionRequest(_serialization.Model):
    """Create spark job definition request payload.

    All required parameters must be populated in order to send to server.

    :ivar display_name: The spark job definition display name. The display name must follow naming
     rules according to item type. Required.
    :vartype display_name: str
    :ivar description: The spark job definition description. Maximum length is 256 characters.
    :vartype description: str
    :ivar folder_id: The folder ID. If not specified or null, the spark job definition is created
     with the workspace as its folder.
    :vartype folder_id: str
    :ivar definition: The spark job definition public definition.
    :vartype definition:
     ~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinitionPublicDefinition
    """

    _validation = {
        "display_name": {"required": True},
    }

    _attribute_map = {
        "display_name": {"key": "displayName", "type": "str"},
        "description": {"key": "description", "type": "str"},
        "folder_id": {"key": "folderId", "type": "str"},
        "definition": {"key": "definition", "type": "SparkJobDefinitionPublicDefinition"},
    }

    def __init__(
        self,
        *,
        display_name: str,
        description: Optional[str] = None,
        folder_id: Optional[str] = None,
        definition: Optional["_models.SparkJobDefinitionPublicDefinition"] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword display_name: The spark job definition display name. The display name must follow
         naming rules according to item type. Required.
        :paramtype display_name: str
        :keyword description: The spark job definition description. Maximum length is 256 characters.
        :paramtype description: str
        :keyword folder_id: The folder ID. If not specified or null, the spark job definition is
         created with the workspace as its folder.
        :paramtype folder_id: str
        :keyword definition: The spark job definition public definition.
        :paramtype definition:
         ~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinitionPublicDefinition
        """
        super().__init__(**kwargs)
        self.display_name = display_name
        self.description = description
        self.folder_id = folder_id
        self.definition = definition


class Duration(_serialization.Model):
    """A duration.

    All required parameters must be populated in order to send to server.

    :ivar value: The number of timeUnits in the duration. Required.
    :vartype value: float
    :ivar time_unit: The unit of time for the duration. Additional duration types may be added over
     time. Required. Known values are: "Seconds", "Minutes", "Hours", and "Days".
    :vartype time_unit: str or ~microsoft.fabric.api.sparkjobdefinition.models.TimeUnit
    """

    _validation = {
        "value": {"required": True},
        "time_unit": {"required": True},
    }

    _attribute_map = {
        "value": {"key": "value", "type": "float"},
        "time_unit": {"key": "timeUnit", "type": "str"},
    }

    def __init__(self, *, value: float, time_unit: Union[str, "_models.TimeUnit"], **kwargs: Any) -> None:
        """
        :keyword value: The number of timeUnits in the duration. Required.
        :paramtype value: float
        :keyword time_unit: The unit of time for the duration. Additional duration types may be added
         over time. Required. Known values are: "Seconds", "Minutes", "Hours", and "Days".
        :paramtype time_unit: str or ~microsoft.fabric.api.sparkjobdefinition.models.TimeUnit
        """
        super().__init__(**kwargs)
        self.value = value
        self.time_unit = time_unit


class ErrorRelatedResource(_serialization.Model):
    """The error related resource details object.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to server.

    :ivar resource_id: The resource ID that's involved in the error. Required.
    :vartype resource_id: str
    :ivar resource_type: The type of the resource that's involved in the error. Required.
    :vartype resource_type: str
    """

    _validation = {
        "resource_id": {"required": True, "readonly": True},
        "resource_type": {"required": True, "readonly": True},
    }

    _attribute_map = {
        "resource_id": {"key": "resourceId", "type": "str"},
        "resource_type": {"key": "resourceType", "type": "str"},
    }

    def __init__(self, **kwargs: Any) -> None:
        """ """
        super().__init__(**kwargs)
        self.resource_id = None
        self.resource_type = None


class ErrorResponseDetails(_serialization.Model):
    """The error response details.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to server.

    :ivar error_code: A specific identifier that provides information about an error condition,
     allowing for standardized communication between our service and its users. Required.
    :vartype error_code: str
    :ivar message: A human readable representation of the error. Required.
    :vartype message: str
    :ivar related_resource: The error related resource details.
    :vartype related_resource: ~microsoft.fabric.api.sparkjobdefinition.models.ErrorRelatedResource
    """

    _validation = {
        "error_code": {"required": True, "readonly": True},
        "message": {"required": True, "readonly": True},
        "related_resource": {"readonly": True},
    }

    _attribute_map = {
        "error_code": {"key": "errorCode", "type": "str"},
        "message": {"key": "message", "type": "str"},
        "related_resource": {"key": "relatedResource", "type": "ErrorRelatedResource"},
    }

    def __init__(self, **kwargs: Any) -> None:
        """ """
        super().__init__(**kwargs)
        self.error_code = None
        self.message = None
        self.related_resource = None


class ErrorResponse(ErrorResponseDetails):
    """The error response.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to server.

    :ivar error_code: A specific identifier that provides information about an error condition,
     allowing for standardized communication between our service and its users. Required.
    :vartype error_code: str
    :ivar message: A human readable representation of the error. Required.
    :vartype message: str
    :ivar related_resource: The error related resource details.
    :vartype related_resource: ~microsoft.fabric.api.sparkjobdefinition.models.ErrorRelatedResource
    :ivar request_id: ID of the request associated with the error.
    :vartype request_id: str
    :ivar more_details: List of additional error details.
    :vartype more_details:
     list[~microsoft.fabric.api.sparkjobdefinition.models.ErrorResponseDetails]
    """

    _validation = {
        "error_code": {"required": True, "readonly": True},
        "message": {"required": True, "readonly": True},
        "related_resource": {"readonly": True},
        "request_id": {"readonly": True},
        "more_details": {"readonly": True},
    }

    _attribute_map = {
        "error_code": {"key": "errorCode", "type": "str"},
        "message": {"key": "message", "type": "str"},
        "related_resource": {"key": "relatedResource", "type": "ErrorRelatedResource"},
        "request_id": {"key": "requestId", "type": "str"},
        "more_details": {"key": "moreDetails", "type": "[ErrorResponseDetails]"},
    }

    def __init__(self, **kwargs: Any) -> None:
        """ """
        super().__init__(**kwargs)
        self.request_id = None
        self.more_details = None


class ExecutionData(_serialization.Model):
    """ExecutionData for spark job definition run if customer wants to override default values.

    :ivar executable_file: Executable main file to be used. The path must be an abfs path.
    :vartype executable_file: str
    :ivar main_class: Main class name to be used. This is not needed for python and r executable
     files.
    :vartype main_class: str
    :ivar command_line_arguments: Command line arguments. The arguments are space separated.
    :vartype command_line_arguments: str
    :ivar additional_library_uris: List of additional library paths needed for execution.
    :vartype additional_library_uris: list[str]
    :ivar environment_id: The environment ID that will be used for the Spark job definition. Can be
     used to specify Spark settings.
    :vartype environment_id: ~microsoft.fabric.api.sparkjobdefinition.models.ItemReference
    :ivar default_lakehouse_id: The lakehouse ID that will be used as the default lakehouse of the
     Spark job definition.
    :vartype default_lakehouse_id: ~microsoft.fabric.api.sparkjobdefinition.models.ItemReference
    """

    _attribute_map = {
        "executable_file": {"key": "executableFile", "type": "str"},
        "main_class": {"key": "mainClass", "type": "str"},
        "command_line_arguments": {"key": "commandLineArguments", "type": "str"},
        "additional_library_uris": {"key": "additionalLibraryUris", "type": "[str]"},
        "environment_id": {"key": "environmentId", "type": "ItemReference"},
        "default_lakehouse_id": {"key": "defaultLakehouseId", "type": "ItemReference"},
    }

    def __init__(
        self,
        *,
        executable_file: Optional[str] = None,
        main_class: Optional[str] = None,
        command_line_arguments: Optional[str] = None,
        additional_library_uris: Optional[List[str]] = None,
        environment_id: Optional["_models.ItemReference"] = None,
        default_lakehouse_id: Optional["_models.ItemReference"] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword executable_file: Executable main file to be used. The path must be an abfs path.
        :paramtype executable_file: str
        :keyword main_class: Main class name to be used. This is not needed for python and r executable
         files.
        :paramtype main_class: str
        :keyword command_line_arguments: Command line arguments. The arguments are space separated.
        :paramtype command_line_arguments: str
        :keyword additional_library_uris: List of additional library paths needed for execution.
        :paramtype additional_library_uris: list[str]
        :keyword environment_id: The environment ID that will be used for the Spark job definition. Can
         be used to specify Spark settings.
        :paramtype environment_id: ~microsoft.fabric.api.sparkjobdefinition.models.ItemReference
        :keyword default_lakehouse_id: The lakehouse ID that will be used as the default lakehouse of
         the Spark job definition.
        :paramtype default_lakehouse_id: ~microsoft.fabric.api.sparkjobdefinition.models.ItemReference
        """
        super().__init__(**kwargs)
        self.executable_file = executable_file
        self.main_class = main_class
        self.command_line_arguments = command_line_arguments
        self.additional_library_uris = additional_library_uris
        self.environment_id = environment_id
        self.default_lakehouse_id = default_lakehouse_id


class Item(_serialization.Model):
    """An item object.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to server.

    :ivar id: The item ID.
    :vartype id: str
    :ivar display_name: The item display name.
    :vartype display_name: str
    :ivar description: The item description.
    :vartype description: str
    :ivar type: The item type. Required. Known values are: "Dashboard", "Report", "SemanticModel",
     "PaginatedReport", "Datamart", "Lakehouse", "Eventhouse", "Environment", "KQLDatabase",
     "KQLQueryset", "KQLDashboard", "DataPipeline", "Notebook", "SparkJobDefinition",
     "MLExperiment", "MLModel", "Warehouse", "Eventstream", "SQLEndpoint", "MirroredWarehouse",
     "MirroredDatabase", "Reflex", "GraphQLApi", "MountedDataFactory", "ApacheAirflowJob",
     "SQLDatabase", "CopyJob", "VariableLibrary", "MirroredAzureDatabricksCatalog", "Dataflow",
     "WarehouseSnapshot", "DigitalTwinBuilder", "DigitalTwinBuilderFlow", "AnomalyDetector", "Map",
     and "UserDataFunction".
    :vartype type: str or ~microsoft.fabric.api.sparkjobdefinition.models.ItemType
    :ivar workspace_id: The workspace ID.
    :vartype workspace_id: str
    :ivar folder_id: The folder ID.
    :vartype folder_id: str
    :ivar tags: List of applied tags.
    :vartype tags: list[~microsoft.fabric.api.sparkjobdefinition.models.ItemTag]
    """

    _validation = {
        "id": {"readonly": True},
        "type": {"required": True},
        "workspace_id": {"readonly": True},
        "folder_id": {"readonly": True},
        "tags": {"readonly": True},
    }

    _attribute_map = {
        "id": {"key": "id", "type": "str"},
        "display_name": {"key": "displayName", "type": "str"},
        "description": {"key": "description", "type": "str"},
        "type": {"key": "type", "type": "str"},
        "workspace_id": {"key": "workspaceId", "type": "str"},
        "folder_id": {"key": "folderId", "type": "str"},
        "tags": {"key": "tags", "type": "[ItemTag]"},
    }

    def __init__(
        self,
        *,
        type: Union[str, "_models.ItemType"],
        display_name: Optional[str] = None,
        description: Optional[str] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword display_name: The item display name.
        :paramtype display_name: str
        :keyword description: The item description.
        :paramtype description: str
        :keyword type: The item type. Required. Known values are: "Dashboard", "Report",
         "SemanticModel", "PaginatedReport", "Datamart", "Lakehouse", "Eventhouse", "Environment",
         "KQLDatabase", "KQLQueryset", "KQLDashboard", "DataPipeline", "Notebook", "SparkJobDefinition",
         "MLExperiment", "MLModel", "Warehouse", "Eventstream", "SQLEndpoint", "MirroredWarehouse",
         "MirroredDatabase", "Reflex", "GraphQLApi", "MountedDataFactory", "ApacheAirflowJob",
         "SQLDatabase", "CopyJob", "VariableLibrary", "MirroredAzureDatabricksCatalog", "Dataflow",
         "WarehouseSnapshot", "DigitalTwinBuilder", "DigitalTwinBuilderFlow", "AnomalyDetector", "Map",
         and "UserDataFunction".
        :paramtype type: str or ~microsoft.fabric.api.sparkjobdefinition.models.ItemType
        """
        super().__init__(**kwargs)
        self.id = None
        self.display_name = display_name
        self.description = description
        self.type = type
        self.workspace_id = None
        self.folder_id = None
        self.tags = None


class ItemReference(_serialization.Model):
    """An item reference object.

    You probably want to use the sub-classes and not this class directly. Known sub-classes are:
    ItemReferenceById

    All required parameters must be populated in order to send to server.

    :ivar reference_type: The item reference type. Required. "ById"
    :vartype reference_type: str or
     ~microsoft.fabric.api.sparkjobdefinition.models.ItemReferenceType
    """

    _validation = {
        "reference_type": {"required": True},
    }

    _attribute_map = {
        "reference_type": {"key": "referenceType", "type": "str"},
    }

    _subtype_map = {"reference_type": {"ById": "ItemReferenceById"}}

    def __init__(self, **kwargs: Any) -> None:
        """ """
        super().__init__(**kwargs)
        self.reference_type: Optional[str] = None


class ItemReferenceById(ItemReference):
    """An item reference by ID object.

    All required parameters must be populated in order to send to server.

    :ivar reference_type: The item reference type. Required. "ById"
    :vartype reference_type: str or
     ~microsoft.fabric.api.sparkjobdefinition.models.ItemReferenceType
    :ivar item_id: The ID of the item. Required.
    :vartype item_id: str
    :ivar workspace_id: The workspace ID of the item. Required.
    :vartype workspace_id: str
    """

    _validation = {
        "reference_type": {"required": True},
        "item_id": {"required": True},
        "workspace_id": {"required": True},
    }

    _attribute_map = {
        "reference_type": {"key": "referenceType", "type": "str"},
        "item_id": {"key": "itemId", "type": "str"},
        "workspace_id": {"key": "workspaceId", "type": "str"},
    }

    def __init__(self, *, item_id: str, workspace_id: str, **kwargs: Any) -> None:
        """
        :keyword item_id: The ID of the item. Required.
        :paramtype item_id: str
        :keyword workspace_id: The workspace ID of the item. Required.
        :paramtype workspace_id: str
        """
        super().__init__(**kwargs)
        self.reference_type: str = "ById"
        self.item_id = item_id
        self.workspace_id = workspace_id


class ItemTag(_serialization.Model):
    """Represents a tag applied on an item.

    All required parameters must be populated in order to send to server.

    :ivar id: The tag ID. Required.
    :vartype id: str
    :ivar display_name: The name of the tag. Required.
    :vartype display_name: str
    """

    _validation = {
        "id": {"required": True},
        "display_name": {"required": True},
    }

    _attribute_map = {
        "id": {"key": "id", "type": "str"},
        "display_name": {"key": "displayName", "type": "str"},
    }

    def __init__(self, *, id: str, display_name: str, **kwargs: Any) -> None:  # pylint: disable=redefined-builtin
        """
        :keyword id: The tag ID. Required.
        :paramtype id: str
        :keyword display_name: The name of the tag. Required.
        :paramtype display_name: str
        """
        super().__init__(**kwargs)
        self.id = id
        self.display_name = display_name


class LivySession(_serialization.Model):  # pylint: disable=too-many-instance-attributes
    """The livy session response.

    :ivar spark_application_id: A Spark application ID is a unique identifier assigned to each
     Apache Spark application. It also appears in the Spark UI.
    :vartype spark_application_id: str
    :ivar state: Current state of the job. Known values are: "InProgress", "Cancelled",
     "NotStarted", "Succeeded", "Failed", and "Unknown".
    :vartype state: str or ~microsoft.fabric.api.sparkjobdefinition.models.State
    :ivar livy_id: ID of the Livy session or Livy batch.
    :vartype livy_id: str
    :ivar origin: Origin of the job. Known values are: "SubmittedJob" and "PendingJob".
    :vartype origin: str or ~microsoft.fabric.api.sparkjobdefinition.models.Origin
    :ivar attempt_number: Current attempt number.
    :vartype attempt_number: int
    :ivar max_number_of_attempts: Maximum number of attempts.
    :vartype max_number_of_attempts: int
    :ivar livy_name: Name of the Livy session or Livy batch.
    :vartype livy_name: str
    :ivar submitter: ID of the submitter.
    :vartype submitter: ~microsoft.fabric.api.sparkjobdefinition.models.Principal
    :ivar item: ID of the item.
    :vartype item: ~microsoft.fabric.api.sparkjobdefinition.models.ItemReferenceById
    :ivar item_type: The item type. Known values are: "Dashboard", "Report", "SemanticModel",
     "PaginatedReport", "Datamart", "Lakehouse", "Eventhouse", "Environment", "KQLDatabase",
     "KQLQueryset", "KQLDashboard", "DataPipeline", "Notebook", "SparkJobDefinition",
     "MLExperiment", "MLModel", "Warehouse", "Eventstream", "SQLEndpoint", "MirroredWarehouse",
     "MirroredDatabase", "Reflex", "GraphQLApi", "MountedDataFactory", "ApacheAirflowJob",
     "SQLDatabase", "CopyJob", "VariableLibrary", "MirroredAzureDatabricksCatalog", "Dataflow",
     "WarehouseSnapshot", "DigitalTwinBuilder", "DigitalTwinBuilderFlow", "AnomalyDetector", "Map",
     and "UserDataFunction".
    :vartype item_type: str or ~microsoft.fabric.api.sparkjobdefinition.models.ItemType
    :ivar item_name: Name of the item.
    :vartype item_name: str
    :ivar job_type: Current state of the job. Known values are: "Unknown", "SparkSession",
     "SparkBatch", and "JupyterSession".
    :vartype job_type: str or ~microsoft.fabric.api.sparkjobdefinition.models.JobType
    :ivar submitted_date_time: Timestamp when the job was submitted in UTC, using the
     YYYY-MM-DDTHH:mm:ssZ format.
    :vartype submitted_date_time: ~datetime.datetime
    :ivar start_date_time: Timestamp when the job started in UTC, using the YYYY-MM-DDTHH:mm:ssZ
     format.
    :vartype start_date_time: ~datetime.datetime
    :ivar end_date_time: Timestamp when the job ended in UTC, using the YYYY-MM-DDTHH:mm:ssZ
     format.
    :vartype end_date_time: ~datetime.datetime
    :ivar queued_duration: Duration for which the job was queued.
    :vartype queued_duration: ~microsoft.fabric.api.sparkjobdefinition.models.Duration
    :ivar running_duration: Time it took the job to run.
    :vartype running_duration: ~microsoft.fabric.api.sparkjobdefinition.models.Duration
    :ivar total_duration: Total duration of the job.
    :vartype total_duration: ~microsoft.fabric.api.sparkjobdefinition.models.Duration
    :ivar job_instance_id: ID of the job instance.
    :vartype job_instance_id: str
    :ivar creator_item: ID of the item creator. When ``isHighConcurrency`` is set to ``true`` this
     value might be different than ``itemId``.
    :vartype creator_item: ~microsoft.fabric.api.sparkjobdefinition.models.ItemReferenceById
    :ivar is_high_concurrency: Flag indicating high concurrency.
    :vartype is_high_concurrency: bool
    :ivar cancellation_reason: Reason for the job cancellation.
    :vartype cancellation_reason: str
    :ivar capacity_id: ID of the capacity.
    :vartype capacity_id: str
    :ivar operation_name: Name of the operation. Possible values include: *Notebook run*\\ ,
     *Notebook HC run* and *Notebook pipeline run*.
    :vartype operation_name: str
    :ivar consumer_id: ID of the consumer.
    :vartype consumer_id: ~microsoft.fabric.api.sparkjobdefinition.models.Principal
    :ivar runtime_version: The fabric runtime version.
    :vartype runtime_version: str
    :ivar livy_session_item_resource_uri: The URI used to retrieve all Livy sessions for a given
     item.
    :vartype livy_session_item_resource_uri: str
    """

    _attribute_map = {
        "spark_application_id": {"key": "sparkApplicationId", "type": "str"},
        "state": {"key": "state", "type": "str"},
        "livy_id": {"key": "livyId", "type": "str"},
        "origin": {"key": "origin", "type": "str"},
        "attempt_number": {"key": "attemptNumber", "type": "int"},
        "max_number_of_attempts": {"key": "maxNumberOfAttempts", "type": "int"},
        "livy_name": {"key": "livyName", "type": "str"},
        "submitter": {"key": "submitter", "type": "Principal"},
        "item": {"key": "item", "type": "ItemReferenceById"},
        "item_type": {"key": "itemType", "type": "str"},
        "item_name": {"key": "itemName", "type": "str"},
        "job_type": {"key": "jobType", "type": "str"},
        "submitted_date_time": {"key": "submittedDateTime", "type": "iso-8601"},
        "start_date_time": {"key": "startDateTime", "type": "iso-8601"},
        "end_date_time": {"key": "endDateTime", "type": "iso-8601"},
        "queued_duration": {"key": "queuedDuration", "type": "Duration"},
        "running_duration": {"key": "runningDuration", "type": "Duration"},
        "total_duration": {"key": "totalDuration", "type": "Duration"},
        "job_instance_id": {"key": "jobInstanceId", "type": "str"},
        "creator_item": {"key": "creatorItem", "type": "ItemReferenceById"},
        "is_high_concurrency": {"key": "isHighConcurrency", "type": "bool"},
        "cancellation_reason": {"key": "cancellationReason", "type": "str"},
        "capacity_id": {"key": "capacityId", "type": "str"},
        "operation_name": {"key": "operationName", "type": "str"},
        "consumer_id": {"key": "consumerId", "type": "Principal"},
        "runtime_version": {"key": "runtimeVersion", "type": "str"},
        "livy_session_item_resource_uri": {"key": "livySessionItemResourceUri", "type": "str"},
    }

    def __init__(  # pylint: disable=too-many-locals
        self,
        *,
        spark_application_id: Optional[str] = None,
        state: Optional[Union[str, "_models.State"]] = None,
        livy_id: Optional[str] = None,
        origin: Optional[Union[str, "_models.Origin"]] = None,
        attempt_number: Optional[int] = None,
        max_number_of_attempts: Optional[int] = None,
        livy_name: Optional[str] = None,
        submitter: Optional["_models.Principal"] = None,
        item: Optional["_models.ItemReferenceById"] = None,
        item_type: Optional[Union[str, "_models.ItemType"]] = None,
        item_name: Optional[str] = None,
        job_type: Optional[Union[str, "_models.JobType"]] = None,
        submitted_date_time: Optional[datetime.datetime] = None,
        start_date_time: Optional[datetime.datetime] = None,
        end_date_time: Optional[datetime.datetime] = None,
        queued_duration: Optional["_models.Duration"] = None,
        running_duration: Optional["_models.Duration"] = None,
        total_duration: Optional["_models.Duration"] = None,
        job_instance_id: Optional[str] = None,
        creator_item: Optional["_models.ItemReferenceById"] = None,
        is_high_concurrency: Optional[bool] = None,
        cancellation_reason: Optional[str] = None,
        capacity_id: Optional[str] = None,
        operation_name: Optional[str] = None,
        consumer_id: Optional["_models.Principal"] = None,
        runtime_version: Optional[str] = None,
        livy_session_item_resource_uri: Optional[str] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword spark_application_id: A Spark application ID is a unique identifier assigned to each
         Apache Spark application. It also appears in the Spark UI.
        :paramtype spark_application_id: str
        :keyword state: Current state of the job. Known values are: "InProgress", "Cancelled",
         "NotStarted", "Succeeded", "Failed", and "Unknown".
        :paramtype state: str or ~microsoft.fabric.api.sparkjobdefinition.models.State
        :keyword livy_id: ID of the Livy session or Livy batch.
        :paramtype livy_id: str
        :keyword origin: Origin of the job. Known values are: "SubmittedJob" and "PendingJob".
        :paramtype origin: str or ~microsoft.fabric.api.sparkjobdefinition.models.Origin
        :keyword attempt_number: Current attempt number.
        :paramtype attempt_number: int
        :keyword max_number_of_attempts: Maximum number of attempts.
        :paramtype max_number_of_attempts: int
        :keyword livy_name: Name of the Livy session or Livy batch.
        :paramtype livy_name: str
        :keyword submitter: ID of the submitter.
        :paramtype submitter: ~microsoft.fabric.api.sparkjobdefinition.models.Principal
        :keyword item: ID of the item.
        :paramtype item: ~microsoft.fabric.api.sparkjobdefinition.models.ItemReferenceById
        :keyword item_type: The item type. Known values are: "Dashboard", "Report", "SemanticModel",
         "PaginatedReport", "Datamart", "Lakehouse", "Eventhouse", "Environment", "KQLDatabase",
         "KQLQueryset", "KQLDashboard", "DataPipeline", "Notebook", "SparkJobDefinition",
         "MLExperiment", "MLModel", "Warehouse", "Eventstream", "SQLEndpoint", "MirroredWarehouse",
         "MirroredDatabase", "Reflex", "GraphQLApi", "MountedDataFactory", "ApacheAirflowJob",
         "SQLDatabase", "CopyJob", "VariableLibrary", "MirroredAzureDatabricksCatalog", "Dataflow",
         "WarehouseSnapshot", "DigitalTwinBuilder", "DigitalTwinBuilderFlow", "AnomalyDetector", "Map",
         and "UserDataFunction".
        :paramtype item_type: str or ~microsoft.fabric.api.sparkjobdefinition.models.ItemType
        :keyword item_name: Name of the item.
        :paramtype item_name: str
        :keyword job_type: Current state of the job. Known values are: "Unknown", "SparkSession",
         "SparkBatch", and "JupyterSession".
        :paramtype job_type: str or ~microsoft.fabric.api.sparkjobdefinition.models.JobType
        :keyword submitted_date_time: Timestamp when the job was submitted in UTC, using the
         YYYY-MM-DDTHH:mm:ssZ format.
        :paramtype submitted_date_time: ~datetime.datetime
        :keyword start_date_time: Timestamp when the job started in UTC, using the YYYY-MM-DDTHH:mm:ssZ
         format.
        :paramtype start_date_time: ~datetime.datetime
        :keyword end_date_time: Timestamp when the job ended in UTC, using the YYYY-MM-DDTHH:mm:ssZ
         format.
        :paramtype end_date_time: ~datetime.datetime
        :keyword queued_duration: Duration for which the job was queued.
        :paramtype queued_duration: ~microsoft.fabric.api.sparkjobdefinition.models.Duration
        :keyword running_duration: Time it took the job to run.
        :paramtype running_duration: ~microsoft.fabric.api.sparkjobdefinition.models.Duration
        :keyword total_duration: Total duration of the job.
        :paramtype total_duration: ~microsoft.fabric.api.sparkjobdefinition.models.Duration
        :keyword job_instance_id: ID of the job instance.
        :paramtype job_instance_id: str
        :keyword creator_item: ID of the item creator. When ``isHighConcurrency`` is set to ``true``
         this value might be different than ``itemId``.
        :paramtype creator_item: ~microsoft.fabric.api.sparkjobdefinition.models.ItemReferenceById
        :keyword is_high_concurrency: Flag indicating high concurrency.
        :paramtype is_high_concurrency: bool
        :keyword cancellation_reason: Reason for the job cancellation.
        :paramtype cancellation_reason: str
        :keyword capacity_id: ID of the capacity.
        :paramtype capacity_id: str
        :keyword operation_name: Name of the operation. Possible values include: *Notebook run*\\ ,
         *Notebook HC run* and *Notebook pipeline run*.
        :paramtype operation_name: str
        :keyword consumer_id: ID of the consumer.
        :paramtype consumer_id: ~microsoft.fabric.api.sparkjobdefinition.models.Principal
        :keyword runtime_version: The fabric runtime version.
        :paramtype runtime_version: str
        :keyword livy_session_item_resource_uri: The URI used to retrieve all Livy sessions for a given
         item.
        :paramtype livy_session_item_resource_uri: str
        """
        super().__init__(**kwargs)
        self.spark_application_id = spark_application_id
        self.state = state
        self.livy_id = livy_id
        self.origin = origin
        self.attempt_number = attempt_number
        self.max_number_of_attempts = max_number_of_attempts
        self.livy_name = livy_name
        self.submitter = submitter
        self.item = item
        self.item_type = item_type
        self.item_name = item_name
        self.job_type = job_type
        self.submitted_date_time = submitted_date_time
        self.start_date_time = start_date_time
        self.end_date_time = end_date_time
        self.queued_duration = queued_duration
        self.running_duration = running_duration
        self.total_duration = total_duration
        self.job_instance_id = job_instance_id
        self.creator_item = creator_item
        self.is_high_concurrency = is_high_concurrency
        self.cancellation_reason = cancellation_reason
        self.capacity_id = capacity_id
        self.operation_name = operation_name
        self.consumer_id = consumer_id
        self.runtime_version = runtime_version
        self.livy_session_item_resource_uri = livy_session_item_resource_uri


class PaginatedResponse(_serialization.Model):
    """PaginatedResponse.

    :ivar continuation_token: The token for the next result set batch. If there are no more
     records, it's removed from the response.
    :vartype continuation_token: str
    :ivar continuation_uri: The URI of the next result set batch. If there are no more records,
     it's removed from the response.
    :vartype continuation_uri: str
    """

    _attribute_map = {
        "continuation_token": {"key": "continuationToken", "type": "str"},
        "continuation_uri": {"key": "continuationUri", "type": "str"},
    }

    def __init__(
        self, *, continuation_token: Optional[str] = None, continuation_uri: Optional[str] = None, **kwargs: Any
    ) -> None:
        """
        :keyword continuation_token: The token for the next result set batch. If there are no more
         records, it's removed from the response.
        :paramtype continuation_token: str
        :keyword continuation_uri: The URI of the next result set batch. If there are no more records,
         it's removed from the response.
        :paramtype continuation_uri: str
        """
        super().__init__(**kwargs)
        self.continuation_token = continuation_token
        self.continuation_uri = continuation_uri


class LivySessions(PaginatedResponse):
    """A paginated list of livy sessions.

    All required parameters must be populated in order to send to server.

    :ivar continuation_token: The token for the next result set batch. If there are no more
     records, it's removed from the response.
    :vartype continuation_token: str
    :ivar continuation_uri: The URI of the next result set batch. If there are no more records,
     it's removed from the response.
    :vartype continuation_uri: str
    :ivar value: A list of livy sessions. Required.
    :vartype value: list[~microsoft.fabric.api.sparkjobdefinition.models.LivySession]
    """

    _validation = {
        "value": {"required": True},
    }

    _attribute_map = {
        "continuation_token": {"key": "continuationToken", "type": "str"},
        "continuation_uri": {"key": "continuationUri", "type": "str"},
        "value": {"key": "value", "type": "[LivySession]"},
    }

    def __init__(
        self,
        *,
        value: List["_models.LivySession"],
        continuation_token: Optional[str] = None,
        continuation_uri: Optional[str] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword continuation_token: The token for the next result set batch. If there are no more
         records, it's removed from the response.
        :paramtype continuation_token: str
        :keyword continuation_uri: The URI of the next result set batch. If there are no more records,
         it's removed from the response.
        :paramtype continuation_uri: str
        :keyword value: A list of livy sessions. Required.
        :paramtype value: list[~microsoft.fabric.api.sparkjobdefinition.models.LivySession]
        """
        super().__init__(continuation_token=continuation_token, continuation_uri=continuation_uri, **kwargs)
        self.value = value


class Principal(_serialization.Model):
    """Represents an identity or a Microsoft Entra group.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to server.

    :ivar id: The principal's ID. Required.
    :vartype id: str
    :ivar display_name: The principal's display name.
    :vartype display_name: str
    :ivar type: The type of the principal. Additional principal types may be added over time.
     Required. Known values are: "User", "ServicePrincipal", "Group", "ServicePrincipalProfile", and
     "EntireTenant".
    :vartype type: str or ~microsoft.fabric.api.sparkjobdefinition.models.PrincipalType
    :ivar user_details: User principal specific details. Applicable when the principal type is
     ``User``.
    :vartype user_details: ~microsoft.fabric.api.sparkjobdefinition.models.PrincipalUserDetails
    :ivar service_principal_details: Service principal specific details. Applicable when the
     principal type is ``ServicePrincipal``.
    :vartype service_principal_details:
     ~microsoft.fabric.api.sparkjobdefinition.models.PrincipalServicePrincipalDetails
    :ivar group_details: Group specific details. Applicable when the principal type is ``Group``.
    :vartype group_details: ~microsoft.fabric.api.sparkjobdefinition.models.PrincipalGroupDetails
    :ivar service_principal_profile_details: Service principal profile details. Applicable when the
     principal type is ``ServicePrincipalProfile``.
    :vartype service_principal_profile_details:
     ~microsoft.fabric.api.sparkjobdefinition.models.PrincipalServicePrincipalProfileDetails
    """

    _validation = {
        "id": {"required": True},
        "display_name": {"readonly": True},
        "type": {"required": True},
        "user_details": {"readonly": True},
        "service_principal_details": {"readonly": True},
    }

    _attribute_map = {
        "id": {"key": "id", "type": "str"},
        "display_name": {"key": "displayName", "type": "str"},
        "type": {"key": "type", "type": "str"},
        "user_details": {"key": "userDetails", "type": "PrincipalUserDetails"},
        "service_principal_details": {"key": "servicePrincipalDetails", "type": "PrincipalServicePrincipalDetails"},
        "group_details": {"key": "groupDetails", "type": "PrincipalGroupDetails"},
        "service_principal_profile_details": {
            "key": "servicePrincipalProfileDetails",
            "type": "PrincipalServicePrincipalProfileDetails",
        },
    }

    def __init__(
        self,
        *,
        id: str,  # pylint: disable=redefined-builtin
        type: Union[str, "_models.PrincipalType"],
        group_details: Optional["_models.PrincipalGroupDetails"] = None,
        service_principal_profile_details: Optional["_models.PrincipalServicePrincipalProfileDetails"] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword id: The principal's ID. Required.
        :paramtype id: str
        :keyword type: The type of the principal. Additional principal types may be added over time.
         Required. Known values are: "User", "ServicePrincipal", "Group", "ServicePrincipalProfile", and
         "EntireTenant".
        :paramtype type: str or ~microsoft.fabric.api.sparkjobdefinition.models.PrincipalType
        :keyword group_details: Group specific details. Applicable when the principal type is
         ``Group``.
        :paramtype group_details: ~microsoft.fabric.api.sparkjobdefinition.models.PrincipalGroupDetails
        :keyword service_principal_profile_details: Service principal profile details. Applicable when
         the principal type is ``ServicePrincipalProfile``.
        :paramtype service_principal_profile_details:
         ~microsoft.fabric.api.sparkjobdefinition.models.PrincipalServicePrincipalProfileDetails
        """
        super().__init__(**kwargs)
        self.id = id
        self.display_name = None
        self.type = type
        self.user_details = None
        self.service_principal_details = None
        self.group_details = group_details
        self.service_principal_profile_details = service_principal_profile_details


class PrincipalGroupDetails(_serialization.Model):
    """Group specific details. Applicable when the principal type is ``Group``.

    :ivar group_type: The type of the group. Additional group types may be added over time. Known
     values are: "Unknown", "SecurityGroup", and "DistributionList".
    :vartype group_type: str or ~microsoft.fabric.api.sparkjobdefinition.models.GroupType
    """

    _attribute_map = {
        "group_type": {"key": "groupType", "type": "str"},
    }

    def __init__(self, *, group_type: Optional[Union[str, "_models.GroupType"]] = None, **kwargs: Any) -> None:
        """
        :keyword group_type: The type of the group. Additional group types may be added over time.
         Known values are: "Unknown", "SecurityGroup", and "DistributionList".
        :paramtype group_type: str or ~microsoft.fabric.api.sparkjobdefinition.models.GroupType
        """
        super().__init__(**kwargs)
        self.group_type = group_type


class PrincipalServicePrincipalDetails(_serialization.Model):
    """Service principal specific details. Applicable when the principal type is ``ServicePrincipal``.

    Variables are only populated by the server, and will be ignored when sending a request.

    :ivar aad_app_id: The service principal's Microsoft Entra AppId.
    :vartype aad_app_id: str
    """

    _validation = {
        "aad_app_id": {"readonly": True},
    }

    _attribute_map = {
        "aad_app_id": {"key": "aadAppId", "type": "str"},
    }

    def __init__(self, **kwargs: Any) -> None:
        """ """
        super().__init__(**kwargs)
        self.aad_app_id = None


class PrincipalServicePrincipalProfileDetails(_serialization.Model):
    """Service principal profile details. Applicable when the principal type is
    ``ServicePrincipalProfile``.

    :ivar parent_principal: The service principal profile's parent principal.
    :vartype parent_principal: ~microsoft.fabric.api.sparkjobdefinition.models.Principal
    """

    _attribute_map = {
        "parent_principal": {"key": "parentPrincipal", "type": "Principal"},
    }

    def __init__(self, *, parent_principal: Optional["_models.Principal"] = None, **kwargs: Any) -> None:
        """
        :keyword parent_principal: The service principal profile's parent principal.
        :paramtype parent_principal: ~microsoft.fabric.api.sparkjobdefinition.models.Principal
        """
        super().__init__(**kwargs)
        self.parent_principal = parent_principal


class PrincipalUserDetails(_serialization.Model):
    """User principal specific details. Applicable when the principal type is ``User``.

    Variables are only populated by the server, and will be ignored when sending a request.

    :ivar user_principal_name: The user principal name.
    :vartype user_principal_name: str
    """

    _validation = {
        "user_principal_name": {"readonly": True},
    }

    _attribute_map = {
        "user_principal_name": {"key": "userPrincipalName", "type": "str"},
    }

    def __init__(self, **kwargs: Any) -> None:
        """ """
        super().__init__(**kwargs)
        self.user_principal_name = None


class RunSparkJobDefinitionRequest(_serialization.Model):
    """Run spark job definition request with executionData.

    :ivar execution_data: The spark job definition parameters to be used during execution if
     needed. By default no body is needed.
    :vartype execution_data: ~microsoft.fabric.api.sparkjobdefinition.models.ExecutionData
    """

    _attribute_map = {
        "execution_data": {"key": "executionData", "type": "ExecutionData"},
    }

    def __init__(self, *, execution_data: Optional["_models.ExecutionData"] = None, **kwargs: Any) -> None:
        """
        :keyword execution_data: The spark job definition parameters to be used during execution if
         needed. By default no body is needed.
        :paramtype execution_data: ~microsoft.fabric.api.sparkjobdefinition.models.ExecutionData
        """
        super().__init__(**kwargs)
        self.execution_data = execution_data


class SparkJobDefinition(Item):
    """A spark job definition object.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to server.

    :ivar id: The item ID.
    :vartype id: str
    :ivar display_name: The item display name.
    :vartype display_name: str
    :ivar description: The item description.
    :vartype description: str
    :ivar type: The item type. Required. Known values are: "Dashboard", "Report", "SemanticModel",
     "PaginatedReport", "Datamart", "Lakehouse", "Eventhouse", "Environment", "KQLDatabase",
     "KQLQueryset", "KQLDashboard", "DataPipeline", "Notebook", "SparkJobDefinition",
     "MLExperiment", "MLModel", "Warehouse", "Eventstream", "SQLEndpoint", "MirroredWarehouse",
     "MirroredDatabase", "Reflex", "GraphQLApi", "MountedDataFactory", "ApacheAirflowJob",
     "SQLDatabase", "CopyJob", "VariableLibrary", "MirroredAzureDatabricksCatalog", "Dataflow",
     "WarehouseSnapshot", "DigitalTwinBuilder", "DigitalTwinBuilderFlow", "AnomalyDetector", "Map",
     and "UserDataFunction".
    :vartype type: str or ~microsoft.fabric.api.sparkjobdefinition.models.ItemType
    :ivar workspace_id: The workspace ID.
    :vartype workspace_id: str
    :ivar folder_id: The folder ID.
    :vartype folder_id: str
    :ivar tags: List of applied tags.
    :vartype tags: list[~microsoft.fabric.api.sparkjobdefinition.models.ItemTag]
    :ivar properties: The spark job definition properties.
    :vartype properties:
     ~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinitionProperties
    """

    _validation = {
        "id": {"readonly": True},
        "type": {"required": True},
        "workspace_id": {"readonly": True},
        "folder_id": {"readonly": True},
        "tags": {"readonly": True},
    }

    _attribute_map = {
        "id": {"key": "id", "type": "str"},
        "display_name": {"key": "displayName", "type": "str"},
        "description": {"key": "description", "type": "str"},
        "type": {"key": "type", "type": "str"},
        "workspace_id": {"key": "workspaceId", "type": "str"},
        "folder_id": {"key": "folderId", "type": "str"},
        "tags": {"key": "tags", "type": "[ItemTag]"},
        "properties": {"key": "properties", "type": "SparkJobDefinitionProperties"},
    }

    def __init__(
        self,
        *,
        type: Union[str, "_models.ItemType"],
        display_name: Optional[str] = None,
        description: Optional[str] = None,
        properties: Optional["_models.SparkJobDefinitionProperties"] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword display_name: The item display name.
        :paramtype display_name: str
        :keyword description: The item description.
        :paramtype description: str
        :keyword type: The item type. Required. Known values are: "Dashboard", "Report",
         "SemanticModel", "PaginatedReport", "Datamart", "Lakehouse", "Eventhouse", "Environment",
         "KQLDatabase", "KQLQueryset", "KQLDashboard", "DataPipeline", "Notebook", "SparkJobDefinition",
         "MLExperiment", "MLModel", "Warehouse", "Eventstream", "SQLEndpoint", "MirroredWarehouse",
         "MirroredDatabase", "Reflex", "GraphQLApi", "MountedDataFactory", "ApacheAirflowJob",
         "SQLDatabase", "CopyJob", "VariableLibrary", "MirroredAzureDatabricksCatalog", "Dataflow",
         "WarehouseSnapshot", "DigitalTwinBuilder", "DigitalTwinBuilderFlow", "AnomalyDetector", "Map",
         and "UserDataFunction".
        :paramtype type: str or ~microsoft.fabric.api.sparkjobdefinition.models.ItemType
        :keyword properties: The spark job definition properties.
        :paramtype properties:
         ~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinitionProperties
        """
        super().__init__(display_name=display_name, description=description, type=type, **kwargs)
        self.properties = properties


class SparkJobDefinitionProperties(_serialization.Model):
    """The spark job definition properties.

    All required parameters must be populated in order to send to server.

    :ivar one_lake_root_path: OneLake path to the SparkJobDefinition root directory. Required.
    :vartype one_lake_root_path: str
    """

    _validation = {
        "one_lake_root_path": {"required": True},
    }

    _attribute_map = {
        "one_lake_root_path": {"key": "oneLakeRootPath", "type": "str"},
    }

    def __init__(self, *, one_lake_root_path: str, **kwargs: Any) -> None:
        """
        :keyword one_lake_root_path: OneLake path to the SparkJobDefinition root directory. Required.
        :paramtype one_lake_root_path: str
        """
        super().__init__(**kwargs)
        self.one_lake_root_path = one_lake_root_path


class SparkJobDefinitionPublicDefinition(_serialization.Model):
    """Spark job definition public definition object. Refer to this `article
    </rest/api/fabric/articles/item-management/definitions/spark-job-definition>`_ for more details
    on how to craft a spark job definition public definition.

    All required parameters must be populated in order to send to server.

    :ivar format: The format of the item definition. Supported format: ``SparkJobDefinitionV1``.
    :vartype format: str
    :ivar parts: A list of definition parts. Required.
    :vartype parts:
     list[~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinitionPublicDefinitionPart]
    """

    _validation = {
        "parts": {"required": True},
    }

    _attribute_map = {
        "format": {"key": "format", "type": "str"},
        "parts": {"key": "parts", "type": "[SparkJobDefinitionPublicDefinitionPart]"},
    }

    def __init__(
        self,
        *,
        parts: List["_models.SparkJobDefinitionPublicDefinitionPart"],
        format: Optional[str] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword format: The format of the item definition. Supported format: ``SparkJobDefinitionV1``.
        :paramtype format: str
        :keyword parts: A list of definition parts. Required.
        :paramtype parts:
         list[~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinitionPublicDefinitionPart]
        """
        super().__init__(**kwargs)
        self.format = format
        self.parts = parts


class SparkJobDefinitionPublicDefinitionPart(_serialization.Model):
    """Spark job definition definition part object.

    :ivar path: The spark job definition public definition part path.
    :vartype path: str
    :ivar payload: The spark job definition public definition part payload.
    :vartype payload: str
    :ivar payload_type: The payload type. "InlineBase64"
    :vartype payload_type: str or ~microsoft.fabric.api.sparkjobdefinition.models.PayloadType
    """

    _attribute_map = {
        "path": {"key": "path", "type": "str"},
        "payload": {"key": "payload", "type": "str"},
        "payload_type": {"key": "payloadType", "type": "str"},
    }

    def __init__(
        self,
        *,
        path: Optional[str] = None,
        payload: Optional[str] = None,
        payload_type: Optional[Union[str, "_models.PayloadType"]] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword path: The spark job definition public definition part path.
        :paramtype path: str
        :keyword payload: The spark job definition public definition part payload.
        :paramtype payload: str
        :keyword payload_type: The payload type. "InlineBase64"
        :paramtype payload_type: str or ~microsoft.fabric.api.sparkjobdefinition.models.PayloadType
        """
        super().__init__(**kwargs)
        self.path = path
        self.payload = payload
        self.payload_type = payload_type


class SparkJobDefinitionResponse(_serialization.Model):
    """Spark job definition public definition response.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to server.

    :ivar definition: Spark job definition public definition object. Refer to this `article
     </rest/api/fabric/articles/item-management/definitions/spark-job-definition>`_ for more details
     on how to craft a spark job definition public definition. Required.
    :vartype definition:
     ~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinitionPublicDefinition
    """

    _validation = {
        "definition": {"required": True, "readonly": True},
    }

    _attribute_map = {
        "definition": {"key": "definition", "type": "SparkJobDefinitionPublicDefinition"},
    }

    def __init__(self, **kwargs: Any) -> None:
        """ """
        super().__init__(**kwargs)
        self.definition = None


class SparkJobDefinitions(PaginatedResponse):
    """A list of spark job definitions.

    All required parameters must be populated in order to send to server.

    :ivar continuation_token: The token for the next result set batch. If there are no more
     records, it's removed from the response.
    :vartype continuation_token: str
    :ivar continuation_uri: The URI of the next result set batch. If there are no more records,
     it's removed from the response.
    :vartype continuation_uri: str
    :ivar value: A list of spark job definitions. Required.
    :vartype value: list[~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinition]
    """

    _validation = {
        "value": {"required": True},
    }

    _attribute_map = {
        "continuation_token": {"key": "continuationToken", "type": "str"},
        "continuation_uri": {"key": "continuationUri", "type": "str"},
        "value": {"key": "value", "type": "[SparkJobDefinition]"},
    }

    def __init__(
        self,
        *,
        value: List["_models.SparkJobDefinition"],
        continuation_token: Optional[str] = None,
        continuation_uri: Optional[str] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword continuation_token: The token for the next result set batch. If there are no more
         records, it's removed from the response.
        :paramtype continuation_token: str
        :keyword continuation_uri: The URI of the next result set batch. If there are no more records,
         it's removed from the response.
        :paramtype continuation_uri: str
        :keyword value: A list of spark job definitions. Required.
        :paramtype value: list[~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinition]
        """
        super().__init__(continuation_token=continuation_token, continuation_uri=continuation_uri, **kwargs)
        self.value = value


class UpdateSparkJobDefinitionDefinitionRequest(_serialization.Model):  # pylint: disable=name-too-long
    """Update spark job definition public definition request payload.

    All required parameters must be populated in order to send to server.

    :ivar definition: Spark job definition public definition object. Refer to this `article
     </rest/api/fabric/articles/item-management/definitions/spark-job-definition>`_ for more details
     on how to craft a spark job definition public definition. Required.
    :vartype definition:
     ~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinitionPublicDefinition
    """

    _validation = {
        "definition": {"required": True},
    }

    _attribute_map = {
        "definition": {"key": "definition", "type": "SparkJobDefinitionPublicDefinition"},
    }

    def __init__(self, *, definition: "_models.SparkJobDefinitionPublicDefinition", **kwargs: Any) -> None:
        """
        :keyword definition: Spark job definition public definition object. Refer to this `article
         </rest/api/fabric/articles/item-management/definitions/spark-job-definition>`_ for more details
         on how to craft a spark job definition public definition. Required.
        :paramtype definition:
         ~microsoft.fabric.api.sparkjobdefinition.models.SparkJobDefinitionPublicDefinition
        """
        super().__init__(**kwargs)
        self.definition = definition


class UpdateSparkJobDefinitionRequest(_serialization.Model):
    """Update spark job definition request.

    :ivar display_name: The spark job definition display name. The display name must follow naming
     rules according to item type.
    :vartype display_name: str
    :ivar description: The spark job definition description. Maximum length is 256 characters.
    :vartype description: str
    """

    _attribute_map = {
        "display_name": {"key": "displayName", "type": "str"},
        "description": {"key": "description", "type": "str"},
    }

    def __init__(self, *, display_name: Optional[str] = None, description: Optional[str] = None, **kwargs: Any) -> None:
        """
        :keyword display_name: The spark job definition display name. The display name must follow
         naming rules according to item type.
        :paramtype display_name: str
        :keyword description: The spark job definition description. Maximum length is 256 characters.
        :paramtype description: str
        """
        super().__init__(**kwargs)
        self.display_name = display_name
        self.description = description
