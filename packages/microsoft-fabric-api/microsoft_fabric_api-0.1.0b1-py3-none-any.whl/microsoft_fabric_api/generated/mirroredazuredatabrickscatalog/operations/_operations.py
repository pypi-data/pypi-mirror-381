# pylint: disable=too-many-lines,too-many-statements
# coding=utf-8
# --------------------------------------------------------------------------
# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.10.3, generator: @autorest/python@6.15.0)
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
from io import IOBase
import sys
from typing import Any, Callable, Dict, IO, Iterable, Iterator, Optional, Type, TypeVar, Union, cast, overload

from azure.core.exceptions import (
    ClientAuthenticationError,
    HttpResponseError,
    ResourceExistsError,
    ResourceNotFoundError,
    ResourceNotModifiedError,
    map_error,
)
from azure.core.paging import ItemPaged
from azure.core.pipeline import PipelineResponse
from azure.core.polling import LROPoller, NoPolling, PollingMethod
from azure.core.polling.base_polling import LROBasePolling
from azure.core.rest import HttpRequest, HttpResponse
from azure.core.tracing.decorator import distributed_trace
from azure.core.utils import case_insensitive_dict

from .. import models as _models
from .._serialization import Serializer

if sys.version_info >= (3, 9):
    from collections.abc import MutableMapping
else:
    from typing import MutableMapping  # type: ignore  # pylint: disable=ungrouped-imports
T = TypeVar("T")
ClsType = Optional[Callable[[PipelineResponse[HttpRequest, HttpResponse], T, Dict[str, Any]], Any]]

_SERIALIZER = Serializer()
_SERIALIZER.client_side_validation = False


def build_items_list_mirrored_azure_databricks_catalogs_request(  # pylint: disable=name-too-long
    workspace_id: str, *, continuation_token_parameter: Optional[str] = None, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/mirroredAzureDatabricksCatalogs"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct parameters
    if continuation_token_parameter is not None:
        _params["continuationToken"] = _SERIALIZER.query(
            "continuation_token_parameter", continuation_token_parameter, "str"
        )

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="GET", url=_url, params=_params, headers=_headers, **kwargs)


def build_items_create_mirrored_azure_databricks_catalog_request(  # pylint: disable=name-too-long
    workspace_id: str, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/mirroredAzureDatabricksCatalogs"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="POST", url=_url, headers=_headers, **kwargs)


def build_items_get_mirrored_azure_databricks_catalog_request(  # pylint: disable=name-too-long
    workspace_id: str, mirrored_azure_databricks_catalog_id: str, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/mirroredAzureDatabricksCatalogs/{mirroredAzureDatabricksCatalogId}"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "mirroredAzureDatabricksCatalogId": _SERIALIZER.url(
            "mirrored_azure_databricks_catalog_id", mirrored_azure_databricks_catalog_id, "str"
        ),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="GET", url=_url, headers=_headers, **kwargs)


def build_items_update_mirrored_azure_databricks_catalog_request(  # pylint: disable=name-too-long
    workspace_id: str, mirrored_azure_databricks_catalog_id: str, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/mirroredAzureDatabricksCatalogs/{mirroredAzureDatabricksCatalogId}"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "mirroredAzureDatabricksCatalogId": _SERIALIZER.url(
            "mirrored_azure_databricks_catalog_id", mirrored_azure_databricks_catalog_id, "str"
        ),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="PATCH", url=_url, headers=_headers, **kwargs)


def build_items_delete_mirrored_azure_databricks_catalog_request(  # pylint: disable=name-too-long
    workspace_id: str, mirrored_azure_databricks_catalog_id: str, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/mirroredAzureDatabricksCatalogs/{mirroredAzureDatabricksCatalogId}"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "mirroredAzureDatabricksCatalogId": _SERIALIZER.url(
            "mirrored_azure_databricks_catalog_id", mirrored_azure_databricks_catalog_id, "str"
        ),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="DELETE", url=_url, headers=_headers, **kwargs)


def build_items_get_mirrored_azure_databricks_catalog_definition_request(  # pylint: disable=name-too-long
    workspace_id: str, mirrored_azure_databricks_catalog_id: str, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/mirroredAzureDatabricksCatalogs/{mirroredAzureDatabricksCatalogId}/getDefinition"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "mirroredAzureDatabricksCatalogId": _SERIALIZER.url(
            "mirrored_azure_databricks_catalog_id", mirrored_azure_databricks_catalog_id, "str"
        ),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="POST", url=_url, headers=_headers, **kwargs)


def build_items_update_mirrored_azure_databricks_catalog_definition_request(  # pylint: disable=name-too-long
    workspace_id: str, mirrored_azure_databricks_catalog_id: str, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = (
        "/workspaces/{workspaceId}/mirroredAzureDatabricksCatalogs/{mirroredAzureDatabricksCatalogId}/updateDefinition"
    )
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "mirroredAzureDatabricksCatalogId": _SERIALIZER.url(
            "mirrored_azure_databricks_catalog_id", mirrored_azure_databricks_catalog_id, "str"
        ),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="POST", url=_url, headers=_headers, **kwargs)


def build_refresh_refresh_catalog_metadata_request(  # pylint: disable=name-too-long
    workspace_id: str, mirrored_azure_databricks_catalog_id: str, **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/mirroredAzureDatabricksCatalogs/{mirroredAzureDatabricksCatalogId}/refreshCatalogMetadata"  # pylint: disable=line-too-long
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "mirroredAzureDatabricksCatalogId": _SERIALIZER.url(
            "mirrored_azure_databricks_catalog_id", mirrored_azure_databricks_catalog_id, "str"
        ),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="POST", url=_url, headers=_headers, **kwargs)


def build_discovery_discover_catalogs_request(  # pylint: disable=name-too-long
    workspace_id: str,
    *,
    databricks_workspace_connection_id: str,
    continuation_token_parameter: Optional[str] = None,
    max_results: Optional[int] = None,
    **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/azuredatabricks/catalogs"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct parameters
    _params["databricksWorkspaceConnectionId"] = _SERIALIZER.query(
        "databricks_workspace_connection_id", databricks_workspace_connection_id, "str"
    )
    if continuation_token_parameter is not None:
        _params["continuationToken"] = _SERIALIZER.query(
            "continuation_token_parameter", continuation_token_parameter, "str"
        )
    if max_results is not None:
        _params["maxResults"] = _SERIALIZER.query("max_results", max_results, "int")

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="GET", url=_url, params=_params, headers=_headers, **kwargs)


def build_discovery_discover_schemas_request(
    workspace_id: str,
    catalog_name: str,
    *,
    databricks_workspace_connection_id: str,
    continuation_token_parameter: Optional[str] = None,
    max_results: Optional[int] = None,
    **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/azuredatabricks/catalogs/{catalogName}/schemas"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "catalogName": _SERIALIZER.url("catalog_name", catalog_name, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct parameters
    _params["databricksWorkspaceConnectionId"] = _SERIALIZER.query(
        "databricks_workspace_connection_id", databricks_workspace_connection_id, "str"
    )
    if continuation_token_parameter is not None:
        _params["continuationToken"] = _SERIALIZER.query(
            "continuation_token_parameter", continuation_token_parameter, "str"
        )
    if max_results is not None:
        _params["maxResults"] = _SERIALIZER.query("max_results", max_results, "int")

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="GET", url=_url, params=_params, headers=_headers, **kwargs)


def build_discovery_discover_tables_request(
    workspace_id: str,
    catalog_name: str,
    schema_name: str,
    *,
    databricks_workspace_connection_id: str,
    continuation_token_parameter: Optional[str] = None,
    max_results: Optional[int] = None,
    **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/workspaces/{workspaceId}/azuredatabricks/catalogs/{catalogName}/schemas/{schemaName}/tables"
    path_format_arguments = {
        "workspaceId": _SERIALIZER.url("workspace_id", workspace_id, "str"),
        "catalogName": _SERIALIZER.url("catalog_name", catalog_name, "str"),
        "schemaName": _SERIALIZER.url("schema_name", schema_name, "str"),
    }

    _url: str = _url.format(**path_format_arguments)  # type: ignore

    # Construct parameters
    _params["databricksWorkspaceConnectionId"] = _SERIALIZER.query(
        "databricks_workspace_connection_id", databricks_workspace_connection_id, "str"
    )
    if continuation_token_parameter is not None:
        _params["continuationToken"] = _SERIALIZER.query(
            "continuation_token_parameter", continuation_token_parameter, "str"
        )
    if max_results is not None:
        _params["maxResults"] = _SERIALIZER.query("max_results", max_results, "int")

    # Construct headers
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")

    return HttpRequest(method="GET", url=_url, params=_params, headers=_headers, **kwargs)


class ItemsOperations:
    """
    .. warning::
        **DO NOT** instantiate this class directly.

        Instead, you should access the following operations through
        :class:`~microsoft.fabric.api.mirroredazuredatabrickscatalog.FabricMirroredAzureDatabricksCatalogClient`'s
        :attr:`items` attribute.
    """

    models = _models

    def __init__(self, *args, **kwargs):
        input_args = list(args)
        self._client = input_args.pop(0) if input_args else kwargs.pop("client")
        self._config = input_args.pop(0) if input_args else kwargs.pop("config")
        self._serialize = input_args.pop(0) if input_args else kwargs.pop("serializer")
        self._deserialize = input_args.pop(0) if input_args else kwargs.pop("deserializer")

    @distributed_trace
    def list_mirrored_azure_databricks_catalogs(
        self, workspace_id: str, *, continuation_token_parameter: Optional[str] = None, **kwargs: Any
    ) -> Iterable["_models.MirroredAzureDatabricksCatalog"]:
        """Returns a list of mirroredAzureDatabricksCatalogs from the specified workspace.

        ..

           [!NOTE]
           Mirrored Azure Databricks Catalog item is currently in Preview (\\ `learn more
        </fabric/fundamentals/preview>`_\\ ).


        This API supports `pagination </rest/api/fabric/articles/pagination>`_.

        Permissions
        -----------

         The caller must have a *viewer* workspace role.

        Required Delegated Scopes
        -------------------------

        Workspace.Read.All or Workspace.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :keyword continuation_token_parameter: A token for retrieving the next page of results. Default
         value is None.
        :paramtype continuation_token_parameter: str
        :return: An iterator like instance of MirroredAzureDatabricksCatalog
        :rtype:
         ~azure.core.paging.ItemPaged[~microsoft.fabric.api.mirroredazuredatabrickscatalog.models.MirroredAzureDatabricksCatalog]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models._models.MirroredAzureDatabricksCatalogs] = kwargs.pop(  # pylint: disable=protected-access
            "cls", None
        )

        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        def prepare_request(next_link=None):
            if not next_link:

                _request = build_items_list_mirrored_azure_databricks_catalogs_request(
                    workspace_id=workspace_id,
                    continuation_token_parameter=continuation_token_parameter,
                    headers=_headers,
                    params=_params,
                )
                _request.url = self._client.format_url(_request.url)

            else:
                _request = HttpRequest("GET", next_link)
                _request.url = self._client.format_url(_request.url)

            return _request

        def extract_data(pipeline_response):
            deserialized = self._deserialize(
                _models._models.MirroredAzureDatabricksCatalogs, pipeline_response  # pylint: disable=protected-access
            )
            list_of_elem = deserialized.value
            if cls:
                list_of_elem = cls(list_of_elem)  # type: ignore
            return deserialized.continuation_uri or None, iter(list_of_elem)

        def get_next(next_link=None):
            _request = prepare_request(next_link)

            _stream = False
            pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
                _request, stream=_stream, **kwargs
            )
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
                raise HttpResponseError(response=response, model=error)

            return pipeline_response

        return ItemPaged(get_next, extract_data)

    def _create_mirrored_azure_databricks_catalog_initial(  # pylint: disable=name-too-long
        self,
        workspace_id: str,
        create_mirrored_azure_databricks_catalog_request: Union[
            _models.CreateMirroredAzureDatabricksCatalogRequest, IO[bytes]
        ],
        **kwargs: Any
    ) -> Iterator[bytes]:
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[Iterator[bytes]] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _json = None
        _content = None
        if isinstance(create_mirrored_azure_databricks_catalog_request, (IOBase, bytes)):
            _content = create_mirrored_azure_databricks_catalog_request
        else:
            _json = self._serialize.body(
                create_mirrored_azure_databricks_catalog_request, "CreateMirroredAzureDatabricksCatalogRequest"
            )

        _request = build_items_create_mirrored_azure_databricks_catalog_request(
            workspace_id=workspace_id,
            content_type=content_type,
            json=_json,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = True
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [201, 202]:
            response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        if response.status_code == 201:
            deserialized = response.iter_bytes()

        if response.status_code == 202:
            response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))
            response_headers["x-ms-operation-id"] = self._deserialize("str", response.headers.get("x-ms-operation-id"))
            response_headers["Retry-After"] = self._deserialize("int", response.headers.get("Retry-After"))

            deserialized = response.iter_bytes()

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @overload
    def begin_create_mirrored_azure_databricks_catalog(  # pylint: disable=name-too-long
        self,
        workspace_id: str,
        create_mirrored_azure_databricks_catalog_request: _models.CreateMirroredAzureDatabricksCatalogRequest,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> LROPoller[_models.MirroredAzureDatabricksCatalog]:
        """Creates a mirroredAzureDatabricksCatalog in the specified workspace.

        ..

           [!NOTE]
           Mirrored Azure Databricks Catalog item is currently in Preview (\\ `learn more
        </fabric/fundamentals/preview>`_\\ ).


        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Permissions
        -----------

         The caller must have a *contributor* workspace role.

        Required Delegated Scopes
        -------------------------

         MirroredAzureDatabricksCatalog.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param create_mirrored_azure_databricks_catalog_request: Create item request payload. Required.
        :type create_mirrored_azure_databricks_catalog_request:
         ~microsoft.fabric.api.mirroredazuredatabrickscatalog.models.CreateMirroredAzureDatabricksCatalogRequest
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: An instance of LROPoller that returns MirroredAzureDatabricksCatalog
        :rtype:
         ~azure.core.polling.LROPoller[~microsoft.fabric.api.mirroredazuredatabrickscatalog.models.MirroredAzureDatabricksCatalog]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    def begin_create_mirrored_azure_databricks_catalog(  # pylint: disable=name-too-long
        self,
        workspace_id: str,
        create_mirrored_azure_databricks_catalog_request: IO[bytes],
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> LROPoller[_models.MirroredAzureDatabricksCatalog]:
        """Creates a mirroredAzureDatabricksCatalog in the specified workspace.

        ..

           [!NOTE]
           Mirrored Azure Databricks Catalog item is currently in Preview (\\ `learn more
        </fabric/fundamentals/preview>`_\\ ).


        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Permissions
        -----------

         The caller must have a *contributor* workspace role.

        Required Delegated Scopes
        -------------------------

         MirroredAzureDatabricksCatalog.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param create_mirrored_azure_databricks_catalog_request: Create item request payload. Required.
        :type create_mirrored_azure_databricks_catalog_request: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: An instance of LROPoller that returns MirroredAzureDatabricksCatalog
        :rtype:
         ~azure.core.polling.LROPoller[~microsoft.fabric.api.mirroredazuredatabrickscatalog.models.MirroredAzureDatabricksCatalog]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace
    def begin_create_mirrored_azure_databricks_catalog(  # pylint: disable=name-too-long
        self,
        workspace_id: str,
        create_mirrored_azure_databricks_catalog_request: Union[
            _models.CreateMirroredAzureDatabricksCatalogRequest, IO[bytes]
        ],
        **kwargs: Any
    ) -> LROPoller[_models.MirroredAzureDatabricksCatalog]:
        """Creates a mirroredAzureDatabricksCatalog in the specified workspace.

        ..

           [!NOTE]
           Mirrored Azure Databricks Catalog item is currently in Preview (\\ `learn more
        </fabric/fundamentals/preview>`_\\ ).


        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Permissions
        -----------

         The caller must have a *contributor* workspace role.

        Required Delegated Scopes
        -------------------------

         MirroredAzureDatabricksCatalog.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param create_mirrored_azure_databricks_catalog_request: Create item request payload. Is either
         a CreateMirroredAzureDatabricksCatalogRequest type or a IO[bytes] type. Required.
        :type create_mirrored_azure_databricks_catalog_request:
         ~microsoft.fabric.api.mirroredazuredatabrickscatalog.models.CreateMirroredAzureDatabricksCatalogRequest
         or IO[bytes]
        :return: An instance of LROPoller that returns MirroredAzureDatabricksCatalog
        :rtype:
         ~azure.core.polling.LROPoller[~microsoft.fabric.api.mirroredazuredatabrickscatalog.models.MirroredAzureDatabricksCatalog]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.MirroredAzureDatabricksCatalog] = kwargs.pop("cls", None)
        polling: Union[bool, PollingMethod] = kwargs.pop("polling", True)
        lro_delay = kwargs.pop("polling_interval", self._config.polling_interval)
        cont_token: Optional[str] = kwargs.pop("continuation_token", None)
        if cont_token is None:
            raw_result = self._create_mirrored_azure_databricks_catalog_initial(
                workspace_id=workspace_id,
                create_mirrored_azure_databricks_catalog_request=create_mirrored_azure_databricks_catalog_request,
                content_type=content_type,
                cls=lambda x, y, z: x,
                headers=_headers,
                params=_params,
                **kwargs
            )
            raw_result.http_response.read()  # type: ignore
        kwargs.pop("error_map", None)

        def get_long_running_output(pipeline_response):
            deserialized = self._deserialize("MirroredAzureDatabricksCatalog", pipeline_response.http_response)
            if cls:
                return cls(pipeline_response, deserialized, {})  # type: ignore
            return deserialized

        if polling is True:
            polling_method: PollingMethod = cast(PollingMethod, LROBasePolling(lro_delay, **kwargs))
        elif polling is False:
            polling_method = cast(PollingMethod, NoPolling())
        else:
            polling_method = polling
        if cont_token:
            return LROPoller[_models.MirroredAzureDatabricksCatalog].from_continuation_token(
                polling_method=polling_method,
                continuation_token=cont_token,
                client=self._client,
                deserialization_callback=get_long_running_output,
            )
        return LROPoller[_models.MirroredAzureDatabricksCatalog](
            self._client, raw_result, get_long_running_output, polling_method  # type: ignore
        )

    @distributed_trace
    def get_mirrored_azure_databricks_catalog(
        self, workspace_id: str, mirrored_azure_databricks_catalog_id: str, **kwargs: Any
    ) -> _models.MirroredAzureDatabricksCatalog:
        """Returns properties of the specified mirroredAzureDatabricksCatalog.

        ..

           [!NOTE]
           Mirrored Azure Databricks Catalog item is currently in Preview (\\ `learn more
        </fabric/fundamentals/preview>`_\\ ).


        Permissions
        -----------

         The caller must have *read* permissions for the mirrored azure databricks catalog.

        Required Delegated Scopes
        -------------------------

         MirroredAzureDatabricksCatalog.Read.All or MirroredAzureDatabricksCatalog.ReadWrite.All or
        Item.Read.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param mirrored_azure_databricks_catalog_id: The mirroredAzureDatabricksCatalog ID. Required.
        :type mirrored_azure_databricks_catalog_id: str
        :return: MirroredAzureDatabricksCatalog
        :rtype:
         ~microsoft.fabric.api.mirroredazuredatabrickscatalog.models.MirroredAzureDatabricksCatalog
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.MirroredAzureDatabricksCatalog] = kwargs.pop("cls", None)

        _request = build_items_get_mirrored_azure_databricks_catalog_request(
            workspace_id=workspace_id,
            mirrored_azure_databricks_catalog_id=mirrored_azure_databricks_catalog_id,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        deserialized = self._deserialize("MirroredAzureDatabricksCatalog", pipeline_response.http_response)

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    def update_mirrored_azure_databricks_catalog(
        self,
        workspace_id: str,
        mirrored_azure_databricks_catalog_id: str,
        update_mirrored_azure_databricks_catalog_request: _models.UpdateMirroredAzureDatabricksCatalogRequest,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.MirroredAzureDatabricksCatalog:
        """Updates the properties of the specified mirroredAzureDatabricksCatalog.

        ..

           [!NOTE]
           Mirrored Azure Databricks Catalog item is currently in Preview (\\ `learn more
        </fabric/fundamentals/preview>`_\\ ).


        Permissions
        -----------

         The caller must have *read and write* permissions for the mirrored azure databricks catalog.

        Required Delegated Scopes
        -------------------------

         MirroredAzureDatabricksCatalog.ReadWrite.All or Item.ReadWrite.All

         [!NOTE]

        ..

           Item.Execute.All is required if you are updating AutoSync property.


        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param mirrored_azure_databricks_catalog_id: The mirroredAzureDatabricksCatalog ID. Required.
        :type mirrored_azure_databricks_catalog_id: str
        :param update_mirrored_azure_databricks_catalog_request: Update mirroredAzureDatabricksCatalog
         request payload. Required.
        :type update_mirrored_azure_databricks_catalog_request:
         ~microsoft.fabric.api.mirroredazuredatabrickscatalog.models.UpdateMirroredAzureDatabricksCatalogRequest
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: MirroredAzureDatabricksCatalog
        :rtype:
         ~microsoft.fabric.api.mirroredazuredatabrickscatalog.models.MirroredAzureDatabricksCatalog
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    def update_mirrored_azure_databricks_catalog(
        self,
        workspace_id: str,
        mirrored_azure_databricks_catalog_id: str,
        update_mirrored_azure_databricks_catalog_request: IO[bytes],
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.MirroredAzureDatabricksCatalog:
        """Updates the properties of the specified mirroredAzureDatabricksCatalog.

        ..

           [!NOTE]
           Mirrored Azure Databricks Catalog item is currently in Preview (\\ `learn more
        </fabric/fundamentals/preview>`_\\ ).


        Permissions
        -----------

         The caller must have *read and write* permissions for the mirrored azure databricks catalog.

        Required Delegated Scopes
        -------------------------

         MirroredAzureDatabricksCatalog.ReadWrite.All or Item.ReadWrite.All

         [!NOTE]

        ..

           Item.Execute.All is required if you are updating AutoSync property.


        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param mirrored_azure_databricks_catalog_id: The mirroredAzureDatabricksCatalog ID. Required.
        :type mirrored_azure_databricks_catalog_id: str
        :param update_mirrored_azure_databricks_catalog_request: Update mirroredAzureDatabricksCatalog
         request payload. Required.
        :type update_mirrored_azure_databricks_catalog_request: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: MirroredAzureDatabricksCatalog
        :rtype:
         ~microsoft.fabric.api.mirroredazuredatabrickscatalog.models.MirroredAzureDatabricksCatalog
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace
    def update_mirrored_azure_databricks_catalog(
        self,
        workspace_id: str,
        mirrored_azure_databricks_catalog_id: str,
        update_mirrored_azure_databricks_catalog_request: Union[
            _models.UpdateMirroredAzureDatabricksCatalogRequest, IO[bytes]
        ],
        **kwargs: Any
    ) -> _models.MirroredAzureDatabricksCatalog:
        """Updates the properties of the specified mirroredAzureDatabricksCatalog.

        ..

           [!NOTE]
           Mirrored Azure Databricks Catalog item is currently in Preview (\\ `learn more
        </fabric/fundamentals/preview>`_\\ ).


        Permissions
        -----------

         The caller must have *read and write* permissions for the mirrored azure databricks catalog.

        Required Delegated Scopes
        -------------------------

         MirroredAzureDatabricksCatalog.ReadWrite.All or Item.ReadWrite.All

         [!NOTE]

        ..

           Item.Execute.All is required if you are updating AutoSync property.


        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param mirrored_azure_databricks_catalog_id: The mirroredAzureDatabricksCatalog ID. Required.
        :type mirrored_azure_databricks_catalog_id: str
        :param update_mirrored_azure_databricks_catalog_request: Update mirroredAzureDatabricksCatalog
         request payload. Is either a UpdateMirroredAzureDatabricksCatalogRequest type or a IO[bytes]
         type. Required.
        :type update_mirrored_azure_databricks_catalog_request:
         ~microsoft.fabric.api.mirroredazuredatabrickscatalog.models.UpdateMirroredAzureDatabricksCatalogRequest
         or IO[bytes]
        :return: MirroredAzureDatabricksCatalog
        :rtype:
         ~microsoft.fabric.api.mirroredazuredatabrickscatalog.models.MirroredAzureDatabricksCatalog
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.MirroredAzureDatabricksCatalog] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _json = None
        _content = None
        if isinstance(update_mirrored_azure_databricks_catalog_request, (IOBase, bytes)):
            _content = update_mirrored_azure_databricks_catalog_request
        else:
            _json = self._serialize.body(
                update_mirrored_azure_databricks_catalog_request, "UpdateMirroredAzureDatabricksCatalogRequest"
            )

        _request = build_items_update_mirrored_azure_databricks_catalog_request(
            workspace_id=workspace_id,
            mirrored_azure_databricks_catalog_id=mirrored_azure_databricks_catalog_id,
            content_type=content_type,
            json=_json,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        deserialized = self._deserialize("MirroredAzureDatabricksCatalog", pipeline_response.http_response)

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace
    def delete_mirrored_azure_databricks_catalog(  # pylint: disable=inconsistent-return-statements
        self, workspace_id: str, mirrored_azure_databricks_catalog_id: str, **kwargs: Any
    ) -> None:
        """Deletes the specified mirroredAzureDatabricksCatalog.

        ..

           [!NOTE]
           Mirrored Azure Databricks Catalog item is currently in Preview (\\ `learn more
        </fabric/fundamentals/preview>`_\\ ).


        Permissions
        -----------

         The caller must have *write* permissions for the mirrored azure databricks catalog.

        Required Delegated Scopes
        -------------------------

         MirroredAzureDatabricksCatalog.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param mirrored_azure_databricks_catalog_id: The mirroredAzureDatabricksCatalog ID. Required.
        :type mirrored_azure_databricks_catalog_id: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[None] = kwargs.pop("cls", None)

        _request = build_items_delete_mirrored_azure_databricks_catalog_request(
            workspace_id=workspace_id,
            mirrored_azure_databricks_catalog_id=mirrored_azure_databricks_catalog_id,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        if cls:
            return cls(pipeline_response, None, {})  # type: ignore

    def _get_mirrored_azure_databricks_catalog_definition_initial(  # pylint: disable=name-too-long
        self, workspace_id: str, mirrored_azure_databricks_catalog_id: str, **kwargs: Any
    ) -> Iterator[bytes]:
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[Iterator[bytes]] = kwargs.pop("cls", None)

        _request = build_items_get_mirrored_azure_databricks_catalog_definition_request(
            workspace_id=workspace_id,
            mirrored_azure_databricks_catalog_id=mirrored_azure_databricks_catalog_id,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = True
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200, 202]:
            response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        if response.status_code == 200:
            deserialized = response.iter_bytes()

        if response.status_code == 202:
            response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))
            response_headers["x-ms-operation-id"] = self._deserialize("str", response.headers.get("x-ms-operation-id"))
            response_headers["Retry-After"] = self._deserialize("int", response.headers.get("Retry-After"))

            deserialized = response.iter_bytes()

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace
    def begin_get_mirrored_azure_databricks_catalog_definition(  # pylint: disable=name-too-long
        self, workspace_id: str, mirrored_azure_databricks_catalog_id: str, **kwargs: Any
    ) -> LROPoller[_models.MirroredAzureDatabricksCatalogDefinitionResponse]:
        """Returns the specified mirroredAzureDatabricksCatalog public definition.

        ..

           [!NOTE]
           Mirrored Azure Databricks Catalog item is currently in Preview (\\ `learn more
        </fabric/fundamentals/preview>`_\\ ).


        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Permissions
        -----------

         The caller must have *read and write* permissions for the mirrored azure databricks catalog.

        Required Delegated Scopes
        -------------------------

         MirroredAzureDatabricksCatalog.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param mirrored_azure_databricks_catalog_id: The mirroredAzureDatabricksCatalog ID. Required.
        :type mirrored_azure_databricks_catalog_id: str
        :return: An instance of LROPoller that returns MirroredAzureDatabricksCatalogDefinitionResponse
        :rtype:
         ~azure.core.polling.LROPoller[~microsoft.fabric.api.mirroredazuredatabrickscatalog.models.MirroredAzureDatabricksCatalogDefinitionResponse]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.MirroredAzureDatabricksCatalogDefinitionResponse] = kwargs.pop("cls", None)
        polling: Union[bool, PollingMethod] = kwargs.pop("polling", True)
        lro_delay = kwargs.pop("polling_interval", self._config.polling_interval)
        cont_token: Optional[str] = kwargs.pop("continuation_token", None)
        if cont_token is None:
            raw_result = self._get_mirrored_azure_databricks_catalog_definition_initial(
                workspace_id=workspace_id,
                mirrored_azure_databricks_catalog_id=mirrored_azure_databricks_catalog_id,
                cls=lambda x, y, z: x,
                headers=_headers,
                params=_params,
                **kwargs
            )
            raw_result.http_response.read()  # type: ignore
        kwargs.pop("error_map", None)

        def get_long_running_output(pipeline_response):
            deserialized = self._deserialize(
                "MirroredAzureDatabricksCatalogDefinitionResponse", pipeline_response.http_response
            )
            if cls:
                return cls(pipeline_response, deserialized, {})  # type: ignore
            return deserialized

        if polling is True:
            polling_method: PollingMethod = cast(PollingMethod, LROBasePolling(lro_delay, **kwargs))
        elif polling is False:
            polling_method = cast(PollingMethod, NoPolling())
        else:
            polling_method = polling
        if cont_token:
            return LROPoller[_models.MirroredAzureDatabricksCatalogDefinitionResponse].from_continuation_token(
                polling_method=polling_method,
                continuation_token=cont_token,
                client=self._client,
                deserialization_callback=get_long_running_output,
            )
        return LROPoller[_models.MirroredAzureDatabricksCatalogDefinitionResponse](
            self._client, raw_result, get_long_running_output, polling_method  # type: ignore
        )

    def _update_mirrored_azure_databricks_catalog_definition_initial(  # pylint: disable=name-too-long
        self,
        workspace_id: str,
        mirrored_azure_databricks_catalog_id: str,
        update_mirrored_azure_databricks_catalog_definition_request: Union[
            _models.UpdatemirroredAzureDatabricksCatalogDefinitionRequest, IO[bytes]
        ],
        **kwargs: Any
    ) -> Iterator[bytes]:
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[Iterator[bytes]] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _json = None
        _content = None
        if isinstance(update_mirrored_azure_databricks_catalog_definition_request, (IOBase, bytes)):
            _content = update_mirrored_azure_databricks_catalog_definition_request
        else:
            _json = self._serialize.body(
                update_mirrored_azure_databricks_catalog_definition_request,
                "UpdatemirroredAzureDatabricksCatalogDefinitionRequest",
            )

        _request = build_items_update_mirrored_azure_databricks_catalog_definition_request(
            workspace_id=workspace_id,
            mirrored_azure_databricks_catalog_id=mirrored_azure_databricks_catalog_id,
            content_type=content_type,
            json=_json,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = True
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200, 202]:
            response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        if response.status_code == 200:
            deserialized = response.iter_bytes()

        if response.status_code == 202:
            response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))
            response_headers["x-ms-operation-id"] = self._deserialize("str", response.headers.get("x-ms-operation-id"))
            response_headers["Retry-After"] = self._deserialize("int", response.headers.get("Retry-After"))

            deserialized = response.iter_bytes()

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @overload
    def begin_update_mirrored_azure_databricks_catalog_definition(  # pylint: disable=name-too-long
        self,
        workspace_id: str,
        mirrored_azure_databricks_catalog_id: str,
        update_mirrored_azure_databricks_catalog_definition_request: _models.UpdatemirroredAzureDatabricksCatalogDefinitionRequest,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> LROPoller[None]:
        """Overrides the definition for the specified mirroredAzureDatabricksCatalog.

        ..

           [!NOTE]
           Mirrored Azure Databricks Catalog item is currently in Preview (\\ `learn more
        </fabric/fundamentals/preview>`_\\ ).


        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Permissions
        -----------

         The caller must have *read and write* permissions for the mirrored azure databricks catalog.

        Required Delegated Scopes
        -------------------------

         MirroredAzureDatabricksCatalog.ReadWrite.All or Item.ReadWrite.All

         [!NOTE]

        ..

           Item.Execute.All is required if you are updating AutoSync property.


        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param mirrored_azure_databricks_catalog_id: The mirroredAzureDatabricksCatalog ID. Required.
        :type mirrored_azure_databricks_catalog_id: str
        :param update_mirrored_azure_databricks_catalog_definition_request: Update
         mirroredAzureDatabricksCatalog definition request payload. Required.
        :type update_mirrored_azure_databricks_catalog_definition_request:
         ~microsoft.fabric.api.mirroredazuredatabrickscatalog.models.UpdatemirroredAzureDatabricksCatalogDefinitionRequest
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: An instance of LROPoller that returns None
        :rtype: ~azure.core.polling.LROPoller[None]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    def begin_update_mirrored_azure_databricks_catalog_definition(  # pylint: disable=name-too-long
        self,
        workspace_id: str,
        mirrored_azure_databricks_catalog_id: str,
        update_mirrored_azure_databricks_catalog_definition_request: IO[bytes],
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> LROPoller[None]:
        """Overrides the definition for the specified mirroredAzureDatabricksCatalog.

        ..

           [!NOTE]
           Mirrored Azure Databricks Catalog item is currently in Preview (\\ `learn more
        </fabric/fundamentals/preview>`_\\ ).


        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Permissions
        -----------

         The caller must have *read and write* permissions for the mirrored azure databricks catalog.

        Required Delegated Scopes
        -------------------------

         MirroredAzureDatabricksCatalog.ReadWrite.All or Item.ReadWrite.All

         [!NOTE]

        ..

           Item.Execute.All is required if you are updating AutoSync property.


        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param mirrored_azure_databricks_catalog_id: The mirroredAzureDatabricksCatalog ID. Required.
        :type mirrored_azure_databricks_catalog_id: str
        :param update_mirrored_azure_databricks_catalog_definition_request: Update
         mirroredAzureDatabricksCatalog definition request payload. Required.
        :type update_mirrored_azure_databricks_catalog_definition_request: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: An instance of LROPoller that returns None
        :rtype: ~azure.core.polling.LROPoller[None]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace
    def begin_update_mirrored_azure_databricks_catalog_definition(  # pylint: disable=name-too-long
        self,
        workspace_id: str,
        mirrored_azure_databricks_catalog_id: str,
        update_mirrored_azure_databricks_catalog_definition_request: Union[
            _models.UpdatemirroredAzureDatabricksCatalogDefinitionRequest, IO[bytes]
        ],
        **kwargs: Any
    ) -> LROPoller[None]:
        """Overrides the definition for the specified mirroredAzureDatabricksCatalog.

        ..

           [!NOTE]
           Mirrored Azure Databricks Catalog item is currently in Preview (\\ `learn more
        </fabric/fundamentals/preview>`_\\ ).


        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Permissions
        -----------

         The caller must have *read and write* permissions for the mirrored azure databricks catalog.

        Required Delegated Scopes
        -------------------------

         MirroredAzureDatabricksCatalog.ReadWrite.All or Item.ReadWrite.All

         [!NOTE]

        ..

           Item.Execute.All is required if you are updating AutoSync property.


        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param mirrored_azure_databricks_catalog_id: The mirroredAzureDatabricksCatalog ID. Required.
        :type mirrored_azure_databricks_catalog_id: str
        :param update_mirrored_azure_databricks_catalog_definition_request: Update
         mirroredAzureDatabricksCatalog definition request payload. Is either a
         UpdatemirroredAzureDatabricksCatalogDefinitionRequest type or a IO[bytes] type. Required.
        :type update_mirrored_azure_databricks_catalog_definition_request:
         ~microsoft.fabric.api.mirroredazuredatabrickscatalog.models.UpdatemirroredAzureDatabricksCatalogDefinitionRequest
         or IO[bytes]
        :return: An instance of LROPoller that returns None
        :rtype: ~azure.core.polling.LROPoller[None]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[None] = kwargs.pop("cls", None)
        polling: Union[bool, PollingMethod] = kwargs.pop("polling", True)
        lro_delay = kwargs.pop("polling_interval", self._config.polling_interval)
        cont_token: Optional[str] = kwargs.pop("continuation_token", None)
        if cont_token is None:
            raw_result = self._update_mirrored_azure_databricks_catalog_definition_initial(
                workspace_id=workspace_id,
                mirrored_azure_databricks_catalog_id=mirrored_azure_databricks_catalog_id,
                update_mirrored_azure_databricks_catalog_definition_request=update_mirrored_azure_databricks_catalog_definition_request,
                content_type=content_type,
                cls=lambda x, y, z: x,
                headers=_headers,
                params=_params,
                **kwargs
            )
            raw_result.http_response.read()  # type: ignore
        kwargs.pop("error_map", None)

        def get_long_running_output(pipeline_response):  # pylint: disable=inconsistent-return-statements
            if cls:
                return cls(pipeline_response, None, {})  # type: ignore

        if polling is True:
            polling_method: PollingMethod = cast(PollingMethod, LROBasePolling(lro_delay, **kwargs))
        elif polling is False:
            polling_method = cast(PollingMethod, NoPolling())
        else:
            polling_method = polling
        if cont_token:
            return LROPoller[None].from_continuation_token(
                polling_method=polling_method,
                continuation_token=cont_token,
                client=self._client,
                deserialization_callback=get_long_running_output,
            )
        return LROPoller[None](self._client, raw_result, get_long_running_output, polling_method)  # type: ignore


class RefreshOperations:
    """
    .. warning::
        **DO NOT** instantiate this class directly.

        Instead, you should access the following operations through
        :class:`~microsoft.fabric.api.mirroredazuredatabrickscatalog.FabricMirroredAzureDatabricksCatalogClient`'s
        :attr:`refresh` attribute.
    """

    models = _models

    def __init__(self, *args, **kwargs):
        input_args = list(args)
        self._client = input_args.pop(0) if input_args else kwargs.pop("client")
        self._config = input_args.pop(0) if input_args else kwargs.pop("config")
        self._serialize = input_args.pop(0) if input_args else kwargs.pop("serializer")
        self._deserialize = input_args.pop(0) if input_args else kwargs.pop("deserializer")

    def _refresh_catalog_metadata_initial(
        self, workspace_id: str, mirrored_azure_databricks_catalog_id: str, **kwargs: Any
    ) -> Iterator[bytes]:
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[Iterator[bytes]] = kwargs.pop("cls", None)

        _request = build_refresh_refresh_catalog_metadata_request(
            workspace_id=workspace_id,
            mirrored_azure_databricks_catalog_id=mirrored_azure_databricks_catalog_id,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = True
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200, 202]:
            response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        if response.status_code == 200:
            deserialized = response.iter_bytes()

        if response.status_code == 202:
            response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))
            response_headers["x-ms-operation-id"] = self._deserialize("str", response.headers.get("x-ms-operation-id"))
            response_headers["Retry-After"] = self._deserialize("int", response.headers.get("Retry-After"))

            deserialized = response.iter_bytes()

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace
    def begin_refresh_catalog_metadata(
        self, workspace_id: str, mirrored_azure_databricks_catalog_id: str, **kwargs: Any
    ) -> LROPoller[None]:
        """Refresh Databricks catalog metadata in mirroredAzureDatabricksCatalogs Item.

        ..

           [!NOTE]
           Mirrored Azure Databricks Catalog item is currently in Preview (\\ `learn more
        </fabric/fundamentals/preview>`_\\ ).


        This API supports `long running operations (LRO)
        </rest/api/fabric/articles/long-running-operation>`_.

        Permissions
        -----------

         The API caller must have *contributor* or higher workspace role.

        Required Delegated Scopes
        -------------------------

         MirroredAzureDatabricksCatalog.ReadWrite.All or Item.ReadWrite.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param mirrored_azure_databricks_catalog_id: The mirroredAzureDatabricksCatalog ID. Required.
        :type mirrored_azure_databricks_catalog_id: str
        :return: An instance of LROPoller that returns None
        :rtype: ~azure.core.polling.LROPoller[None]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[None] = kwargs.pop("cls", None)
        polling: Union[bool, PollingMethod] = kwargs.pop("polling", True)
        lro_delay = kwargs.pop("polling_interval", self._config.polling_interval)
        cont_token: Optional[str] = kwargs.pop("continuation_token", None)
        if cont_token is None:
            raw_result = self._refresh_catalog_metadata_initial(
                workspace_id=workspace_id,
                mirrored_azure_databricks_catalog_id=mirrored_azure_databricks_catalog_id,
                cls=lambda x, y, z: x,
                headers=_headers,
                params=_params,
                **kwargs
            )
            raw_result.http_response.read()  # type: ignore
        kwargs.pop("error_map", None)

        def get_long_running_output(pipeline_response):  # pylint: disable=inconsistent-return-statements
            if cls:
                return cls(pipeline_response, None, {})  # type: ignore

        if polling is True:
            polling_method: PollingMethod = cast(PollingMethod, LROBasePolling(lro_delay, **kwargs))
        elif polling is False:
            polling_method = cast(PollingMethod, NoPolling())
        else:
            polling_method = polling
        if cont_token:
            return LROPoller[None].from_continuation_token(
                polling_method=polling_method,
                continuation_token=cont_token,
                client=self._client,
                deserialization_callback=get_long_running_output,
            )
        return LROPoller[None](self._client, raw_result, get_long_running_output, polling_method)  # type: ignore


class DiscoveryOperations:
    """
    .. warning::
        **DO NOT** instantiate this class directly.

        Instead, you should access the following operations through
        :class:`~microsoft.fabric.api.mirroredazuredatabrickscatalog.FabricMirroredAzureDatabricksCatalogClient`'s
        :attr:`discovery` attribute.
    """

    models = _models

    def __init__(self, *args, **kwargs):
        input_args = list(args)
        self._client = input_args.pop(0) if input_args else kwargs.pop("client")
        self._config = input_args.pop(0) if input_args else kwargs.pop("config")
        self._serialize = input_args.pop(0) if input_args else kwargs.pop("serializer")
        self._deserialize = input_args.pop(0) if input_args else kwargs.pop("deserializer")

    @distributed_trace
    def discover_catalogs(
        self,
        workspace_id: str,
        *,
        databricks_workspace_connection_id: str,
        continuation_token_parameter: Optional[str] = None,
        max_results: Optional[int] = None,
        **kwargs: Any
    ) -> Iterable["_models.DatabricksCatalog"]:
        """Returns a list of catalogs from Unity Catalog.

        ..

           [!NOTE]
           Mirrored Azure Databricks Catalog item is currently in Preview (\\ `learn more
        </fabric/fundamentals/preview>`_\\ ).


        This API supports `pagination </rest/api/fabric/articles/pagination>`_.

        Permissions
        -----------

         The caller must have *viewer* or higher workspace role.

        Required Delegated Scopes
        -------------------------

        Workspace.Read.All or Workspace.ReadWrite.All or DatabricksCatalog.Read.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :keyword databricks_workspace_connection_id: The Databricks workspace connection ID. Required.
        :paramtype databricks_workspace_connection_id: str
        :keyword continuation_token_parameter: A token for retrieving the next page of results. Default
         value is None.
        :paramtype continuation_token_parameter: str
        :keyword max_results: The maximum number of results to return. Default value is None.
        :paramtype max_results: int
        :return: An iterator like instance of DatabricksCatalog
        :rtype:
         ~azure.core.paging.ItemPaged[~microsoft.fabric.api.mirroredazuredatabrickscatalog.models.DatabricksCatalog]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models._models.DatabricksCatalogs] = kwargs.pop("cls", None)  # pylint: disable=protected-access

        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        def prepare_request(next_link=None):
            if not next_link:

                _request = build_discovery_discover_catalogs_request(
                    workspace_id=workspace_id,
                    databricks_workspace_connection_id=databricks_workspace_connection_id,
                    continuation_token_parameter=continuation_token_parameter,
                    max_results=max_results,
                    headers=_headers,
                    params=_params,
                )
                _request.url = self._client.format_url(_request.url)

            else:
                _request = HttpRequest("GET", next_link)
                _request.url = self._client.format_url(_request.url)

            return _request

        def extract_data(pipeline_response):
            deserialized = self._deserialize(
                _models._models.DatabricksCatalogs, pipeline_response  # pylint: disable=protected-access
            )
            list_of_elem = deserialized.value
            if cls:
                list_of_elem = cls(list_of_elem)  # type: ignore
            return deserialized.continuation_uri or None, iter(list_of_elem)

        def get_next(next_link=None):
            _request = prepare_request(next_link)

            _stream = False
            pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
                _request, stream=_stream, **kwargs
            )
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
                raise HttpResponseError(response=response, model=error)

            return pipeline_response

        return ItemPaged(get_next, extract_data)

    @distributed_trace
    def discover_schemas(
        self,
        workspace_id: str,
        catalog_name: str,
        *,
        databricks_workspace_connection_id: str,
        continuation_token_parameter: Optional[str] = None,
        max_results: Optional[int] = None,
        **kwargs: Any
    ) -> Iterable["_models.DatabricksSchema"]:
        """Returns a list of schemas in the given catalog from Unity Catalog.

        ..

           [!NOTE]
           Mirrored Azure Databricks Catalog item is currently in Preview (\\ `learn more
        </fabric/fundamentals/preview>`_\\ ).


        This API supports `pagination </rest/api/fabric/articles/pagination>`_.

        Permissions
        -----------

         The caller must have *viewer* or higher workspace role.

        Required Delegated Scopes
        -------------------------

        Workspace.Read.All or Workspace.ReadWrite.All or DatabricksCatalog.Read.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param catalog_name: The catalog name. Required.
        :type catalog_name: str
        :keyword databricks_workspace_connection_id: The Databricks workspace connection ID. Required.
        :paramtype databricks_workspace_connection_id: str
        :keyword continuation_token_parameter: A token for retrieving the next page of results. Default
         value is None.
        :paramtype continuation_token_parameter: str
        :keyword max_results: The maximum number of results to return. Default value is None.
        :paramtype max_results: int
        :return: An iterator like instance of DatabricksSchema
        :rtype:
         ~azure.core.paging.ItemPaged[~microsoft.fabric.api.mirroredazuredatabrickscatalog.models.DatabricksSchema]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models._models.DatabricksSchemas] = kwargs.pop("cls", None)  # pylint: disable=protected-access

        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        def prepare_request(next_link=None):
            if not next_link:

                _request = build_discovery_discover_schemas_request(
                    workspace_id=workspace_id,
                    catalog_name=catalog_name,
                    databricks_workspace_connection_id=databricks_workspace_connection_id,
                    continuation_token_parameter=continuation_token_parameter,
                    max_results=max_results,
                    headers=_headers,
                    params=_params,
                )
                _request.url = self._client.format_url(_request.url)

            else:
                _request = HttpRequest("GET", next_link)
                _request.url = self._client.format_url(_request.url)

            return _request

        def extract_data(pipeline_response):
            deserialized = self._deserialize(
                _models._models.DatabricksSchemas, pipeline_response  # pylint: disable=protected-access
            )
            list_of_elem = deserialized.value
            if cls:
                list_of_elem = cls(list_of_elem)  # type: ignore
            return deserialized.continuation_uri or None, iter(list_of_elem)

        def get_next(next_link=None):
            _request = prepare_request(next_link)

            _stream = False
            pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
                _request, stream=_stream, **kwargs
            )
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
                raise HttpResponseError(response=response, model=error)

            return pipeline_response

        return ItemPaged(get_next, extract_data)

    @distributed_trace
    def discover_tables(
        self,
        workspace_id: str,
        catalog_name: str,
        schema_name: str,
        *,
        databricks_workspace_connection_id: str,
        continuation_token_parameter: Optional[str] = None,
        max_results: Optional[int] = None,
        **kwargs: Any
    ) -> Iterable["_models.DatabricksTable"]:
        """Returns a list of tables in the given schema from Unity Catalog.

        ..

           [!NOTE]
           Mirrored Azure Databricks Catalog item is currently in Preview (\\ `learn more
        </fabric/fundamentals/preview>`_\\ ).


        This API supports `pagination </rest/api/fabric/articles/pagination>`_.

        Permissions
        -----------

         The caller must have *viewer* or higher workspace role.

        Required Delegated Scopes
        -------------------------

        Workspace.Read.All or Workspace.ReadWrite.All or DatabricksCatalog.Read.All

        Microsoft Entra supported identities
        ------------------------------------

        This API supports the Microsoft `identities </rest/api/fabric/articles/identity-support>`_
        listed in this section.

        .. list-table::
           :header-rows: 1

           * - Identity
             - Support
           * - User
             - Yes
           * - `Service principal
        </entra/identity-platform/app-objects-and-service-principals#service-principal-object>`_ and
        `Managed identities </entra/identity/managed-identities-azure-resources/overview>`_
             - Yes


        Interface
        ---------.

        :param workspace_id: The workspace ID. Required.
        :type workspace_id: str
        :param catalog_name: The catalog name. Required.
        :type catalog_name: str
        :param schema_name: The schema name. Required.
        :type schema_name: str
        :keyword databricks_workspace_connection_id: The Databricks workspace connection ID. Required.
        :paramtype databricks_workspace_connection_id: str
        :keyword continuation_token_parameter: A token for retrieving the next page of results. Default
         value is None.
        :paramtype continuation_token_parameter: str
        :keyword max_results: The maximum number of results to return. Default value is None.
        :paramtype max_results: int
        :return: An iterator like instance of DatabricksTable
        :rtype:
         ~azure.core.paging.ItemPaged[~microsoft.fabric.api.mirroredazuredatabrickscatalog.models.DatabricksTable]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models._models.DatabricksTables] = kwargs.pop("cls", None)  # pylint: disable=protected-access

        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        def prepare_request(next_link=None):
            if not next_link:

                _request = build_discovery_discover_tables_request(
                    workspace_id=workspace_id,
                    catalog_name=catalog_name,
                    schema_name=schema_name,
                    databricks_workspace_connection_id=databricks_workspace_connection_id,
                    continuation_token_parameter=continuation_token_parameter,
                    max_results=max_results,
                    headers=_headers,
                    params=_params,
                )
                _request.url = self._client.format_url(_request.url)

            else:
                _request = HttpRequest("GET", next_link)
                _request.url = self._client.format_url(_request.url)

            return _request

        def extract_data(pipeline_response):
            deserialized = self._deserialize(
                _models._models.DatabricksTables, pipeline_response  # pylint: disable=protected-access
            )
            list_of_elem = deserialized.value
            if cls:
                list_of_elem = cls(list_of_elem)  # type: ignore
            return deserialized.continuation_uri or None, iter(list_of_elem)

        def get_next(next_link=None):
            _request = prepare_request(next_link)

            _stream = False
            pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
                _request, stream=_stream, **kwargs
            )
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)
                raise HttpResponseError(response=response, model=error)

            return pipeline_response

        return ItemPaged(get_next, extract_data)
