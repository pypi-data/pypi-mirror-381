---
title: Overview
---

```{=html}
<style>h1.title { display:none; }</style>
```

<img src="/logos/hero/hero.png" alt="chatlas website banner image" class="d-block mx-auto mb-3" style="max-width:100%; max-height:325px"/>

<p class="display-2 header text-center" style="font-weight:600;">
chatlas
</p>

<p class="fs-2 lead mt-3 mb-3 text-center" style="max-width: 600px; margin: auto;">
Your friendly guide to building LLM chat apps in Python with less effort and more clarity.
</p>

<div class="d-flex justify-content-center mb-3">
<!-- badges start -->
<a href="https://pypi.org/project/chatlas/"><img alt="PyPI" src="https://img.shields.io/pypi/v/chatlas?logo=python&logoColor=white&color=orange"></a>
<a href="https://choosealicense.com/licenses/mit/"><img src="https://img.shields.io/badge/License-MIT-blue.svg" alt="MIT License"></a>
<a href="https://pypi.org/project/chatlas"><img src="https://img.shields.io/pypi/pyversions/chatlas.svg" alt="versions"></a>
<a href="https://github.com/posit-dev/chatlas"><img src="https://github.com/posit-dev/chatlas/actions/workflows/test.yml/badge.svg?branch=main" alt="Python Tests"></a>
<!-- badges end -->
</div>



## Quick start

Get started in 3 simple steps:

1. Choose a model provider, such as [ChatOpenAI](https://posit-dev.github.io/chatlas/reference/ChatOpenAI.html) or [ChatAnthropic](https://posit-dev.github.io/chatlas/reference/ChatAnthropic.html).
2. Visit the provider's [reference](reference/index.qmd) page to get setup with necessary credentials.
3. Create the relevant `Chat` client and start chatting!

```python
from chatlas import ChatOpenAI

# Optional (but recommended) model and system_prompt
chat = ChatOpenAI(
    model="gpt-4.1-mini",
    system_prompt="You are a helpful assistant.",
)

# Optional tool registration
def get_current_weather(lat: float, lng: float):
    "Get the current weather for a given location."
    return "sunny"

chat.register_tool(get_current_weather)

# Send user prompt to the model for a response.
chat.chat("How's the weather in San Francisco?")
```

::: chatlas-response-container

```python
# 🛠️ tool request
get_current_weather(37.7749, -122.4194)
```

```
# ✅ tool result
sunny
```

The current weather in San Francisco is sunny.
:::


<!--
## Video tutorial

The following video provides a nice visual overview of chatlas and what it's capable of.

![Screen-recording of chatlas in action]()
-->


## Install

Install the latest stable release [from PyPI](https://pypi.org/project/chatlas/):

```bash
pip install -U chatlas
```



## Why chatlas?

🚀 **Opinionated design**: most problems just need the right [model](get-started/models.qmd), [system prompt](get-started/system-prompt.qmd), and [tool calls](get-started/tools.qmd). Spend more time mastering the fundamentals and less time navigating needless complexity.

🧩 [**Model agnostic**](get-started/models.qmd): try different models with minimal code changes.

🌊 [**Stream output**](get-started/chat.qmd): automatically in notebooks, at the console, and your favorite IDE. You can also [stream](get-started/stream.qmd) responses into bespoke applications (e.g., [chatbots](get-started/chatbots.qmd)).

🛠️ [**Tool calling**](get-started/tools.qmd): give the LLM "agentic" capabilities by simply writing Python function(s).

🔄 [**Multi-turn chat**](get-started/chat.qmd#chat-history): history is retained by default, making the common case easy.

🖼️ [**Multi-modal input**](get-started/chat.qmd#multi-modal-input): submit input like images, pdfs, and more.

📂 [**Structured output**](get-started/structured-data.qmd): easily extract structure from unstructured input.

⏱️ [**Async**](get-started/async.qmd): supports async operations for efficiency and scale.

✏️ [**Autocomplete**](get-started/models.qmd#auto-complete): easily discover and use provider-specific [parameters](get-started/parameters.qmd) like `temperature`, `max_tokens`, and more.

🔍 **Inspectable**: tools for [debugging](get-started/debug.qmd) and [monitoring](get-started/monitor.qmd) in production.

🔌 **Extensible**: add new [model providers](reference/Provider.qmd), [content types](reference/types.Content.qmd), and more.


## Next steps

Next we'll learn more about what [model providers](get-started/models.qmd) are available and how to approach picking a particular model.
If you already have a model in mind, or just want to see what chatlas can do, skip ahead to [hello chat](get-started/chat.qmd).