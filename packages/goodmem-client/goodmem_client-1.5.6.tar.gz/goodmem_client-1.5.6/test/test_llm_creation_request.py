# coding: utf-8

"""
    GoodMem API

    API for interacting with the GoodMem service, providing vector-based memory storage and retrieval with multiple embedder support. The service enables creation of memory spaces, storing memories with vector representations, and efficient similarity-based retrieval.

    The version of the OpenAPI document: v1
    Contact: support@goodmem.io
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


import unittest

from goodmem_client.models.llm_creation_request import LLMCreationRequest

class TestLLMCreationRequest(unittest.TestCase):
    """LLMCreationRequest unit test stubs"""

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def make_instance(self, include_optional) -> LLMCreationRequest:
        """Test LLMCreationRequest
            include_optional is a boolean, when False only required
            params are included, when True both required and
            optional params are included """
        # uncomment below to create an instance of `LLMCreationRequest`
        """
        model = LLMCreationRequest()
        if include_optional:
            return LLMCreationRequest(
                validate = None,
                display_name = 'GPT-4 Turbo',
                description = 'OpenAI's GPT-4 Turbo model for chat completions',
                provider_type = 'OPENAI',
                endpoint_url = 'https://api.openai.com',
                api_path = '/v1/chat/completions',
                model_identifier = 'gpt-4-turbo-preview',
                supported_modalities = ["TEXT"],
                credentials = 'sk-1234567890abcdef',
                labels = {"environment": "production", "team": "ai"},
                version = '1.0.0',
                monitoring_endpoint = 'https://monitoring.example.com/llms/status',
                capabilities = goodmem_client.models.llm_capabilities.LLMCapabilities(
                    supports_chat = True, 
                    supports_completion = True, 
                    supports_function_calling = True, 
                    supports_system_messages = True, 
                    supports_streaming = True, 
                    supports_sampling_parameters = True, ),
                default_sampling_params = goodmem_client.models.llm_sampling_params.LLMSamplingParams(
                    max_tokens = 2048, 
                    temperature = 0.7, 
                    top_p = 0.9, 
                    top_k = 50, 
                    frequency_penalty = 0.0, 
                    presence_penalty = 0.0, 
                    stop_sequences = ["\n\n", "END"], ),
                max_context_length = 32768,
                client_config = {
                    'key' : None
                    },
                owner_id = '550e8400-e29b-41d4-a716-446655440000'
            )
        else:
            return LLMCreationRequest(
                display_name = 'GPT-4 Turbo',
                provider_type = 'OPENAI',
                endpoint_url = 'https://api.openai.com',
                model_identifier = 'gpt-4-turbo-preview',
                credentials = 'sk-1234567890abcdef',
                capabilities = goodmem_client.models.llm_capabilities.LLMCapabilities(
                    supports_chat = True, 
                    supports_completion = True, 
                    supports_function_calling = True, 
                    supports_system_messages = True, 
                    supports_streaming = True, 
                    supports_sampling_parameters = True, ),
        )
        """

    def testLLMCreationRequest(self):
        """Test LLMCreationRequest"""
        # inst_req_only = self.make_instance(include_optional=False)
        # inst_req_and_optional = self.make_instance(include_optional=True)

if __name__ == '__main__':
    unittest.main()
