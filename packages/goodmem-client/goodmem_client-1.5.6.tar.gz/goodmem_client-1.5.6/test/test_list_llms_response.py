# coding: utf-8

"""
    GoodMem API

    API for interacting with the GoodMem service, providing vector-based memory storage and retrieval with multiple embedder support. The service enables creation of memory spaces, storing memories with vector representations, and efficient similarity-based retrieval.

    The version of the OpenAPI document: v1
    Contact: support@goodmem.io
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


import unittest

from goodmem_client.models.list_llms_response import ListLLMsResponse

class TestListLLMsResponse(unittest.TestCase):
    """ListLLMsResponse unit test stubs"""

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def make_instance(self, include_optional) -> ListLLMsResponse:
        """Test ListLLMsResponse
            include_optional is a boolean, when False only required
            params are included, when True both required and
            optional params are included """
        # uncomment below to create an instance of `ListLLMsResponse`
        """
        model = ListLLMsResponse()
        if include_optional:
            return ListLLMsResponse(
                llms = [
                    goodmem_client.models.llm_response.LLMResponse(
                        llm_id = '550e8400-e29b-41d4-a716-446655440000', 
                        display_name = 'GPT-4 Turbo', 
                        description = 'OpenAI's GPT-4 Turbo model for chat completions', 
                        provider_type = 'OPENAI', 
                        endpoint_url = 'https://api.openai.com', 
                        api_path = '/v1/chat/completions', 
                        model_identifier = 'gpt-4-turbo-preview', 
                        supported_modalities = [
                            'TEXT'
                            ], 
                        labels = {"environment": "production", "team": "ai"}, 
                        version = '1.0.0', 
                        monitoring_endpoint = 'https://monitoring.example.com/llms/status', 
                        capabilities = goodmem_client.models.llm_capabilities.LLMCapabilities(
                            supports_chat = True, 
                            supports_completion = True, 
                            supports_function_calling = True, 
                            supports_system_messages = True, 
                            supports_streaming = True, 
                            supports_sampling_parameters = True, ), 
                        default_sampling_params = goodmem_client.models.llm_sampling_params.LLMSamplingParams(
                            max_tokens = 2048, 
                            temperature = 0.7, 
                            top_p = 0.9, 
                            top_k = 50, 
                            frequency_penalty = 0.0, 
                            presence_penalty = 0.0, 
                            stop_sequences = ["\n\n", "END"], ), 
                        max_context_length = 32768, 
                        client_config = {
                            'key' : None
                            }, 
                        owner_id = '550e8400-e29b-41d4-a716-446655440000', 
                        created_at = 1617293472000, 
                        updated_at = 1617293472000, 
                        created_by_id = '550e8400-e29b-41d4-a716-446655440000', 
                        updated_by_id = '550e8400-e29b-41d4-a716-446655440000', )
                    ]
            )
        else:
            return ListLLMsResponse(
                llms = [
                    goodmem_client.models.llm_response.LLMResponse(
                        llm_id = '550e8400-e29b-41d4-a716-446655440000', 
                        display_name = 'GPT-4 Turbo', 
                        description = 'OpenAI's GPT-4 Turbo model for chat completions', 
                        provider_type = 'OPENAI', 
                        endpoint_url = 'https://api.openai.com', 
                        api_path = '/v1/chat/completions', 
                        model_identifier = 'gpt-4-turbo-preview', 
                        supported_modalities = [
                            'TEXT'
                            ], 
                        labels = {"environment": "production", "team": "ai"}, 
                        version = '1.0.0', 
                        monitoring_endpoint = 'https://monitoring.example.com/llms/status', 
                        capabilities = goodmem_client.models.llm_capabilities.LLMCapabilities(
                            supports_chat = True, 
                            supports_completion = True, 
                            supports_function_calling = True, 
                            supports_system_messages = True, 
                            supports_streaming = True, 
                            supports_sampling_parameters = True, ), 
                        default_sampling_params = goodmem_client.models.llm_sampling_params.LLMSamplingParams(
                            max_tokens = 2048, 
                            temperature = 0.7, 
                            top_p = 0.9, 
                            top_k = 50, 
                            frequency_penalty = 0.0, 
                            presence_penalty = 0.0, 
                            stop_sequences = ["\n\n", "END"], ), 
                        max_context_length = 32768, 
                        client_config = {
                            'key' : None
                            }, 
                        owner_id = '550e8400-e29b-41d4-a716-446655440000', 
                        created_at = 1617293472000, 
                        updated_at = 1617293472000, 
                        created_by_id = '550e8400-e29b-41d4-a716-446655440000', 
                        updated_by_id = '550e8400-e29b-41d4-a716-446655440000', )
                    ],
        )
        """

    def testListLLMsResponse(self):
        """Test ListLLMsResponse"""
        # inst_req_only = self.make_instance(include_optional=False)
        # inst_req_and_optional = self.make_instance(include_optional=True)

if __name__ == '__main__':
    unittest.main()
