# coding: utf-8

"""
    GoodMem API

    API for interacting with the GoodMem service, providing vector-based memory storage and retrieval with multiple embedder support. The service enables creation of memory spaces, storing memories with vector representations, and efficient similarity-based retrieval.

    The version of the OpenAPI document: v1
    Contact: support@goodmem.io
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


import unittest

from goodmem_client.models.llm_update_request import LLMUpdateRequest

class TestLLMUpdateRequest(unittest.TestCase):
    """LLMUpdateRequest unit test stubs"""

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def make_instance(self, include_optional) -> LLMUpdateRequest:
        """Test LLMUpdateRequest
            include_optional is a boolean, when False only required
            params are included, when True both required and
            optional params are included """
        # uncomment below to create an instance of `LLMUpdateRequest`
        """
        model = LLMUpdateRequest()
        if include_optional:
            return LLMUpdateRequest(
                validate = None,
                display_name = 'Updated GPT-4 Turbo',
                description = 'Updated OpenAI's GPT-4 Turbo model for enhanced chat completions',
                endpoint_url = 'https://api.openai.com',
                api_path = '/v1/chat/completions',
                model_identifier = 'gpt-4-turbo-preview',
                supported_modalities = ["TEXT"],
                credentials = 'sk-updated-api-key-here',
                version = '2.0.1',
                monitoring_endpoint = 'https://monitoring.company.com/llms/status',
                capabilities = goodmem_client.models.llm_capabilities.LLMCapabilities(
                    supports_chat = True, 
                    supports_completion = True, 
                    supports_function_calling = True, 
                    supports_system_messages = True, 
                    supports_streaming = True, 
                    supports_sampling_parameters = True, ),
                default_sampling_params = goodmem_client.models.llm_sampling_params.LLMSamplingParams(
                    max_tokens = 2048, 
                    temperature = 0.7, 
                    top_p = 0.9, 
                    top_k = 50, 
                    frequency_penalty = 0.0, 
                    presence_penalty = 0.0, 
                    stop_sequences = ["\n\n", "END"], ),
                max_context_length = 32768,
                client_config = {
                    'key' : None
                    },
                replace_labels = {"environment": "production", "team": "ml-platform"},
                merge_labels = {"cost-center": "ai-infrastructure"}
            )
        else:
            return LLMUpdateRequest(
        )
        """

    def testLLMUpdateRequest(self):
        """Test LLMUpdateRequest"""
        # inst_req_only = self.make_instance(include_optional=False)
        # inst_req_and_optional = self.make_instance(include_optional=True)

if __name__ == '__main__':
    unittest.main()
